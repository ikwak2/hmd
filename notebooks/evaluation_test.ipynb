{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c017699",
   "metadata": {},
   "source": [
    "python evaluate_model.py labels outputs scores.csv class_scores.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b0af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path, sys, numpy as np\n",
    "from helper_code import *\n",
    "#load_patient_data, get_label, load_challenge_outputs, compare_strings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9fce69",
   "metadata": {},
   "source": [
    "classes, auroc, auprc, auroc_classes, auprc_classes, accuracy, f_measure, f_measure_classes, challenge_score  = evaluate_model(labels, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de77606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_folder = '/home/ubuntu/hmd/notebooks/physionet.org/files/circor-heart-sound/1.0.1/training_data'\n",
    "output_folder = '/home/ubuntu/hmd/notebooks/our_classifier/tmp_out1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdb74a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_folder = '/home/ubuntu/data/hmd/murmur/test/'\n",
    "output_folder = '/home/ubuntu/python-classifier-2022/test_outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae46b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnms = os.listdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c3a3bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['33151.csv', '38337.csv', '40798.csv', '49558.csv', '49572.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnms[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34474375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#33151\r\n",
      "Present,Unknown,Absent,Abnormal,Normal\r\n",
      "0,0,1,0,1\r\n",
      "0.0750779,0.023012226,0.9019099,0.38914064,0.61085933\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/ubuntu/python-classifier-2022/test_outputs/33151.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccef404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models.\n",
    "def evaluate_model(label_folder, output_folder):\n",
    "    # Define murmur and outcome classes.\n",
    "    murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "    outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "    # Load and parse label and model output files.\n",
    "    label_files, output_files = find_challenge_files(label_folder, output_folder)\n",
    "    murmur_labels = load_murmurs(label_files, murmur_classes)\n",
    "    murmur_binary_outputs, murmur_scalar_outputs = load_classifier_outputs(output_files, murmur_classes)\n",
    "    outcome_labels = load_outcomes(label_files, outcome_classes)\n",
    "    outcome_binary_outputs, outcome_scalar_outputs = load_classifier_outputs(output_files, outcome_classes)\n",
    "\n",
    "    # For each patient, set the 'Present' or 'Abnormal' class to positive if no class is positive or if multiple classes are positive.\n",
    "    murmur_labels = enforce_positives(murmur_labels, murmur_classes, 'Present')\n",
    "    murmur_binary_outputs = enforce_positives(murmur_binary_outputs, murmur_classes, 'Present')\n",
    "    outcome_labels = enforce_positives(outcome_labels, outcome_classes, 'Abnormal')\n",
    "    outcome_binary_outputs = enforce_positives(outcome_binary_outputs, outcome_classes, 'Abnormal')\n",
    "\n",
    "    # Evaluate the murmur model by comparing the labels and model outputs.\n",
    "    murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes = compute_auc(murmur_labels, murmur_scalar_outputs)\n",
    "    murmur_f_measure, murmur_f_measure_classes = compute_f_measure(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_accuracy, murmur_accuracy_classes = compute_accuracy(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_weighted_accuracy = compute_weighted_accuracy(murmur_labels, murmur_binary_outputs, murmur_classes) # This is the murmur scoring metric.\n",
    "    murmur_cost = compute_cost(outcome_labels, murmur_binary_outputs, outcome_classes, murmur_classes) # Use *outcomes* to score *murmurs* for the Challenge cost metric, but this is not the actual murmur scoring metric.\n",
    "    murmur_scores = (murmur_classes, murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes, \\\n",
    "        murmur_f_measure, murmur_f_measure_classes, murmur_accuracy, murmur_accuracy_classes, murmur_weighted_accuracy, murmur_cost)\n",
    "\n",
    "    # Evaluate the outcome model by comparing the labels and model outputs.\n",
    "    outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes = compute_auc(outcome_labels, outcome_scalar_outputs)\n",
    "    outcome_f_measure, outcome_f_measure_classes = compute_f_measure(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_accuracy, outcome_accuracy_classes = compute_accuracy(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_weighted_accuracy = compute_weighted_accuracy(outcome_labels, outcome_binary_outputs, outcome_classes)\n",
    "    outcome_cost = compute_cost(outcome_labels, outcome_binary_outputs, outcome_classes, outcome_classes) # This is the clinical outcomes scoring metric.\n",
    "    outcome_scores = (outcome_classes, outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes, \\\n",
    "        outcome_f_measure, outcome_f_measure_classes, outcome_accuracy, outcome_accuracy_classes, outcome_weighted_accuracy, outcome_cost)\n",
    "\n",
    "    # Return the results.\n",
    "    return murmur_scores, outcome_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26f77a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Find Challenge files.\n",
    "def find_challenge_files(label_folder, output_folder):\n",
    "    label_files = list()\n",
    "    output_files = list()\n",
    "    for label_file in sorted(os.listdir(label_folder)):\n",
    "        label_file_path = os.path.join(label_folder, label_file) # Full path for label file\n",
    "        if os.path.isfile(label_file_path) and label_file.lower().endswith('.txt') and not label_file.lower().startswith('.'):\n",
    "            root, ext = os.path.splitext(label_file)\n",
    "            output_file = root + '.csv'\n",
    "            output_file_path = os.path.join(output_folder, output_file) # Full path for corresponding output file\n",
    "            if os.path.isfile(output_file_path):\n",
    "                label_files.append(label_file_path)\n",
    "                output_files.append(output_file_path)\n",
    "            else:\n",
    "                raise IOError('Output file {} not found for label file {}.'.format(output_file, label_file))\n",
    "\n",
    "    if label_files and output_files:\n",
    "        return label_files, output_files\n",
    "    else:\n",
    "        raise IOError('No label or output files found.')\n",
    "\n",
    "# Load murmurs from label files.\n",
    "def load_murmurs(label_files, classes):\n",
    "    num_patients = len(label_files)\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    # Use one-hot encoding for the labels.\n",
    "    labels = np.zeros((num_patients, num_classes), dtype=np.bool_)\n",
    "\n",
    "    # Iterate over the patients.\n",
    "    for i in range(num_patients):\n",
    "        data = load_patient_data(label_files[i])\n",
    "        label = get_murmur(data)\n",
    "        for j, x in enumerate(classes):\n",
    "            if compare_strings(label, x):\n",
    "                labels[i, j] = 1\n",
    "\n",
    "    return labels\n",
    "\n",
    "# Load outcomes from label files.\n",
    "def load_outcomes(label_files, classes):\n",
    "    num_patients = len(label_files)\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    # Use one-hot encoding for the labels.\n",
    "    labels = np.zeros((num_patients, num_classes), dtype=np.bool_)\n",
    "\n",
    "    # Iterate over the patients.\n",
    "    for i in range(num_patients):\n",
    "        data = load_patient_data(label_files[i])\n",
    "        label = get_outcome(data)\n",
    "        for j, x in enumerate(classes):\n",
    "            if compare_strings(label, x):\n",
    "                labels[i, j] = 1\n",
    "\n",
    "    return labels\n",
    "\n",
    "# Load outputs from output files.\n",
    "def load_classifier_outputs(output_files, classes):\n",
    "    # The outputs should have the following form:\n",
    "    #\n",
    "    # #Record ID\n",
    "    # class_1, class_2, class_3\n",
    "    #       0,       1,       1\n",
    "    #    0.12,    0.34,    0.56\n",
    "    #\n",
    "    num_patients = len(output_files)\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    # Use one-hot encoding for the outputs.\n",
    "    binary_outputs = np.zeros((num_patients, num_classes), dtype=np.bool_)\n",
    "    scalar_outputs = np.zeros((num_patients, num_classes), dtype=np.float64)\n",
    "\n",
    "    # Iterate over the patients.\n",
    "    for i in range(num_patients):\n",
    "        patient_id, patient_classes, patient_binary_outputs, patient_scalar_outputs = load_challenge_outputs(output_files[i])\n",
    "\n",
    "        # Allow for unordered or reordered classes.\n",
    "        for j, x in enumerate(classes):\n",
    "            for k, y in enumerate(patient_classes):\n",
    "                if compare_strings(x, y):\n",
    "                    binary_outputs[i, j] = patient_binary_outputs[k]\n",
    "                    scalar_outputs[i, j] = patient_scalar_outputs[k]\n",
    "\n",
    "    return binary_outputs, scalar_outputs\n",
    "\n",
    "# For each patient, set a specific class to positive if no class is positive or multiple classes are positive.\n",
    "def enforce_positives(outputs, classes, positive_class):\n",
    "    num_patients, num_classes = np.shape(outputs)\n",
    "    j = classes.index(positive_class)\n",
    "\n",
    "    for i in range(num_patients):\n",
    "        if np.sum(outputs[i, :]) != 1:\n",
    "            outputs[i, :] = 0\n",
    "            outputs[i, j] = 1\n",
    "    return outputs\n",
    "\n",
    "# Compute macro AUROC and macro AUPRC.\n",
    "def compute_auc(labels, outputs):\n",
    "    num_patients, num_classes = np.shape(labels)\n",
    "\n",
    "    # Compute and summarize the confusion matrices for each class across at distinct output values.\n",
    "    auroc = np.zeros(num_classes)\n",
    "    auprc = np.zeros(num_classes)\n",
    "\n",
    "    for k in range(num_classes):\n",
    "        # We only need to compute TPs, FPs, FNs, and TNs at distinct output values.\n",
    "        thresholds = np.unique(outputs[:, k])\n",
    "        thresholds = np.append(thresholds, thresholds[-1]+1)\n",
    "        thresholds = thresholds[::-1]\n",
    "        num_thresholds = len(thresholds)\n",
    "\n",
    "        # Initialize the TPs, FPs, FNs, and TNs.\n",
    "        tp = np.zeros(num_thresholds)\n",
    "        fp = np.zeros(num_thresholds)\n",
    "        fn = np.zeros(num_thresholds)\n",
    "        tn = np.zeros(num_thresholds)\n",
    "        fn[0] = np.sum(labels[:, k] == 1)\n",
    "        tn[0] = np.sum(labels[:, k] == 0)\n",
    "\n",
    "        # Find the indices that result in sorted output values.\n",
    "        idx = np.argsort(outputs[:, k])[::-1]\n",
    "\n",
    "        # Compute the TPs, FPs, FNs, and TNs for class k across thresholds.\n",
    "        i = 0\n",
    "        for j in range(1, num_thresholds):\n",
    "            # Initialize TPs, FPs, FNs, and TNs using values at previous threshold.\n",
    "            tp[j] = tp[j-1]\n",
    "            fp[j] = fp[j-1]\n",
    "            fn[j] = fn[j-1]\n",
    "            tn[j] = tn[j-1]\n",
    "\n",
    "            # Update the TPs, FPs, FNs, and TNs at i-th output value.\n",
    "            while i < num_patients and outputs[idx[i], k] >= thresholds[j]:\n",
    "                if labels[idx[i], k]:\n",
    "                    tp[j] += 1\n",
    "                    fn[j] -= 1\n",
    "                else:\n",
    "                    fp[j] += 1\n",
    "                    tn[j] -= 1\n",
    "                i += 1\n",
    "\n",
    "        # Summarize the TPs, FPs, FNs, and TNs for class k.\n",
    "        tpr = np.zeros(num_thresholds)\n",
    "        tnr = np.zeros(num_thresholds)\n",
    "        ppv = np.zeros(num_thresholds)\n",
    "        for j in range(num_thresholds):\n",
    "            if tp[j] + fn[j]:\n",
    "                tpr[j] = float(tp[j]) / float(tp[j] + fn[j])\n",
    "            else:\n",
    "                tpr[j] = float('nan')\n",
    "            if fp[j] + tn[j]:\n",
    "                tnr[j] = float(tn[j]) / float(fp[j] + tn[j])\n",
    "            else:\n",
    "                tnr[j] = float('nan')\n",
    "            if tp[j] + fp[j]:\n",
    "                ppv[j] = float(tp[j]) / float(tp[j] + fp[j])\n",
    "            else:\n",
    "                ppv[j] = float('nan')\n",
    "\n",
    "        # Compute AUROC as the area under a piecewise linear function with TPR/\n",
    "        # sensitivity (x-axis) and TNR/specificity (y-axis) and AUPRC as the area\n",
    "        # under a piecewise constant with TPR/recall (x-axis) and PPV/precision\n",
    "        # (y-axis) for class k.\n",
    "        for j in range(num_thresholds-1):\n",
    "            auroc[k] += 0.5 * (tpr[j+1] - tpr[j]) * (tnr[j+1] + tnr[j])\n",
    "            auprc[k] += (tpr[j+1] - tpr[j]) * ppv[j+1]\n",
    "\n",
    "    # Compute macro AUROC and macro AUPRC across classes.\n",
    "    if np.any(np.isfinite(auroc)):\n",
    "        macro_auroc = np.nanmean(auroc)\n",
    "    else:\n",
    "        macro_auroc = float('nan')\n",
    "    if np.any(np.isfinite(auprc)):\n",
    "        macro_auprc = np.nanmean(auprc)\n",
    "    else:\n",
    "        macro_auprc = float('nan')\n",
    "\n",
    "    return macro_auroc, macro_auprc, auroc, auprc\n",
    "\n",
    "# Compute a binary confusion matrix, where the columns are the expert labels and the rows are the classifier labels.\n",
    "def compute_confusion_matrix(labels, outputs):\n",
    "    assert(np.shape(labels)[0] == np.shape(outputs)[0])\n",
    "    assert(all(value in (0, 1, True, False) for value in np.unique(labels)))\n",
    "    assert(all(value in (0, 1, True, False) for value in np.unique(outputs)))\n",
    "\n",
    "    num_patients = np.shape(labels)[0]\n",
    "    num_label_classes = np.shape(labels)[1]\n",
    "    num_output_classes = np.shape(outputs)[1]\n",
    "\n",
    "    A = np.zeros((num_output_classes, num_label_classes))\n",
    "    for k in range(num_patients):\n",
    "        for i in range(num_output_classes):\n",
    "            for j in range(num_label_classes):\n",
    "                if outputs[k, i] == 1 and labels[k, j] == 1:\n",
    "                    A[i, j] += 1\n",
    "\n",
    "    return A\n",
    "\n",
    "# Compute binary one-vs-rest confusion matrices, where the columns are the expert labels and the rows are the classifier labels.\n",
    "def compute_one_vs_rest_confusion_matrix(labels, outputs):\n",
    "    assert(np.shape(labels) == np.shape(outputs))\n",
    "    assert(all(value in (0, 1, True, False) for value in np.unique(labels)))\n",
    "    assert(all(value in (0, 1, True, False) for value in np.unique(outputs)))\n",
    "\n",
    "    num_patients, num_classes = np.shape(labels)\n",
    "\n",
    "    A = np.zeros((num_classes, 2, 2))\n",
    "    for i in range(num_patients):\n",
    "        for j in range(num_classes):\n",
    "            if labels[i, j] == 1 and outputs[i, j] == 1: # TP\n",
    "                A[j, 0, 0] += 1\n",
    "            elif labels[i, j] == 0 and outputs[i, j] == 1: # FP\n",
    "                A[j, 0, 1] += 1\n",
    "            elif labels[i, j] == 1 and outputs[i, j] == 0: # FN\n",
    "                A[j, 1, 0] += 1\n",
    "            elif labels[i, j] == 0 and outputs[i, j] == 0: # TN\n",
    "                A[j, 1, 1] += 1\n",
    "\n",
    "    return A\n",
    "\n",
    "# Compute macro F-measure.\n",
    "def compute_f_measure(labels, outputs):\n",
    "    num_patients, num_classes = np.shape(labels)\n",
    "\n",
    "    A = compute_one_vs_rest_confusion_matrix(labels, outputs)\n",
    "\n",
    "    f_measure = np.zeros(num_classes)\n",
    "    for k in range(num_classes):\n",
    "        tp, fp, fn, tn = A[k, 0, 0], A[k, 0, 1], A[k, 1, 0], A[k, 1, 1]\n",
    "        if 2 * tp + fp + fn > 0:\n",
    "            f_measure[k] = float(2 * tp) / float(2 * tp + fp + fn)\n",
    "        else:\n",
    "            f_measure[k] = float('nan')\n",
    "\n",
    "    if np.any(np.isfinite(f_measure)):\n",
    "        macro_f_measure = np.nanmean(f_measure)\n",
    "    else:\n",
    "        macro_f_measure = float('nan')\n",
    "\n",
    "    return macro_f_measure, f_measure\n",
    "\n",
    "# Compute accuracy.\n",
    "def compute_accuracy(labels, outputs):\n",
    "    # Compute confusion matrix.\n",
    "    assert(np.shape(labels) == np.shape(outputs))\n",
    "    num_patients, num_classes = np.shape(labels)\n",
    "    A = compute_confusion_matrix(labels, outputs)\n",
    "\n",
    "    # Compute accuracy.\n",
    "    if np.sum(A) > 0:\n",
    "        accuracy = np.trace(A) / np.sum(A)\n",
    "    else:\n",
    "        accuracy = float('nan')\n",
    "\n",
    "    # Compute per-class accuracy.\n",
    "    accuracy_classes = np.zeros(num_classes)\n",
    "    for i in range(num_classes):\n",
    "        if np.sum(A[:, i]) > 0:\n",
    "            accuracy_classes[i] = A[i, i] / np.sum(A[:, i])\n",
    "        else:\n",
    "            accuracy_classes[i] = float('nan')\n",
    "\n",
    "    return accuracy, accuracy_classes\n",
    "\n",
    "# Compute accuracy.\n",
    "def compute_weighted_accuracy(labels, outputs, classes):\n",
    "    # Define constants.\n",
    "    if classes == ['Present', 'Unknown', 'Absent']:\n",
    "        weights = np.array([[5, 3, 1], [5, 3, 1], [5, 3, 1]])\n",
    "    elif classes == ['Abnormal', 'Normal']:\n",
    "        weights = np.array([[5, 1], [5, 1]])\n",
    "    else:\n",
    "        raise NotImplementedError('Weighted accuracy undefined for classes {}'.format(', '.join(classes)))\n",
    "\n",
    "    # Compute confusion matrix.\n",
    "    assert(np.shape(labels) == np.shape(outputs))\n",
    "    A = compute_confusion_matrix(labels, outputs)\n",
    "\n",
    "    # Multiply the confusion matrix by the weight matrix.\n",
    "    assert(np.shape(A) == np.shape(weights))\n",
    "    B = weights * A\n",
    "\n",
    "    # Compute weighted_accuracy.\n",
    "    if np.sum(B) > 0:\n",
    "        weighted_accuracy = np.trace(B) / np.sum(B)\n",
    "    else:\n",
    "        weighted_accuracy = float('nan')\n",
    "\n",
    "    return weighted_accuracy\n",
    "\n",
    "# Define total cost for algorithmic prescreening of m patients.\n",
    "def cost_algorithm(m):\n",
    "    return 10*m\n",
    "\n",
    "# Define total cost for expert screening of m patients out of a total of n total patients.\n",
    "def cost_expert(m, n):\n",
    "    return (25 + 397*(m/n) -1718*(m/n)**2 + 11296*(m/n)**4) * n\n",
    "\n",
    "# Define total cost for treatment of m patients.\n",
    "def cost_treatment(m):\n",
    "    return 10000*m\n",
    "\n",
    "# Define total cost for missed/late treatement of m patients.\n",
    "def cost_error(m):\n",
    "    return 50000*m\n",
    "\n",
    "# Compute Challenge cost metric.\n",
    "def compute_cost(labels, outputs, label_classes, output_classes):\n",
    "    # Define positive and negative classes for referral and treatment.\n",
    "    positive_classes = ['Present', 'Unknown', 'Abnormal']\n",
    "    negative_classes = ['Absent', 'Normal']\n",
    "\n",
    "    # Compute confusion matrix.\n",
    "    A = compute_confusion_matrix(labels, outputs)\n",
    "\n",
    "    # Identify positive and negative classes for referral.\n",
    "    idx_label_positive = [i for i, x in enumerate(label_classes) if x in positive_classes]\n",
    "    idx_label_negative = [i for i, x in enumerate(label_classes) if x in negative_classes]\n",
    "    idx_output_positive = [i for i, x in enumerate(output_classes) if x in positive_classes]\n",
    "    idx_output_negative = [i for i, x in enumerate(output_classes) if x in negative_classes]\n",
    "\n",
    "    # Identify true positives, false positives, false negatives, and true negatives.\n",
    "    tp = np.sum(A[np.ix_(idx_output_positive, idx_label_positive)])\n",
    "    fp = np.sum(A[np.ix_(idx_output_positive, idx_label_negative)])\n",
    "    fn = np.sum(A[np.ix_(idx_output_negative, idx_label_positive)])\n",
    "    tn = np.sum(A[np.ix_(idx_output_negative, idx_label_negative)])\n",
    "    total_patients = tp + fp + fn + tn\n",
    "\n",
    "    # Compute total cost for all patients.\n",
    "    total_cost = cost_algorithm(total_patients) \\\n",
    "        + cost_expert(tp + fp, total_patients) \\\n",
    "        + cost_treatment(tp) \\\n",
    "        + cost_error(fn)\n",
    "\n",
    "    # Compute mean cost per patient.\n",
    "    if total_patients > 0:\n",
    "        mean_cost = total_cost / total_patients\n",
    "    else:\n",
    "        mean_cost = float('nan')\n",
    "\n",
    "    return mean_cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87148bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models.\n",
    "def evaluate_model(label_folder, output_folder):\n",
    "    # Define murmur and outcome classes.\n",
    "    murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "    outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "    # Load and parse label and model output files.\n",
    "    label_files, output_files = find_challenge_files(label_folder, output_folder)\n",
    "    murmur_labels = load_murmurs(label_files, murmur_classes)\n",
    "    murmur_binary_outputs, murmur_scalar_outputs = load_classifier_outputs(output_files, murmur_classes)\n",
    "    outcome_labels = load_outcomes(label_files, outcome_classes)\n",
    "    outcome_binary_outputs, outcome_scalar_outputs = load_classifier_outputs(output_files, outcome_classes)\n",
    "\n",
    "    # For each patient, set the 'Present' or 'Abnormal' class to positive if no class is positive or if multiple classes are positive.\n",
    "    murmur_labels = enforce_positives(murmur_labels, murmur_classes, 'Present')\n",
    "    murmur_binary_outputs = enforce_positives(murmur_binary_outputs, murmur_classes, 'Present')\n",
    "    outcome_labels = enforce_positives(outcome_labels, outcome_classes, 'Abnormal')\n",
    "    outcome_binary_outputs = enforce_positives(outcome_binary_outputs, outcome_classes, 'Abnormal')\n",
    "\n",
    "    # Evaluate the murmur model by comparing the labels and model outputs.\n",
    "    murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes = compute_auc(murmur_labels, murmur_scalar_outputs)\n",
    "    murmur_f_measure, murmur_f_measure_classes = compute_f_measure(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_accuracy, murmur_accuracy_classes = compute_accuracy(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_weighted_accuracy = compute_weighted_accuracy(murmur_labels, murmur_binary_outputs, murmur_classes) # This is the murmur scoring metric.\n",
    "    murmur_cost = compute_cost(outcome_labels, murmur_binary_outputs, outcome_classes, murmur_classes) # Use *outcomes* to score *murmurs* for the Challenge cost metric, but this is not the actual murmur scoring metric.\n",
    "    murmur_scores = (murmur_classes, murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes, \\\n",
    "        murmur_f_measure, murmur_f_measure_classes, murmur_accuracy, murmur_accuracy_classes, murmur_weighted_accuracy, murmur_cost)\n",
    "\n",
    "    # Evaluate the outcome model by comparing the labels and model outputs.\n",
    "    outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes = compute_auc(outcome_labels, outcome_scalar_outputs)\n",
    "    outcome_f_measure, outcome_f_measure_classes = compute_f_measure(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_accuracy, outcome_accuracy_classes = compute_accuracy(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_weighted_accuracy = compute_weighted_accuracy(outcome_labels, outcome_binary_outputs, outcome_classes)\n",
    "    outcome_cost = compute_cost(outcome_labels, outcome_binary_outputs, outcome_classes, outcome_classes) # This is the clinical outcomes scoring metric.\n",
    "    outcome_scores = (outcome_classes, outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes, \\\n",
    "        outcome_f_measure, outcome_f_measure_classes, outcome_accuracy, outcome_accuracy_classes, outcome_weighted_accuracy, outcome_cost)\n",
    "\n",
    "    # Return the results.\n",
    "    return murmur_scores, outcome_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4866d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "    outcome_classes = ['Abnormal', 'Normal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "047ad5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    label_files, output_files = find_challenge_files(label_folder, output_folder)\n",
    "    murmur_labels = load_murmurs(label_files, murmur_classes)\n",
    "    murmur_binary_outputs, murmur_scalar_outputs = load_classifier_outputs(output_files, murmur_classes)\n",
    "    outcome_labels = load_outcomes(label_files, outcome_classes)\n",
    "    outcome_binary_outputs, outcome_scalar_outputs = load_classifier_outputs(output_files, outcome_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79f664f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ubuntu/data/hmd/murmur/test/33151.txt',\n",
       " '/home/ubuntu/data/hmd/murmur/test/38337.txt',\n",
       " '/home/ubuntu/data/hmd/murmur/test/40798.txt',\n",
       " '/home/ubuntu/data/hmd/murmur/test/49558.txt',\n",
       " '/home/ubuntu/data/hmd/murmur/test/49572.txt']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4aae4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13918 4 4000\r\n",
      "AV 13918_AV.hea 13918_AV.wav 13918_AV.tsv\r\n",
      "PV 13918_PV.hea 13918_PV.wav 13918_PV.tsv\r\n",
      "TV 13918_TV.hea 13918_TV.wav 13918_TV.tsv\r\n",
      "MV 13918_MV.hea 13918_MV.wav 13918_MV.tsv\r\n",
      "#Age: Child\r\n",
      "#Sex: Male\r\n",
      "#Height: 98.0\r\n",
      "#Weight: 15.9\r\n",
      "#Pregnancy status: False\r\n",
      "#Murmur: Present\r\n",
      "#Murmur locations: TV\r\n",
      "#Most audible location: TV\r\n",
      "#Systolic murmur timing: Holosystolic\r\n",
      "#Systolic murmur shape: Plateau\r\n",
      "#Systolic murmur grading: I/VI\r\n",
      "#Systolic murmur pitch: Low\r\n",
      "#Systolic murmur quality: Blowing\r\n",
      "#Diastolic murmur timing: nan\r\n",
      "#Diastolic murmur shape: nan\r\n",
      "#Diastolic murmur grading: nan\r\n",
      "#Diastolic murmur pitch: nan\r\n",
      "#Diastolic murmur quality: nan\r\n",
      "#Outcome: Abnormal\r\n",
      "#Campaign: CC2015\r\n",
      "#Additional ID: nan\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/ubuntu/hmd/notebooks/physionet.org/files/circor-heart-sound/1.0.3/training_data/13918.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47ab986b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ubuntu/python-classifier-2022/test_outputs/33151.csv',\n",
       " '/home/ubuntu/python-classifier-2022/test_outputs/38337.csv',\n",
       " '/home/ubuntu/python-classifier-2022/test_outputs/40798.csv',\n",
       " '/home/ubuntu/python-classifier-2022/test_outputs/49558.csv',\n",
       " '/home/ubuntu/python-classifier-2022/test_outputs/49572.csv']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "317c083e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "murmur_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c4e14bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "murmur_binary_outputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ab834f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0750779 , 0.02301223, 0.9019099 ],\n",
       "       [0.21250477, 0.14293219, 0.644563  ],\n",
       "       [0.17398101, 0.04562256, 0.78039646],\n",
       "       [0.2851596 , 0.13013388, 0.58470654],\n",
       "       [0.14846756, 0.04175028, 0.80978215]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "murmur_scalar_outputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c1927f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False],\n",
       "       [ True, False],\n",
       "       [ True, False],\n",
       "       [ True, False],\n",
       "       [False,  True]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c614797d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True],\n",
       "       [ True, False],\n",
       "       [False,  True],\n",
       "       [ True, False],\n",
       "       [False,  True]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_binary_outputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a21f088a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38914064, 0.61085933],\n",
       "       [0.55287117, 0.44712883],\n",
       "       [0.4957005 , 0.50429946],\n",
       "       [0.5154592 , 0.48454085],\n",
       "       [0.37166628, 0.6283337 ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_scalar_outputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "36df78f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # For each patient, set the 'Present' or 'Abnormal' class to positive if no class is positive or if multiple classes are positive.\n",
    "    murmur_labels = enforce_positives(murmur_labels, murmur_classes, 'Present')\n",
    "    murmur_binary_outputs = enforce_positives(murmur_binary_outputs, murmur_classes, 'Present')\n",
    "    outcome_labels = enforce_positives(outcome_labels, outcome_classes, 'Abnormal')\n",
    "    outcome_binary_outputs = enforce_positives(outcome_binary_outputs, outcome_classes, 'Abnormal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36975c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Evaluate the murmur model by comparing the labels and model outputs.\n",
    "    murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes = compute_auc(murmur_labels, murmur_scalar_outputs)\n",
    "    murmur_f_measure, murmur_f_measure_classes = compute_f_measure(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_accuracy, murmur_accuracy_classes = compute_accuracy(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_weighted_accuracy = compute_weighted_accuracy(murmur_labels, murmur_binary_outputs, murmur_classes) # This is the murmur scoring metric.\n",
    "    murmur_cost = compute_cost(outcome_labels, murmur_binary_outputs, outcome_classes, murmur_classes) # Use *outcomes* to score *murmurs* for the Challenge cost metric, but this is not the actual murmur scoring metric.\n",
    "    murmur_scores = (murmur_classes, murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes, \\\n",
    "        murmur_f_measure, murmur_f_measure_classes, murmur_accuracy, murmur_accuracy_classes, murmur_weighted_accuracy, murmur_cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c492cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Evaluate the outcome model by comparing the labels and model outputs.\n",
    "    outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes = compute_auc(outcome_labels, outcome_scalar_outputs)\n",
    "    outcome_f_measure, outcome_f_measure_classes = compute_f_measure(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_accuracy, outcome_accuracy_classes = compute_accuracy(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_weighted_accuracy = compute_weighted_accuracy(outcome_labels, outcome_binary_outputs, outcome_classes)\n",
    "    outcome_cost = compute_cost(outcome_labels, outcome_binary_outputs, outcome_classes, outcome_classes) # This is the clinical outcomes scoring metric.\n",
    "    outcome_scores = (outcome_classes, outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes, \\\n",
    "        outcome_f_measure, outcome_f_measure_classes, outcome_accuracy, outcome_accuracy_classes, outcome_weighted_accuracy, outcome_cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "51ed14cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Present', 'Unknown', 'Absent'],\n",
       " 0.6687541810011396,\n",
       " 0.4616880157631656,\n",
       " array([0.58548332, 0.78450363, 0.63627559]),\n",
       " array([0.3440901 , 0.24738511, 0.79358883]),\n",
       " 0.30998211091234346,\n",
       " array([0.09302326, 0.        , 0.83692308]),\n",
       " 0.7225130890052356,\n",
       " array([0.05263158, 0.        , 0.97841727]),\n",
       " 0.3935309973045822,\n",
       " 24860.97457650275)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "murmur_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e5f8e344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Abnormal', 'Normal'],\n",
       " 0.6899275839368009,\n",
       " 0.6944868075200309,\n",
       " array([0.68992758, 0.68992758]),\n",
       " array([0.70365504, 0.68531858]),\n",
       " 0.6124698398771661,\n",
       " array([0.60638298, 0.6185567 ]),\n",
       " 0.612565445026178,\n",
       " array([0.58163265, 0.64516129]),\n",
       " 0.5917667238421955,\n",
       " 14114.771718225249)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "22b195e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55c0096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "        ','.join(classes),\n",
    "        ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "        ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "        ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "        ','.join('{:.3f}'.format(x) for x in accuracy_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "708a74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "    outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "        ','.join(classes),\n",
    "        ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "        ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "        ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "        ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "        + '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1a49d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Murmur scores\\nAUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n0.669,0.462,0.310,0.723,0.394,24860.975\\n\\n#Outcome scores\\nAUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n0.690,0.694,0.612,0.613,0.592,14114.772\\n\\n#Murmur scores (per class)\\nClasses,Present,Unknown,Absent\\nAUROC,0.585,0.785,0.636\\nAUPRC,0.344,0.247,0.794\\nF-measure,0.093,0.000,0.837\\nAccuracy,0.053,0.000,0.978\\n\\n#Outcome scores (per class)\\nClasses,Abnormal,Normal\\nAUROC,0.690,0.690\\nAUPRC,0.704,0.685\\nF-measure,0.606,0.619\\nAccuracy,0.582,0.645\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "07037319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.462,0.310,0.723,0.394,24860.975\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.690,0.694,0.612,0.613,0.592,14114.772\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.585,0.785,0.636\n",
      "AUPRC,0.344,0.247,0.794\n",
      "F-measure,0.093,0.000,0.837\n",
      "Accuracy,0.053,0.000,0.978\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.690,0.690\n",
      "AUPRC,0.704,0.685\n",
      "F-measure,0.606,0.619\n",
      "Accuracy,0.582,0.645\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c06f92f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2.4_p37)",
   "language": "python",
   "name": "conda_tensorflow2.4_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding features\n",
    "\n",
    "mel-spec, stft, cqt 피쳐 추가 고려하는버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import librosa\n",
    "import librosa.display\n",
    "import math\n",
    "import sys\n",
    "#sys.path.insert(0,'/home/ikwak2/hmd/notebooks')\n",
    "#sys.path.insert(0,'/home/ikwak2/hmd/iy_classifier')\n",
    "sys.path.insert(0,'utils')\n",
    "from helper_code import *\n",
    "from get_feature import *\n",
    "from models import *\n",
    "from Generator0 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'physionet.org/files/circor-heart-sound/1.0.3'\n",
    "training_data_file = root_dir + '/' + 'training_data.csv'\n",
    "training_data_dir = root_dir + '/' + 'training_data'\n",
    "model_dir = root_dir + '/' + 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help` for usage information.\n",
      "\n",
      "\n",
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help` for usage information.\n",
      "\n",
      "\n",
      "Sat Jul  9 09:51:49 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.80       Driver Version: 460.80       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 35%   35C    P8    12W / 260W |    288MiB / 11011MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       992      G   /usr/lib/xorg/Xorg                 18MiB |\n",
      "|    0   N/A  N/A      1110      G   /usr/bin/gnome-shell               72MiB |\n",
      "|    0   N/A  N/A      1352      G   /usr/lib/xorg/Xorg                146MiB |\n",
      "|    0   N/A  N/A      1472      G   /usr/bin/gnome-shell               26MiB |\n",
      "|    0   N/A  N/A      2063      G   /usr/lib/firefox/firefox           12MiB |\n",
      "|    0   N/A  N/A      2416      G   gnome-control-center                3MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder =  'physionet.org/files/circor-heart-sound/1.0.3/training_data'\n",
    "train_folder =  '/home/ubuntu/data/hmd/murmur/train'\n",
    "test_folder = '/home/ubuntu/data/hmd/murmur/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = 'tmp_model4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidal_decay(e, start=0, end=100, lr_start=1e-3, lr_end=1e-5):\n",
    "    if e < start:\n",
    "        return lr_start\n",
    "    elif e > end:\n",
    "        return lr_end\n",
    "\n",
    "    middle = (start + end) / 2\n",
    "    s = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return s(13 * (-e + middle) / np.abs(end - start)) * np.abs(lr_start - lr_end) + lr_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get feature 함수확장: 음성피쳐 옵션들과, 추가 음성들 고려한 피쳐추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_3lb_all(data_folder, patient_files_trn, \n",
    "                          samp_sec=20, pre_emphasis = 0, hop_length=256, win_length = 512, n_mels = 100,\n",
    "                          filter_scale = 1, n_bins = 80, fmin = 10\n",
    "                         ) :\n",
    "    features = dict()\n",
    "    features['id'] = []\n",
    "    features['age'] = []\n",
    "    features['sex'] = []\n",
    "    features['hw'] = []\n",
    "    features['preg'] = []\n",
    "    features['loc'] = []\n",
    "    features['mel1'] = []\n",
    "    features['cqt1'] = []\n",
    "    features['stft1'] = []\n",
    "#    labels = []\n",
    "    mm_labels = []\n",
    "    out_labels = []\n",
    "\n",
    "    age_classes = ['Neonate', 'Infant', 'Child', 'Adolescent', 'Young Adult']\n",
    "    recording_locations = ['AV', 'MV', 'PV', 'TV', 'PhC']\n",
    "\n",
    "    num_patient_files = len(patient_files_trn)\n",
    "\n",
    "    for i in range(num_patient_files):\n",
    "\n",
    "        # Load the current patient data and recordings.\n",
    "        current_patient_data = load_patient_data(patient_files_trn[i])\n",
    "        num_locations = get_num_locations(current_patient_data)\n",
    "        recording_information = current_patient_data.split('\\n')[1:num_locations+1]\n",
    "        for j in range(num_locations) :\n",
    "            entries = recording_information[j].split(' ')\n",
    "            recording_file = entries[2]\n",
    "            filename = os.path.join(data_folder, recording_file)\n",
    "\n",
    "            # Extract id\n",
    "            id1 = recording_file.split('_')[0]\n",
    "            features['id'].append(id1)\n",
    "\n",
    "            # Extract melspec\n",
    "            mel1 = feature_extract_melspec(filename, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length, \n",
    "                                           win_length = win_length, n_mels = n_mels)[0]\n",
    "            features['mel1'].append(mel1)\n",
    "            mel2 = feature_extract_cqt(filename, samp_sec=samp_sec, pre_emphasis = pre_emphasis, filter_scale = filter_scale, \n",
    "                                        n_bins = n_bins, fmin = fmin)[0]\n",
    "            features['cqt1'].append(mel2)\n",
    "            mel3 = feature_extract_stft(filename, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length, \n",
    "                                       win_length = win_length)[0]\n",
    "            features['stft1'].append(mel3)\n",
    "            \n",
    "            # Extract age_group\n",
    "            age_group = get_age(current_patient_data)\n",
    "            current_age_group = np.zeros(6, dtype=int)\n",
    "            if age_group in age_classes:\n",
    "                j = age_classes.index(age_group)\n",
    "                current_age_group[j] = 1\n",
    "            else :\n",
    "                current_age_group[5] = 1\n",
    "            features['age'].append(current_age_group)\n",
    "\n",
    "            # Extract sex\n",
    "            sex = get_sex(current_patient_data)\n",
    "            sex_features = np.zeros(2, dtype=int)\n",
    "            if compare_strings(sex, 'Female'):\n",
    "                sex_features[0] = 1\n",
    "            elif compare_strings(sex, 'Male'):\n",
    "                sex_features[1] = 1\n",
    "            features['sex'].append(sex_features)\n",
    "\n",
    "            # Extract height and weight.\n",
    "            height = get_height(current_patient_data)\n",
    "            weight = get_weight(current_patient_data)\n",
    "            ## simple impute\n",
    "            if math.isnan(height) :\n",
    "                height = 110.846\n",
    "            if math.isnan(weight) :\n",
    "                weight = 23.767\n",
    "                \n",
    "            features['hw'].append(np.array([height, weight]))\n",
    "\n",
    "            # Extract pregnancy\n",
    "            is_pregnant = get_pregnancy_status(current_patient_data)\n",
    "            features['preg'].append(is_pregnant)\n",
    "\n",
    "            # Extract location\n",
    "            locations = entries[0]\n",
    "            num_recording_locations = len(recording_locations)\n",
    "            loc_features = np.zeros(num_recording_locations)\n",
    "            if locations in recording_locations:\n",
    "                j = recording_locations.index(locations)\n",
    "                loc_features[j] = 1\n",
    "            features['loc'].append(loc_features)\n",
    "\n",
    "            # Extract labels \n",
    "            mm_label = get_murmur(current_patient_data)\n",
    "            out_label = get_outcome(current_patient_data)\n",
    "            current_mm_labels = np.zeros(2)\n",
    "            current_out_labels = np.zeros(2)\n",
    "            if mm_label == 'Absent' :\n",
    "                current_mm_labels = np.array([0, 0, 1])\n",
    "            elif mm_label == 'unknown' :\n",
    "                current_mm_labels = np.array([0, 1, 0])\n",
    "            else :\n",
    "                mm_loc = get_murmur_loc(current_patient_data)\n",
    "                if mm_loc == 'nan' :\n",
    "                    current_mm_labels = np.array([0.9, 0.05, 0.05])\n",
    "                else :\n",
    "                    mm_loc = mm_loc.split('+')\n",
    "                    if locations in mm_loc :\n",
    "                        current_mm_labels = np.array([1, 0, 0])\n",
    "                    else :\n",
    "                        current_mm_labels = np.array([0.7, 0.2, 0.1])\n",
    "\n",
    "            if out_label == 'Normal' :\n",
    "                current_out_labels = np.array([0, 1])\n",
    "            else :\n",
    "                current_out_labels = np.array([1, 0])\n",
    "#                if mm_label == 'Absent' :\n",
    "#                    current_out_labels = np.array([0.8, 0.2])\n",
    "#                elif mm_label == 'unknown' :\n",
    "#                    current_out_labels = np.array([0.85, 0.15])\n",
    "#                else :\n",
    "#                    current_out_labels = np.array([1, 0])\n",
    "                \n",
    "            mm_labels.append(current_mm_labels)\n",
    "            out_labels.append(current_out_labels)\n",
    "\n",
    "    M, N = features['mel1'][i].shape\n",
    "    for i in range(len(features['mel1'])) :\n",
    "        features['mel1'][i] = features['mel1'][i].reshape(M,N,1)\n",
    "    print(\"melspec: \", M,N)\n",
    "    mel_input_shape = (M,N,1)\n",
    "        \n",
    "    M, N = features['cqt1'][i].shape\n",
    "    for i in range(len(features['cqt1'])) :\n",
    "        features['cqt1'][i] = features['cqt1'][i].reshape(M,N,1)\n",
    "    print(\"cqt: \", M,N)\n",
    "    cqt_input_shape = (M,N,1)\n",
    "\n",
    "    M, N = features['stft1'][i].shape\n",
    "    for i in range(len(features['stft1'])) :\n",
    "        features['stft1'][i] = features['stft1'][i].reshape(M,N,1)\n",
    "    print(\"stft: \", M,N)\n",
    "    stft_input_shape = (M,N,1)\n",
    "        \n",
    "    for k1 in features.keys() :\n",
    "        features[k1] = np.array(features[k1])\n",
    "    \n",
    "    mm_labels = np.array(mm_labels)\n",
    "    out_labels = np.array(out_labels)\n",
    "    return features, mm_labels, out_labels, mel_input_shape, cqt_input_shape, stft_input_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_files_trn = find_patient_files(train_folder)\n",
    "patient_files_test = find_patient_files(test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_feature = {'samp_sec': 20,\n",
    "                  #### melspec, stft 피쳐 옵션들  \n",
    "                  'pre_emphasis': 0,\n",
    "                  'hop_length': 256,\n",
    "                  'win_length':512,\n",
    "                  'n_mels': 100,\n",
    "                  #### cqt 피쳐 옵션들  \n",
    "                  'filter_scale': 1,\n",
    "                  'n_bins': 80,\n",
    "                  'fmin': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melspec:  100 313\n",
      "cqt:  80 157\n",
      "stft:  257 313\n",
      "melspec:  100 313\n",
      "cqt:  80 157\n",
      "stft:  257 313\n"
     ]
    }
   ],
   "source": [
    "features_trn, mm_lbs_trn, out_lbs_trn, mel_input_shape, cqt_input_shape, stft_input_shape = get_features_3lb_all(train_folder, patient_files_trn, **params_feature)\n",
    "features_test, mm_lbs_test, out_lbs_test, _, _, _ = get_features_3lb_all(test_folder, patient_files_test, **params_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 313, 1) (80, 157, 1) (257, 313, 1)\n"
     ]
    }
   ],
   "source": [
    "print(mel_input_shape,cqt_input_shape,stft_input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 음성 피쳐 여러개 고려한 모형 함수 짜기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toy5_1(mel_input_shape, cqt_input_shape, stft_input_shape, use_mel = True, use_cqt = True, use_stft = True):\n",
    "        # Create a towy model.\n",
    "    age = keras.Input(shape=(6,), name = 'age_cat')\n",
    "    sex = keras.Input(shape=(2,), name = 'sex_cat')\n",
    "    hw = keras.Input(shape=(2,), name = 'height_weight')\n",
    "    preg = keras.Input(shape=(1,), name = 'is_preg')\n",
    "    loc = keras.Input(shape=(5,), name = 'loc')\n",
    "    mel1 = keras.Input(shape=(mel_input_shape), name = 'mel')\n",
    "    cqt1 = keras.Input(shape=(cqt_input_shape), name = 'cqt')\n",
    "    stft1 = keras.Input(shape=(stft_input_shape), name = 'stft')\n",
    "        \n",
    "        \n",
    "    ## age embeddig\n",
    "    age1 = layers.Dense(2, activation = None)(age)\n",
    "\n",
    "    ## sex embedding\n",
    "    sex1 = layers.Dense(1, activation = None)(sex)\n",
    "\n",
    "    ## hw embedding\n",
    "    hw1 = layers.Dense(1, activation = None)(hw)\n",
    "\n",
    "    ## loc embedding\n",
    "    loc1 = layers.Dense(3, activation = None)(loc)\n",
    "\n",
    "    ## mel embedding\n",
    "    mel2 = layers.Conv2D(16, (3,3), activation = 'relu')(mel1)\n",
    "    mel2 = layers.MaxPooling2D()(mel2)\n",
    "    mel2 = layers.Conv2D(32, (5,5), activation = 'relu')(mel2)\n",
    "    mel2 = layers.MaxPooling2D()(mel2)\n",
    "    mel2 = layers.Conv2D(32, (3,3), activation = 'relu')(mel2)\n",
    "    mel2 = layers.MaxPooling2D()(mel2)\n",
    "    mel2 = layers.Conv2D(64, (3,3), activation = 'relu')(mel2)\n",
    "    mel2 = layers.MaxPooling2D()(mel2)\n",
    "    mel2 = layers.GlobalAveragePooling2D()(mel2)\n",
    "\n",
    "    ## cqt embedding\n",
    "    cqt2 = layers.Conv2D(16, (3,3), activation = 'relu')(cqt1)\n",
    "    cqt2 = layers.MaxPooling2D()(cqt2)\n",
    "    cqt2 = layers.Conv2D(32, (5,5), activation = 'relu')(cqt2)\n",
    "    cqt2 = layers.MaxPooling2D()(cqt2)\n",
    "    cqt2 = layers.Conv2D(32, (3,3), activation = 'relu')(cqt2)\n",
    "    cqt2 = layers.MaxPooling2D()(cqt2)\n",
    "    cqt2 = layers.Conv2D(64, (3,3), activation = 'relu')(cqt2)\n",
    "    cqt2 = layers.MaxPooling2D()(cqt2)\n",
    "    cqt2 = layers.GlobalAveragePooling2D()(cqt2)\n",
    "\n",
    "    ## stft embedding\n",
    "    stft2 = layers.Conv2D(16, (3,3), activation = 'relu')(stft1)\n",
    "    stft2 = layers.MaxPooling2D()(stft2)\n",
    "    stft2 = layers.Conv2D(32, (5,5), activation = 'relu')(stft2)\n",
    "    stft2 = layers.MaxPooling2D()(stft2)\n",
    "    stft2 = layers.Conv2D(32, (3,3), activation = 'relu')(stft2)\n",
    "    stft2 = layers.MaxPooling2D()(stft2)\n",
    "    stft2 = layers.Conv2D(64, (3,3), activation = 'relu')(stft2)\n",
    "    stft2 = layers.MaxPooling2D()(stft2)\n",
    "    stft2 = layers.GlobalAveragePooling2D()(stft2)\n",
    "    \n",
    "#    concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg])\n",
    "#    d1 = layers.Dense(2, activation = 'relu')(concat1)\n",
    "    \n",
    "    if use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2, stft2])\n",
    "    if not use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([cqt2, stft2])\n",
    "    if use_mel and not use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, stft2])\n",
    "    if use_mel and use_cqt and not use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2])\n",
    "    if not use_mel and not use_cqt and use_stft :  ## stft 만\n",
    "        concat2 = layers.Concatenate()([stft2])\n",
    "    if use_mel and not use_cqt and not use_stft :  ### mel만\n",
    "        concat2 = layers.Concatenate()([mel2])\n",
    "    if not use_mel and use_cqt and not use_stft :  ### cqt만\n",
    "        concat2 = layers.Concatenate()([cqt2])\n",
    "#    concat2 = layers.Dense(10, activation = 'relu')(concat2)\n",
    "    res1 = layers.Dense(3, activation = \"softmax\")(concat2)\n",
    "    res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "\n",
    "    model = keras.Model(inputs = [age,sex,hw,preg,loc,mel1,cqt1,stft1] , outputs = res1 )\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy'])\n",
    "    return(model)\n",
    "\n",
    "def get_toy5_2(mel_input_shape, cqt_input_shape, stft_input_shape, use_mel = True, use_cqt = True, use_stft = True):\n",
    "        # Create a towy model.\n",
    "    age = keras.Input(shape=(6,), name = 'age_cat')\n",
    "    sex = keras.Input(shape=(2,), name = 'sex_cat')\n",
    "    hw = keras.Input(shape=(2,), name = 'height_weight')\n",
    "    preg = keras.Input(shape=(1,), name = 'is_preg')\n",
    "    loc = keras.Input(shape=(5,), name = 'loc')\n",
    "    mel1 = keras.Input(shape=(mel_input_shape), name = 'mel')\n",
    "    cqt1 = keras.Input(shape=(cqt_input_shape), name = 'cqt')\n",
    "    stft1 = keras.Input(shape=(stft_input_shape), name = 'stft')\n",
    "        \n",
    "        \n",
    "    ## age embeddig\n",
    "    age1 = layers.Dense(2, activation = None)(age)\n",
    "\n",
    "    ## sex embedding\n",
    "    sex1 = layers.Dense(1, activation = None)(sex)\n",
    "\n",
    "    ## hw embedding\n",
    "    hw1 = layers.Dense(1, activation = None)(hw)\n",
    "\n",
    "    ## loc embedding\n",
    "    loc1 = layers.Dense(3, activation = None)(loc)\n",
    "\n",
    "    ## mel embedding\n",
    "    mel2 = layers.Conv2D(16, (3,3), activation = 'relu')(mel1)\n",
    "    mel2 = layers.MaxPooling2D()(mel2)\n",
    "    mel2 = layers.Conv2D(32, (5,5), activation = 'relu')(mel2)\n",
    "    mel2 = layers.MaxPooling2D()(mel2)\n",
    "    mel2 = layers.Conv2D(32, (3,3), activation = 'relu')(mel2)\n",
    "    mel2 = layers.MaxPooling2D()(mel2)\n",
    "    mel2 = layers.Conv2D(64, (3,3), activation = 'relu')(mel2)\n",
    "    mel2 = layers.MaxPooling2D()(mel2)\n",
    "    mel2 = layers.GlobalAveragePooling2D()(mel2)\n",
    "\n",
    "    ## cqt embedding\n",
    "    cqt2 = layers.Conv2D(16, (3,3), activation = 'relu')(cqt1)\n",
    "    cqt2 = layers.MaxPooling2D()(cqt2)\n",
    "    cqt2 = layers.Conv2D(32, (5,5), activation = 'relu')(cqt2)\n",
    "    cqt2 = layers.MaxPooling2D()(cqt2)\n",
    "    cqt2 = layers.Conv2D(32, (3,3), activation = 'relu')(cqt2)\n",
    "    cqt2 = layers.MaxPooling2D()(cqt2)\n",
    "    cqt2 = layers.Conv2D(64, (3,3), activation = 'relu')(cqt2)\n",
    "    cqt2 = layers.MaxPooling2D()(cqt2)\n",
    "    cqt2 = layers.GlobalAveragePooling2D()(cqt2)\n",
    "\n",
    "    ## stft embedding\n",
    "    stft2 = layers.Conv2D(16, (3,3), activation = 'relu')(stft1)\n",
    "    stft2 = layers.MaxPooling2D()(stft2)\n",
    "    stft2 = layers.Conv2D(32, (5,5), activation = 'relu')(stft2)\n",
    "    stft2 = layers.MaxPooling2D()(stft2)\n",
    "    stft2 = layers.Conv2D(32, (3,3), activation = 'relu')(stft2)\n",
    "    stft2 = layers.MaxPooling2D()(stft2)\n",
    "    stft2 = layers.Conv2D(64, (3,3), activation = 'relu')(stft2)\n",
    "    stft2 = layers.MaxPooling2D()(stft2)\n",
    "    stft2 = layers.GlobalAveragePooling2D()(stft2)\n",
    "    \n",
    "#    concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg])\n",
    "#    d1 = layers.Dense(2, activation = 'relu')(concat1)\n",
    "    \n",
    "    if use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2, stft2])\n",
    "    if not use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([cqt2, stft2])\n",
    "    if use_mel and not use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, stft2])\n",
    "    if use_mel and use_cqt and not use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2])\n",
    "    if not use_mel and not use_cqt and use_stft :  ## stft 만\n",
    "        concat2 = layers.Concatenate()([stft2])\n",
    "    if use_mel and not use_cqt and not use_stft :  ### mel만\n",
    "        concat2 = layers.Concatenate()([mel2])\n",
    "    if not use_mel and use_cqt and not use_stft :  ### cqt만\n",
    "        concat2 = layers.Concatenate()([cqt2])\n",
    "#    concat2 = layers.Dense(10, activation = 'relu')(concat2)\n",
    "    res1 = layers.Dense(3, activation = \"softmax\")(concat2)\n",
    "    res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "\n",
    "    model = keras.Model(inputs = [age,sex,hw,preg,loc,mel1,cqt1,stft1] , outputs = res2 )\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = get_toy5_1(mel_input_shape, cqt_input_shape, stft_input_shape)\n",
    "model2 = get_toy5_2(mel_input_shape, cqt_input_shape, stft_input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch2로 돌아가는지 정도만 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "40/40 [==============================] - 65s 2s/step - loss: 0.8673 - accuracy: 0.7348 - val_loss: 0.5885 - val_accuracy: 0.7480 - lr: 9.9851e-04\n",
      "Epoch 2/2\n",
      "40/40 [==============================] - 65s 2s/step - loss: 0.5706 - accuracy: 0.7684 - val_loss: 0.5770 - val_accuracy: 0.7512 - lr: 5.0500e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4745fb3828>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epoch = 2\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "batch_size = 64\n",
    "params = {'batch_size': batch_size,\n",
    "#          'input_shape': (100, 313, 1),\n",
    "          'shuffle': True,\n",
    "          'beta_param': 0.7,\n",
    "          'mixup': True,\n",
    "#          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "#          'highpass': [.5, [78,79,80,81,82,83,84,85]]\n",
    "          'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "#           'dropblock' : [30, 100]\n",
    "          #'device' : device\n",
    "}\n",
    "\n",
    "params_no_shuffle = {'batch_size': batch_size,\n",
    "#          'input_shape': (100, 313, 1),\n",
    "          'shuffle': False,\n",
    "          'beta_param': 0.7,\n",
    "          'mixup': False\n",
    "          #'device': device\n",
    "}\n",
    "\n",
    "TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], features_trn['preg'], features_trn['loc'], \n",
    "           features_trn['mel1'],features_trn['cqt1'],features_trn['stft1']], \n",
    "                        mm_lbs_trn,  ## our Y\n",
    "                        **params)()\n",
    "\n",
    "#ValDGen_1 = Generator0([features_test[0]['age'],features_test[0]['sex'], features_test[0]['hw'], features_test[0]['preg'], features_test[0]['loc'], \n",
    "#           features_test[0]['mel1'],features_test[0]['cqt1'],features_test[0]['stft1']], \n",
    "#                        mm_lbs_test,  ## our Y\n",
    "#                        **params_no_shuffle)()\n",
    "\n",
    "    \n",
    "model1.fit(TrainDGen_1,\n",
    "          validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                              features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                              features_test['cqt1'], features_test['stft1']], \n",
    "                             mm_lbs_test), \n",
    "                             callbacks=[lr],\n",
    "                              steps_per_epoch=np.ceil(len(mm_lbs_trn)/64),\n",
    "                             epochs = n_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch2로 돌아가는지 정도만 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "40/40 [==============================] - 64s 2s/step - loss: 0.7594 - accuracy: 0.5207 - val_loss: 0.8166 - val_accuracy: 0.5182 - lr: 9.9851e-04\n",
      "Epoch 2/2\n",
      " 8/40 [=====>........................] - ETA: 43s - loss: 0.6819 - accuracy: 0.5586"
     ]
    }
   ],
   "source": [
    "n_epoch = 2\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "batch_size = 64\n",
    "params = {'batch_size': batch_size,\n",
    "#          'input_shape': (100, 313, 1),\n",
    "          'shuffle': True,\n",
    "          'beta_param': 0.7,\n",
    "          'mixup': True,\n",
    "#          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "          'highpass': [.5, [78,79,80,81,82,83,84,85]],\n",
    "          'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "#           'dropblock' : [30, 100]\n",
    "          #'device' : device\n",
    "}\n",
    "\n",
    "params_no_shuffle = {'batch_size': batch_size,\n",
    "#          'input_shape': (100, 313, 1),\n",
    "          'shuffle': False,\n",
    "          'beta_param': 0.7,\n",
    "          'mixup': False\n",
    "          #'device': device\n",
    "}\n",
    "\n",
    "TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], features_trn['preg'], features_trn['loc'], \n",
    "           features_trn['mel1'],features_trn['cqt1'],features_trn['stft1']], \n",
    "                        out_lbs_trn,  ## our Y\n",
    "                        **params)()\n",
    "\n",
    "#ValDGen_1 = Generator0([features_test[0]['age'],features_test[0]['sex'], features_test[0]['hw'], features_test[0]['preg'], features_test[0]['loc'], \n",
    "#           features_test[0]['mel1'],features_test[0]['cqt1'],features_test[0]['stft1']], \n",
    "#                        features_test[2],  ## our Y\n",
    "#                        **params_no_shuffle)()\n",
    "\n",
    "    \n",
    "model2.fit(TrainDGen_1,\n",
    "          validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                              features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                              features_test['cqt1'], features_test['stft1']], \n",
    "                             out_lbs_test), \n",
    "                             callbacks=[lr],\n",
    "                              steps_per_epoch=np.ceil(len(out_lbs_trn)/64),\n",
    "                             epochs = n_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 팀코드 수정해야 할 함수들.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기부터 더 짜야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'samp_sec': 20,\n",
       " 'pre_emphasis': 0,\n",
       " 'hop_length': 256,\n",
       " 'win_length': 512,\n",
       " 'n_mels': 100,\n",
       " 'filter_scale': 1,\n",
       " 'n_bins': 80,\n",
       " 'fmin': 10}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "def save_challenge_model(model_folder, model1, model2, m_name1, m_name2, mel_shape = (100, 313, 1)) :\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    filename1 = os.path.join(model_folder, m_name1 + '_model1.hdf5')\n",
    "    filename2 = os.path.join(model_folder, m_name2 + '_model2.hdf5')\n",
    "    model1.save(filename1)\n",
    "    model2.save(filename2)\n",
    "    d = {'model1': m_name1, 'model2': m_name2, 'mel_shape': mel_shape, 'model_fnm1': filename1, 'model_fnm2': filename2}    \n",
    "    with open(info_fnm, 'wb') as f:\n",
    "        pk.dump(d, f, pk.HIGHEST_PROTOCOL)\n",
    "    return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "def save_challenge_model(model_folder, model1, model2, m_name1, m_name2, mel_shape = (100, 313, 1)) :\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    filename1 = os.path.join(model_folder, m_name1 + '_model1.hdf5')\n",
    "    filename2 = os.path.join(model_folder, m_name2 + '_model2.hdf5')\n",
    "    model1.save(filename1)\n",
    "    model2.save(filename2)\n",
    "    d = {'model1': m_name1, 'model2': m_name2, 'mel_shape': mel_shape, 'model_fnm1': filename1, 'model_fnm2': filename2}    \n",
    "    with open(info_fnm, 'wb') as f:\n",
    "        pk.dump(d, f, pk.HIGHEST_PROTOCOL)\n",
    "    return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp_model4'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_challenge_model(model_folder, model1, model2, m_name1 = 'toy1', m_name2 = 'toy2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_challenge_model(model_folder, verbose):\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    with open(info_fnm, 'rb') as f:\n",
    "        info_m = pk.load(f)\n",
    "#    if info_m['model'] == 'toy' :\n",
    "#        model = get_toy(info_m['mel_shape'])\n",
    "#    filename = os.path.join(model_folder, info_m['model'] + '_model.hdf5')\n",
    "#    model.load_weights(filename)\n",
    "    return info_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_challenge_model(model, data, recordings, verbose):\n",
    "    \n",
    "    murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "    outcome_classes = ['Abnormal', 'Normal']\n",
    "    \n",
    "    if model['model1'] == 'toy1' :\n",
    "        model1 = get_toy4_1(model['mel_shape'])\n",
    "    if model['model2'] == 'toy2' :\n",
    "        model2 = get_toy4_2(model['mel_shape'])\n",
    "    model1.load_weights(model['model_fnm1'])\n",
    "    model2.load_weights(model['model_fnm2'])\n",
    "    \n",
    "#    classes = model['classes']\n",
    "    # Load features.\n",
    "    features = get_feature_one(data, verbose = 0)\n",
    "\n",
    "    features['mel1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        mel1 = feature_extract_melspec(recordings[i]/ 32768)[0]\n",
    "        features['mel1'].append(mel1)\n",
    "\n",
    "    M, N = features['mel1'][0].shape\n",
    "    for i in range(len(features['mel1'])) :\n",
    "        features['mel1'][i] = features['mel1'][i].reshape(M,N,1)   \n",
    "        \n",
    "    features['mel1'] = np.array(features['mel1'])\n",
    "#    print(features)\n",
    "    # Impute missing data.\n",
    "    res1 = model1.predict([features['age'], features['sex'], features['hw'], features['preg'], features['loc'], features['mel1']])\n",
    "    res2 = model2.predict([features['age'], features['sex'], features['hw'], features['preg'], features['loc'], features['mel1']])\n",
    "\n",
    "    # Get classifier probabilities.\n",
    "    idx1 = res1.argmax(axis=0)[0]\n",
    "    murmur_probabilities = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "    outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "#    idx = np.argmax(prob1)\n",
    "\n",
    "    ## 이부분도 생각 필요.. rule 을 cost를 maximize 하는 기준으로 threshold 탐색 필요할지도..\n",
    "    # Choose label with highest probability.\n",
    "    murmur_labels = np.zeros(len(murmur_classes), dtype=np.int_)\n",
    "    idx = np.argmax(murmur_probabilities)\n",
    "    murmur_labels[idx] = 1\n",
    "    outcome_labels = np.zeros(len(outcome_classes), dtype=np.int_)\n",
    "    idx = np.argmax(outcome_probabilities)\n",
    "    outcome_labels[idx] = 1\n",
    "    \n",
    "    # Concatenate classes, labels, and probabilities.\n",
    "    classes = murmur_classes + outcome_classes\n",
    "    labels = np.concatenate((murmur_labels, outcome_labels))\n",
    "    probabilities = np.concatenate((murmur_probabilities, outcome_probabilities))\n",
    "    \n",
    "    return classes, labels, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대회 평가용 run_model 함수 (수정 불필요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model.\n",
    "def run_model(model_folder, data_folder, output_folder, allow_failures, verbose):\n",
    "    # Load models.\n",
    "    if verbose >= 1:\n",
    "        print('Loading Challenge model...')\n",
    "\n",
    "    model = load_challenge_model(model_folder, verbose) ### Teams: Implement this function!!!\n",
    "\n",
    "    # Find the patient data files.\n",
    "    patient_files = find_patient_files(data_folder)\n",
    "    num_patient_files = len(patient_files)\n",
    "\n",
    "    if num_patient_files==0:\n",
    "        raise Exception('No data was provided.')\n",
    "\n",
    "    # Create a folder for the Challenge outputs if it does not already exist.\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Run the team's model on the Challenge data.\n",
    "    if verbose >= 1:\n",
    "        print('Running model on Challenge data...')\n",
    "\n",
    "    # Iterate over the patient files.\n",
    "    for i in range(num_patient_files):\n",
    "        if verbose >= 2:\n",
    "            print('    {}/{}...'.format(i+1, num_patient_files))\n",
    "\n",
    "        patient_data = load_patient_data(patient_files[i])\n",
    "        recordings = load_recordings(data_folder, patient_data)\n",
    "\n",
    "        # Allow or disallow the model to fail on parts of the data; helpful for debugging.\n",
    "        try:\n",
    "            classes, labels, probabilities = run_challenge_model(model, patient_data, recordings, verbose) ### Teams: Implement this function!!!\n",
    "        except:\n",
    "            if allow_failures:\n",
    "                if verbose >= 2:\n",
    "                    print('... failed.')\n",
    "                classes, labels, probabilities = list(), list(), list()\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # Save Challenge outputs.\n",
    "        head, tail = os.path.split(patient_files[i])\n",
    "        root, extension = os.path.splitext(tail)\n",
    "        output_file = os.path.join(output_folder, root + '.csv')\n",
    "        patient_id = get_patient_id(patient_data)\n",
    "        save_challenge_outputs(output_file, patient_id, classes, labels, probabilities)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp_model4'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 우리 모형 저장된 폴더이름\n",
    "model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/hmd/murmur/test'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 테스트 데이터 폴더\n",
    "test_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 테스트 데이터에 모형 돌려서 스코어 저장할 폴더\n",
    "output_folder = '/home/ubuntu/hmd/notebooks/out2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Challenge model...\n",
      "Running model on Challenge data...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "run_model(model_folder, test_folder, output_folder, allow_failures = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#49628\r\n",
      "Present,Unknown,Absent,Abnormal,Normal\r\n",
      "0,0,1,1,0\r\n",
      "0.37117127,0.012293197,0.6165355,0.54467726,0.45532268\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/ubuntu/hmd/notebooks/out2/49628.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수정완료 기록 잘 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.622,0.424,0.281,0.728,0.375,25689.450\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.722,0.707,0.530,0.586,0.350,20898.997\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.654,0.500,0.712\n",
      "AUPRC,0.326,0.076,0.871\n",
      "F-measure,0.000,0.000,0.842\n",
      "Accuracy,0.000,0.000,1.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.722,0.722\n",
      "AUPRC,0.727,0.687\n",
      "F-measure,0.368,0.693\n",
      "Accuracy,0.235,0.957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "murmur_scores, outcome_scores = evaluate_model(test_folder, output_folder)\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "    + '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "\n",
    "if len(sys.argv) == 3:\n",
    "    print(output_string)\n",
    "elif len(sys.argv) == 4:\n",
    "    with open(sys.argv[3], 'w') as f:\n",
    "        f.write(output_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch 2 트레이닝 해서 결과는 별로.. 기록은 잘 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

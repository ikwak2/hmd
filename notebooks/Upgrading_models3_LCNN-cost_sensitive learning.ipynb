{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a6057f",
   "metadata": {},
   "source": [
    "## LCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a45589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import librosa\n",
    "import librosa.display\n",
    "import math\n",
    "import sys\n",
    "#sys.path.insert(0,'/home/ikwak2/hmd/notebooks')\n",
    "#sys.path.insert(0,'/home/ikwak2/hmd/iy_classifier')\n",
    "sys.path.insert(0,'utils')\n",
    "from helper_code import *\n",
    "from get_feature import *\n",
    "from models import *\n",
    "from Generator0 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3bbd292",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'physionet.org/files/circor-heart-sound/1.0.3'\n",
    "training_data_file = root_dir + '/' + 'training_data.csv'\n",
    "training_data_dir = root_dir + '/' + 'training_data'\n",
    "model_dir = root_dir + '/' + 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a9d85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 15 10:34:52 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   43C    P0    66W / 300W |  32506MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:00:06.0 Off |                  Off |\n",
      "| N/A   35C    P0    37W / 300W |      2MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      5442      C   ...sorflow2.4_p37/bin/python    31095MiB |\n",
      "|    0   N/A  N/A     15356      C   ...sorflow2.4_p37/bin/python     1409MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb1c20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b6c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder =  'physionet.org/files/circor-heart-sound/1.0.3/training_data'\n",
    "train_folder =  '/home/ubuntu/data/hmd/murmur/train'\n",
    "test_folder = '/home/ubuntu/data/hmd/murmur/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6cf859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = 'lcnn2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f7665",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7cdbbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3438eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidal_decay(e, start=0, end=100, lr_start=1e-3, lr_end=1e-5):\n",
    "    if e < start:\n",
    "        return lr_start\n",
    "    elif e > end:\n",
    "        return lr_end\n",
    "\n",
    "    middle = (start + end) / 2\n",
    "    s = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return s(13 * (-e + middle) / np.abs(end - start)) * np.abs(lr_start - lr_end) + lr_end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ac699",
   "metadata": {},
   "source": [
    "### get feature 함수확장: 음성피쳐 옵션들과, 추가 음성들 고려한 피쳐추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c63a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_files_trn = find_patient_files(train_folder)\n",
    "patient_files_test = find_patient_files(test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98be9d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_feature = {'samp_sec': 20,\n",
    "                  #### melspec, stft 피쳐 옵션들  \n",
    "                  'pre_emphasis': 0,\n",
    "                  'hop_length': 256,\n",
    "                  'win_length':512,\n",
    "                  'n_mels': 100,\n",
    "                  #### cqt 피쳐 옵션들  \n",
    "                  'filter_scale': 1,\n",
    "                  'n_bins': 80,\n",
    "                  'fmin': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05434c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melspec:  100 313\n",
      "cqt:  80 157\n",
      "stft:  257 313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "utils/get_feature.py:599: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  features[k1] = np.array(features[k1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melspec:  100 313\n",
      "cqt:  80 157\n",
      "stft:  257 313\n"
     ]
    }
   ],
   "source": [
    "features_trn, mm_lbs_trn, out_lbs_trn, mel_input_shape, cqt_input_shape, stft_input_shape = get_features_3lb_all(train_folder, patient_files_trn, **params_feature)\n",
    "features_test, mm_lbs_test, out_lbs_test, _, _, _ = get_features_3lb_all(test_folder, patient_files_test, **params_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d6039a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "227096b2",
   "metadata": {},
   "source": [
    "### LCNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d33f5",
   "metadata": {},
   "source": [
    "LCNN 초기모형 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "62554565",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Convolution2D, GlobalAveragePooling2D, MaxPool2D\n",
    "\n",
    "def get_LCNN_o_1(mel_input_shape, cqt_input_shape, stft_input_shape, use_mel = True, use_cqt = True, use_stft = True, ord1 = True):\n",
    "        # Create a towy model.\n",
    "    age = keras.Input(shape=(6,), name = 'age_cat')\n",
    "    sex = keras.Input(shape=(2,), name = 'sex_cat')\n",
    "    hw = keras.Input(shape=(2,), name = 'height_weight')\n",
    "    preg = keras.Input(shape=(1,), name = 'is_preg')\n",
    "    loc = keras.Input(shape=(5,), name = 'loc')\n",
    "    mel1 = keras.Input(shape=(mel_input_shape), name = 'mel')\n",
    "    cqt1 = keras.Input(shape=(cqt_input_shape), name = 'cqt')\n",
    "    stft1 = keras.Input(shape=(stft_input_shape), name = 'stft')\n",
    "        \n",
    "        \n",
    "    ## age embeddig\n",
    "    age1 = layers.Dense(2, activation = None)(age)\n",
    "\n",
    "    ## sex embedding\n",
    "    sex1 = layers.Dense(1, activation = None)(sex)\n",
    "\n",
    "    ## hw embedding\n",
    "    hw1 = layers.Dense(1, activation = None)(hw)\n",
    "\n",
    "    ## loc embedding\n",
    "    loc1 = layers.Dense(3, activation = None)(loc)\n",
    "\n",
    "    ## mel embedding\n",
    "    conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "    conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "    mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "    max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "    conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "    conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "    mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "    batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "    conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "    conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "    mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "    max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "    batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "    conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "    conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "    mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "    batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "    conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "    conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "    mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "    max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "    conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "    conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "    mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "    batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "    conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "    conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "    mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "    batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "    conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "    conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "    mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "    batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "    conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "    conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "    mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "    max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "    mel2 = layers.GlobalAveragePooling2D()(max28)\n",
    "\n",
    "    ## cqt embedding\n",
    "    conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "    conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "    mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "    max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "    conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "    conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "    mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "    batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "    conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "    conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "    mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "    max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "    batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "    conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "    conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "    mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "    batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "    conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "    conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "    mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "    max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "    conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "    conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "    mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "    batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "    conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "    conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "    mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "    batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "    conv23_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "    conv23_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "    mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "    batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "    conv26_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "    conv26_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "    mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "    max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "    cqt2 = layers.GlobalAveragePooling2D()(max28)\n",
    "\n",
    "    ## stft embedding\n",
    "    conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "    conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "    mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "    max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "    conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "    conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "    mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "    batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "    conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "    conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "    mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "    max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "    batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "    conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "    conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "    mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "    batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "    conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "    conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "    mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "    max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "    conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "    conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "    mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "    batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "    conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "    conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "    mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "    batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "    conv23_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "    conv23_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "    mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "    batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "    conv26_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "    conv26_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "    mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "    max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "    stft2 = layers.GlobalAveragePooling2D()(max28)\n",
    "    \n",
    "#    concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg])\n",
    "#    d1 = layers.Dense(2, activation = 'relu')(concat1)\n",
    "    \n",
    "    if use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2, stft2])\n",
    "    if not use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([cqt2, stft2])\n",
    "    if use_mel and not use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, stft2])\n",
    "    if use_mel and use_cqt and not use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2])\n",
    "    if not use_mel and not use_cqt and use_stft :  ## stft 만\n",
    "        concat2 = stft2\n",
    "    if use_mel and not use_cqt and not use_stft :  ### mel만\n",
    "        concat2 = mel2\n",
    "    if not use_mel and use_cqt and not use_stft :  ### cqt만\n",
    "        concat2 = cqt2\n",
    "    if ord1 :\n",
    "        res1 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "    else :\n",
    "        res1 = layers.Dense(3, activation = \"softmax\")(concat2)\n",
    "        \n",
    "    res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "\n",
    "    model = keras.Model(inputs = [age,sex,hw,preg,loc,mel1,cqt1,stft1] , outputs = res1 )\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy','AUC'])\n",
    "    return(model)\n",
    "\n",
    "def get_LCNN_2(mel_input_shape, cqt_input_shape, stft_input_shape, use_mel = True, use_cqt = True, use_stft = True):\n",
    "        # Create a towy model.\n",
    "    age = keras.Input(shape=(6,), name = 'age_cat')\n",
    "    sex = keras.Input(shape=(2,), name = 'sex_cat')\n",
    "    hw = keras.Input(shape=(2,), name = 'height_weight')\n",
    "    preg = keras.Input(shape=(1,), name = 'is_preg')\n",
    "    loc = keras.Input(shape=(5,), name = 'loc')\n",
    "    mel1 = keras.Input(shape=(mel_input_shape), name = 'mel')\n",
    "    cqt1 = keras.Input(shape=(cqt_input_shape), name = 'cqt')\n",
    "    stft1 = keras.Input(shape=(stft_input_shape), name = 'stft')\n",
    "        \n",
    "        \n",
    "    ## age embeddig\n",
    "    age1 = layers.Dense(2, activation = None)(age)\n",
    "\n",
    "    ## sex embedding\n",
    "    sex1 = layers.Dense(1, activation = None)(sex)\n",
    "\n",
    "    ## hw embedding\n",
    "    hw1 = layers.Dense(1, activation = None)(hw)\n",
    "\n",
    "    ## loc embedding\n",
    "    loc1 = layers.Dense(3, activation = None)(loc)\n",
    "\n",
    "    ## mel embedding\n",
    "    conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "    conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "    mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "    max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "    conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "    conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "    mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "    batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "    conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "    conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "    mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "    max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "    batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "    conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "    conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "    mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "    batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "    conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "    conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "    mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "    max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "    conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "    conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "    mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "    batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "    conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "    conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "    mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "    batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "    conv23_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "    conv23_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "    mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "    batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "    conv26_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "    conv26_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "    mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "    max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "    mel2 = layers.GlobalAveragePooling2D()(max28)\n",
    "\n",
    "    ## cqt embedding\n",
    "    conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "    conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "    mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "    max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "    conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "    conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "    mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "    batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "    conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "    conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "    mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "    max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "    batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "    conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "    conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "    mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "    batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "    conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "    conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "    mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "    max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "    conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "    conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "    mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "    batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "    conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "    conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "    mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "    batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "    conv23_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "    conv23_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "    mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "    batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "    conv26_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "    conv26_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "    mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "    max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "    cqt2 = layers.GlobalAveragePooling2D()(max28)\n",
    "\n",
    "    ## stft embedding\n",
    "    conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "    conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "    mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "    max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "    conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "    conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "    mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "    batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "    conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "    conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "    mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "    max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "    batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "    conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "    conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "    mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "    batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "    conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "    conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "    mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "    max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "    conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "    conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "    mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "    batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "    conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "    conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "    mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "    batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "    conv23_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "    conv23_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "    mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "    batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "    conv26_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "    conv26_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "    mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "    max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "    stft2 = layers.GlobalAveragePooling2D()(max28)\n",
    "    \n",
    "#    concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg])\n",
    "#    d1 = layers.Dense(2, activation = 'relu')(concat1)\n",
    "    \n",
    "    if use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2, stft2])\n",
    "    if not use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([cqt2, stft2])\n",
    "    if use_mel and not use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, stft2])\n",
    "    if use_mel and use_cqt and not use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2])\n",
    "    if not use_mel and not use_cqt and use_stft :  ## stft 만\n",
    "        concat2 = stft2\n",
    "    if use_mel and not use_cqt and not use_stft :  ### mel만\n",
    "        concat2 = mel2\n",
    "    if not use_mel and use_cqt and not use_stft :  ### cqt만\n",
    "        concat2 = cqt2\n",
    "#    concat2 = layers.Dense(10, activation = 'relu')(concat2)\n",
    "    res1 = layers.Dense(3, activation = \"softmax\")(concat2)\n",
    "    res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "\n",
    "    model = keras.Model(inputs = [age,sex,hw,preg,loc,mel1,cqt1,stft1] , outputs = res2 )\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy','AUC'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ffc5531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1 = get_toy5_1(mel_input_shape, cqt_input_shape, stft_input_shape)\n",
    "#model2 = get_toy5_2(mel_input_shape, cqt_input_shape, stft_input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2d247bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = get_LCNN_1(mel_input_shape, cqt_input_shape, stft_input_shape, use_mel = True, use_cqt = False, use_stft = False)\n",
    "model2 = get_LCNN_2(mel_input_shape, cqt_input_shape, stft_input_shape, use_mel = True, use_cqt = False, use_stft = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1f1abd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1150\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mel (InputLayer)                [(None, 100, 313, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62100 (Conv2D)           (None, 100, 313, 32) 832         mel[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62101 (Conv2D)           (None, 100, 313, 32) 832         mel[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "maximum_31050 (Maximum)         (None, 100, 313, 32) 0           conv2d_62100[0][0]               \n",
      "                                                                 conv2d_62101[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13800 (MaxPooling (None, 50, 157, 32)  0           maximum_31050[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62102 (Conv2D)           (None, 50, 157, 32)  1056        max_pooling2d_13800[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62103 (Conv2D)           (None, 50, 157, 32)  1056        max_pooling2d_13800[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "maximum_31051 (Maximum)         (None, 50, 157, 32)  0           conv2d_62102[0][0]               \n",
      "                                                                 conv2d_62103[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20700 (Batc (None, 50, 157, 32)  96          maximum_31051[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62104 (Conv2D)           (None, 50, 157, 48)  13872       batch_normalization_20700[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62105 (Conv2D)           (None, 50, 157, 48)  13872       batch_normalization_20700[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_31052 (Maximum)         (None, 50, 157, 48)  0           conv2d_62104[0][0]               \n",
      "                                                                 conv2d_62105[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13801 (MaxPooling (None, 25, 79, 48)   0           maximum_31052[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20701 (Batc (None, 25, 79, 48)   144         max_pooling2d_13801[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62106 (Conv2D)           (None, 25, 79, 48)   2352        batch_normalization_20701[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62107 (Conv2D)           (None, 25, 79, 48)   2352        batch_normalization_20701[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_31053 (Maximum)         (None, 25, 79, 48)   0           conv2d_62106[0][0]               \n",
      "                                                                 conv2d_62107[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20702 (Batc (None, 25, 79, 48)   144         maximum_31053[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62108 (Conv2D)           (None, 25, 79, 64)   27712       batch_normalization_20702[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62109 (Conv2D)           (None, 25, 79, 64)   27712       batch_normalization_20702[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_31054 (Maximum)         (None, 25, 79, 64)   0           conv2d_62108[0][0]               \n",
      "                                                                 conv2d_62109[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13802 (MaxPooling (None, 13, 40, 64)   0           maximum_31054[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62110 (Conv2D)           (None, 13, 40, 64)   4160        max_pooling2d_13802[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62111 (Conv2D)           (None, 13, 40, 64)   4160        max_pooling2d_13802[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "maximum_31055 (Maximum)         (None, 13, 40, 64)   0           conv2d_62110[0][0]               \n",
      "                                                                 conv2d_62111[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20703 (Batc (None, 13, 40, 64)   192         maximum_31055[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62112 (Conv2D)           (None, 13, 40, 32)   18464       batch_normalization_20703[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62113 (Conv2D)           (None, 13, 40, 32)   18464       batch_normalization_20703[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_31056 (Maximum)         (None, 13, 40, 32)   0           conv2d_62112[0][0]               \n",
      "                                                                 conv2d_62113[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20704 (Batc (None, 13, 40, 32)   96          maximum_31056[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62114 (Conv2D)           (None, 13, 40, 32)   1056        batch_normalization_20704[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62115 (Conv2D)           (None, 13, 40, 32)   1056        batch_normalization_20704[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_31057 (Maximum)         (None, 13, 40, 32)   0           conv2d_62114[0][0]               \n",
      "                                                                 conv2d_62115[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20705 (Batc (None, 13, 40, 32)   96          maximum_31057[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62116 (Conv2D)           (None, 13, 40, 32)   9248        batch_normalization_20705[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62117 (Conv2D)           (None, 13, 40, 32)   9248        batch_normalization_20705[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_31058 (Maximum)         (None, 13, 40, 32)   0           conv2d_62116[0][0]               \n",
      "                                                                 conv2d_62117[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13803 (MaxPooling (None, 6, 20, 32)    0           maximum_31058[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3450 ( (None, 32)           0           max_pooling2d_13803[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "age_cat (InputLayer)            [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sex_cat (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "height_weight (InputLayer)      [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "is_preg (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "loc (InputLayer)                [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cqt (InputLayer)                [(None, 80, 157, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "stft (InputLayer)               [(None, 257, 313, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_6904 (Dense)              (None, 3)            99          global_average_pooling2d_3450[0][\n",
      "==================================================================================================\n",
      "Total params: 158,371\n",
      "Trainable params: 157,859\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fe327878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 9s 192ms/step - loss: 1.2424 - accuracy: 0.6624 - auc: 0.8125 - val_loss: 9.8755 - val_accuracy: 0.2552 - val_auc: 0.4040\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 1.0923 - accuracy: 0.6913 - auc: 0.8460 - val_loss: 10.0003 - val_accuracy: 0.2552 - val_auc: 0.4049\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 1.1246 - accuracy: 0.6913 - auc: 0.8463 - val_loss: 2.3272 - val_accuracy: 0.2583 - val_auc: 0.5237\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 1.0766 - accuracy: 0.7205 - auc: 0.8621 - val_loss: 1.2448 - val_accuracy: 0.2995 - val_auc: 0.5532\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 1.0267 - accuracy: 0.7589 - auc: 0.8723 - val_loss: 0.7467 - val_accuracy: 0.5721 - val_auc: 0.7566\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 7s 171ms/step - loss: 1.0630 - accuracy: 0.7332 - auc: 0.8642 - val_loss: 1.1932 - val_accuracy: 0.2567 - val_auc: 0.6131\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 6s 165ms/step - loss: 1.0735 - accuracy: 0.7199 - auc: 0.8547 - val_loss: 0.8531 - val_accuracy: 0.3217 - val_auc: 0.6116\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 1.0172 - accuracy: 0.7710 - auc: 0.8781 - val_loss: 0.5590 - val_accuracy: 0.7971 - val_auc: 0.8956\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 6s 164ms/step - loss: 1.0581 - accuracy: 0.7409 - auc: 0.8696 - val_loss: 0.5856 - val_accuracy: 0.8067 - val_auc: 0.8809\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 6s 164ms/step - loss: 0.9831 - accuracy: 0.7811 - auc: 0.8806 - val_loss: 0.5163 - val_accuracy: 0.8257 - val_auc: 0.9048\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 1.0556 - accuracy: 0.7475 - auc: 0.8716 - val_loss: 0.6718 - val_accuracy: 0.6941 - val_auc: 0.8153\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 1.0101 - accuracy: 0.7596 - auc: 0.8747 - val_loss: 0.5254 - val_accuracy: 0.8162 - val_auc: 0.9038\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 1.0628 - accuracy: 0.7385 - auc: 0.8701 - val_loss: 0.6499 - val_accuracy: 0.7195 - val_auc: 0.8588\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 6s 164ms/step - loss: 0.9982 - accuracy: 0.7708 - auc: 0.8842 - val_loss: 0.7132 - val_accuracy: 0.6593 - val_auc: 0.7977\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 1.0341 - accuracy: 0.7401 - auc: 0.8735 - val_loss: 0.6338 - val_accuracy: 0.7306 - val_auc: 0.8535\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 0.9727 - accuracy: 0.7893 - auc: 0.8801 - val_loss: 0.5348 - val_accuracy: 0.8526 - val_auc: 0.9086\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 7s 176ms/step - loss: 1.0340 - accuracy: 0.7720 - auc: 0.8799 - val_loss: 1.1629 - val_accuracy: 0.2615 - val_auc: 0.6063\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 7s 170ms/step - loss: 1.0046 - accuracy: 0.7640 - auc: 0.8734 - val_loss: 1.0928 - val_accuracy: 0.4247 - val_auc: 0.6330\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 1.0488 - accuracy: 0.7319 - auc: 0.8669 - val_loss: 0.7629 - val_accuracy: 0.5499 - val_auc: 0.7512\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 1.0389 - accuracy: 0.7620 - auc: 0.8779 - val_loss: 0.5082 - val_accuracy: 0.8479 - val_auc: 0.9194\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 1.0026 - accuracy: 0.7727 - auc: 0.8757 - val_loss: 0.5536 - val_accuracy: 0.8368 - val_auc: 0.9150\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 1.0100 - accuracy: 0.7606 - auc: 0.8704 - val_loss: 0.5599 - val_accuracy: 0.8257 - val_auc: 0.8958\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 6s 162ms/step - loss: 0.9754 - accuracy: 0.7679 - auc: 0.8772 - val_loss: 0.5109 - val_accuracy: 0.8082 - val_auc: 0.8992\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 7s 172ms/step - loss: 1.0295 - accuracy: 0.7607 - auc: 0.8805 - val_loss: 0.6228 - val_accuracy: 0.7956 - val_auc: 0.8940\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 7s 170ms/step - loss: 0.9751 - accuracy: 0.7747 - auc: 0.8943 - val_loss: 0.5033 - val_accuracy: 0.8431 - val_auc: 0.9114\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 0.9605 - accuracy: 0.7750 - auc: 0.8899 - val_loss: 1.0634 - val_accuracy: 0.2758 - val_auc: 0.6344\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 7s 172ms/step - loss: 1.0632 - accuracy: 0.7304 - auc: 0.8692 - val_loss: 0.5787 - val_accuracy: 0.7876 - val_auc: 0.8849\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 7s 170ms/step - loss: 1.0436 - accuracy: 0.7734 - auc: 0.8834 - val_loss: 0.4678 - val_accuracy: 0.8479 - val_auc: 0.9174\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 7s 172ms/step - loss: 1.0617 - accuracy: 0.7276 - auc: 0.8646 - val_loss: 0.4743 - val_accuracy: 0.8542 - val_auc: 0.9007\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 7s 171ms/step - loss: 1.0235 - accuracy: 0.7709 - auc: 0.8827 - val_loss: 0.9521 - val_accuracy: 0.3740 - val_auc: 0.6383\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 1.0000 - accuracy: 0.7718 - auc: 0.8765 - val_loss: 0.5164 - val_accuracy: 0.8526 - val_auc: 0.9118\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 7s 170ms/step - loss: 0.9915 - accuracy: 0.7568 - auc: 0.8707 - val_loss: 0.5467 - val_accuracy: 0.8479 - val_auc: 0.9157\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 7s 171ms/step - loss: 1.0024 - accuracy: 0.7658 - auc: 0.8799 - val_loss: 0.5074 - val_accuracy: 0.8621 - val_auc: 0.9167\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 7s 172ms/step - loss: 0.9560 - accuracy: 0.7939 - auc: 0.8861 - val_loss: 0.4990 - val_accuracy: 0.8637 - val_auc: 0.9160\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 7s 171ms/step - loss: 0.9920 - accuracy: 0.7661 - auc: 0.8856 - val_loss: 0.4573 - val_accuracy: 0.8590 - val_auc: 0.9257\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 7s 172ms/step - loss: 1.0045 - accuracy: 0.7635 - auc: 0.8804 - val_loss: 0.6577 - val_accuracy: 0.7385 - val_auc: 0.8619\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 0.9611 - accuracy: 0.7698 - auc: 0.8888 - val_loss: 0.9090 - val_accuracy: 0.3930 - val_auc: 0.6533\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 1.0195 - accuracy: 0.7493 - auc: 0.8793 - val_loss: 0.7712 - val_accuracy: 0.4311 - val_auc: 0.6870\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 0.9886 - accuracy: 0.7754 - auc: 0.8833 - val_loss: 0.4674 - val_accuracy: 0.8368 - val_auc: 0.9020\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 7s 174ms/step - loss: 0.9934 - accuracy: 0.7706 - auc: 0.8897 - val_loss: 0.6695 - val_accuracy: 0.6545 - val_auc: 0.8114\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 0.9998 - accuracy: 0.7803 - auc: 0.8859 - val_loss: 0.5492 - val_accuracy: 0.7956 - val_auc: 0.8958\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 1.0026 - accuracy: 0.7760 - auc: 0.8880 - val_loss: 0.4736 - val_accuracy: 0.8605 - val_auc: 0.9296\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 6s 165ms/step - loss: 1.0137 - accuracy: 0.7561 - auc: 0.8809 - val_loss: 0.7485 - val_accuracy: 0.5198 - val_auc: 0.7252\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 6s 161ms/step - loss: 0.9888 - accuracy: 0.7681 - auc: 0.8841 - val_loss: 0.5222 - val_accuracy: 0.8288 - val_auc: 0.9052\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 7s 173ms/step - loss: 1.0185 - accuracy: 0.7756 - auc: 0.8750 - val_loss: 0.5301 - val_accuracy: 0.8352 - val_auc: 0.9109\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 0.9524 - accuracy: 0.7936 - auc: 0.8846 - val_loss: 0.5087 - val_accuracy: 0.8447 - val_auc: 0.9177\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 6s 165ms/step - loss: 1.0337 - accuracy: 0.7603 - auc: 0.8742 - val_loss: 0.5036 - val_accuracy: 0.8479 - val_auc: 0.9210\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 1.0291 - accuracy: 0.7649 - auc: 0.8798 - val_loss: 0.5390 - val_accuracy: 0.8431 - val_auc: 0.9160\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 0.9594 - accuracy: 0.7794 - auc: 0.8851 - val_loss: 0.8589 - val_accuracy: 0.3772 - val_auc: 0.6616\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 0.9767 - accuracy: 0.7637 - auc: 0.8903 - val_loss: 0.5528 - val_accuracy: 0.8288 - val_auc: 0.9097\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 1.0048 - accuracy: 0.7649 - auc: 0.8758 - val_loss: 0.4853 - val_accuracy: 0.8479 - val_auc: 0.9214\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 7s 170ms/step - loss: 1.0170 - accuracy: 0.7623 - auc: 0.8843 - val_loss: 0.5033 - val_accuracy: 0.8494 - val_auc: 0.9152\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 7s 170ms/step - loss: 0.9980 - accuracy: 0.7467 - auc: 0.8804 - val_loss: 0.5534 - val_accuracy: 0.8384 - val_auc: 0.9146\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 1.0199 - accuracy: 0.7802 - auc: 0.8821 - val_loss: 0.6879 - val_accuracy: 0.6228 - val_auc: 0.8048\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 6s 158ms/step - loss: 0.9961 - accuracy: 0.7762 - auc: 0.8854 - val_loss: 0.4769 - val_accuracy: 0.8526 - val_auc: 0.9231\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 7s 173ms/step - loss: 0.9963 - accuracy: 0.7834 - auc: 0.8967 - val_loss: 0.6433 - val_accuracy: 0.6989 - val_auc: 0.8385\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 0.9834 - accuracy: 0.7804 - auc: 0.8810 - val_loss: 0.5425 - val_accuracy: 0.8304 - val_auc: 0.9061\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 6s 164ms/step - loss: 0.9653 - accuracy: 0.7813 - auc: 0.8874 - val_loss: 0.4827 - val_accuracy: 0.8637 - val_auc: 0.9234\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 0.9765 - accuracy: 0.7828 - auc: 0.8863 - val_loss: 0.6474 - val_accuracy: 0.7036 - val_auc: 0.8316\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 6s 161ms/step - loss: 0.9673 - accuracy: 0.7824 - auc: 0.8895 - val_loss: 0.7242 - val_accuracy: 0.5610 - val_auc: 0.7542\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 0.9440 - accuracy: 0.7928 - auc: 0.8971 - val_loss: 0.6108 - val_accuracy: 0.7528 - val_auc: 0.8601\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 6s 162ms/step - loss: 1.0047 - accuracy: 0.7372 - auc: 0.8784 - val_loss: 0.6174 - val_accuracy: 0.7528 - val_auc: 0.8632\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 1.0109 - accuracy: 0.7628 - auc: 0.8835 - val_loss: 0.5654 - val_accuracy: 0.8130 - val_auc: 0.9015\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 7s 170ms/step - loss: 0.9870 - accuracy: 0.7898 - auc: 0.8871 - val_loss: 0.5244 - val_accuracy: 0.8352 - val_auc: 0.9107\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 6s 165ms/step - loss: 0.9815 - accuracy: 0.7794 - auc: 0.8894 - val_loss: 0.4899 - val_accuracy: 0.8431 - val_auc: 0.9212\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 6s 164ms/step - loss: 0.9658 - accuracy: 0.7946 - auc: 0.8908 - val_loss: 0.5264 - val_accuracy: 0.8368 - val_auc: 0.9061\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 6s 162ms/step - loss: 0.9629 - accuracy: 0.7862 - auc: 0.8896 - val_loss: 0.4817 - val_accuracy: 0.8558 - val_auc: 0.9179\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 0.9842 - accuracy: 0.7736 - auc: 0.8894 - val_loss: 0.5131 - val_accuracy: 0.8415 - val_auc: 0.9158\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 6s 161ms/step - loss: 0.9263 - accuracy: 0.7927 - auc: 0.8900 - val_loss: 0.4792 - val_accuracy: 0.8526 - val_auc: 0.9213\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 0.9711 - accuracy: 0.7805 - auc: 0.8858 - val_loss: 0.5206 - val_accuracy: 0.8463 - val_auc: 0.9149\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 6s 162ms/step - loss: 0.9960 - accuracy: 0.7787 - auc: 0.8849 - val_loss: 0.5042 - val_accuracy: 0.8590 - val_auc: 0.9207\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 6s 165ms/step - loss: 1.0055 - accuracy: 0.7757 - auc: 0.8948 - val_loss: 0.5460 - val_accuracy: 0.8067 - val_auc: 0.8988\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 6s 161ms/step - loss: 0.9382 - accuracy: 0.8015 - auc: 0.8863 - val_loss: 0.4796 - val_accuracy: 0.8510 - val_auc: 0.9222\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 6s 165ms/step - loss: 0.9610 - accuracy: 0.7921 - auc: 0.8864 - val_loss: 0.4941 - val_accuracy: 0.8542 - val_auc: 0.9187\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 0.9589 - accuracy: 0.8049 - auc: 0.8889 - val_loss: 0.5065 - val_accuracy: 0.8637 - val_auc: 0.9211\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 7s 171ms/step - loss: 0.9434 - accuracy: 0.8054 - auc: 0.8898 - val_loss: 0.4977 - val_accuracy: 0.8526 - val_auc: 0.9173\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 6s 163ms/step - loss: 0.9484 - accuracy: 0.7851 - auc: 0.8929 - val_loss: 0.4934 - val_accuracy: 0.8558 - val_auc: 0.9185\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 6s 165ms/step - loss: 0.9700 - accuracy: 0.7815 - auc: 0.8943 - val_loss: 0.4958 - val_accuracy: 0.8574 - val_auc: 0.9188\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 6s 163ms/step - loss: 0.9444 - accuracy: 0.7932 - auc: 0.8956 - val_loss: 0.4952 - val_accuracy: 0.8574 - val_auc: 0.9188\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 0.9675 - accuracy: 0.7827 - auc: 0.8859 - val_loss: 0.4997 - val_accuracy: 0.8590 - val_auc: 0.9187\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 6s 162ms/step - loss: 1.0113 - accuracy: 0.7798 - auc: 0.8829 - val_loss: 0.4857 - val_accuracy: 0.8605 - val_auc: 0.9193\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 6s 162ms/step - loss: 0.9626 - accuracy: 0.7844 - auc: 0.8935 - val_loss: 0.4880 - val_accuracy: 0.8637 - val_auc: 0.9188\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 6s 161ms/step - loss: 0.9651 - accuracy: 0.7877 - auc: 0.8874 - val_loss: 0.4861 - val_accuracy: 0.8605 - val_auc: 0.9204\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 7s 170ms/step - loss: 0.9728 - accuracy: 0.7769 - auc: 0.8934 - val_loss: 0.4893 - val_accuracy: 0.8653 - val_auc: 0.9189\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 7s 173ms/step - loss: 0.9419 - accuracy: 0.7898 - auc: 0.8966 - val_loss: 0.4987 - val_accuracy: 0.8542 - val_auc: 0.9172\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 6s 165ms/step - loss: 0.9541 - accuracy: 0.7908 - auc: 0.8919 - val_loss: 0.4953 - val_accuracy: 0.8574 - val_auc: 0.9177\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 0.9427 - accuracy: 0.7924 - auc: 0.8897 - val_loss: 0.4879 - val_accuracy: 0.8653 - val_auc: 0.9176\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 7s 172ms/step - loss: 0.9999 - accuracy: 0.7796 - auc: 0.8888 - val_loss: 0.4902 - val_accuracy: 0.8574 - val_auc: 0.9172\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 6s 163ms/step - loss: 0.9755 - accuracy: 0.7909 - auc: 0.8889 - val_loss: 0.4932 - val_accuracy: 0.8526 - val_auc: 0.9168\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 0.9606 - accuracy: 0.7835 - auc: 0.8907 - val_loss: 0.5047 - val_accuracy: 0.8510 - val_auc: 0.9147\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 1.0092 - accuracy: 0.7799 - auc: 0.8904 - val_loss: 0.5005 - val_accuracy: 0.8510 - val_auc: 0.9163\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 6s 158ms/step - loss: 0.9930 - accuracy: 0.7800 - auc: 0.8861 - val_loss: 0.4975 - val_accuracy: 0.8574 - val_auc: 0.9162\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - 6s 160ms/step - loss: 0.9316 - accuracy: 0.7908 - auc: 0.8959 - val_loss: 0.4911 - val_accuracy: 0.8590 - val_auc: 0.9171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100\n",
      "40/40 [==============================] - 6s 163ms/step - loss: 0.9583 - accuracy: 0.7877 - auc: 0.8970 - val_loss: 0.5009 - val_accuracy: 0.8463 - val_auc: 0.9151\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 6s 163ms/step - loss: 0.9891 - accuracy: 0.7859 - auc: 0.8946 - val_loss: 0.5041 - val_accuracy: 0.8494 - val_auc: 0.9168\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 7s 173ms/step - loss: 1.0170 - accuracy: 0.7707 - auc: 0.8923 - val_loss: 0.5079 - val_accuracy: 0.8494 - val_auc: 0.9162\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 0.9123 - accuracy: 0.8142 - auc: 0.9029 - val_loss: 0.4925 - val_accuracy: 0.8574 - val_auc: 0.9186\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 6s 158ms/step - loss: 0.9909 - accuracy: 0.7781 - auc: 0.8884 - val_loss: 0.5064 - val_accuracy: 0.8447 - val_auc: 0.9158\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 0.9830 - accuracy: 0.7632 - auc: 0.8964 - val_loss: 0.5032 - val_accuracy: 0.8494 - val_auc: 0.9162\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 6s 163ms/step - loss: 0.9749 - accuracy: 0.7763 - auc: 0.8970 - val_loss: 0.5033 - val_accuracy: 0.8463 - val_auc: 0.9161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7d381246d0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epoch = 100\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "batch_size = 64\n",
    "params = {'batch_size': batch_size,\n",
    "#          'input_shape': (100, 313, 1),\n",
    "          'shuffle': True,\n",
    "          'beta_param': 0.7,\n",
    "          'mixup': True,\n",
    "#          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "#          'highpass': [.5, [78,79,80,81,82,83,84,85]]\n",
    "          'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "#           'dropblock' : [30, 100]\n",
    "          #'device' : device\n",
    "}\n",
    "\n",
    "params_no_shuffle = {'batch_size': batch_size,\n",
    "#          'input_shape': (100, 313, 1),\n",
    "          'shuffle': False,\n",
    "          'beta_param': 0.7,\n",
    "          'mixup': False\n",
    "          #'device': device\n",
    "}\n",
    "\n",
    "TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], features_trn['preg'], features_trn['loc'], \n",
    "           features_trn['mel1'],features_trn['cqt1'],features_trn['stft1']], \n",
    "                        mm_lbs_trn,  ## our Y\n",
    "                        **params)()\n",
    "\n",
    "#ValDGen_1 = Generator0([features_test[0]['age'],features_test[0]['sex'], features_test[0]['hw'], features_test[0]['preg'], features_test[0]['loc'], \n",
    "#           features_test[0]['mel1'],features_test[0]['cqt1'],features_test[0]['stft1']], \n",
    "#                        mm_lbs_test,  ## our Y\n",
    "#                        **params_no_shuffle)()\n",
    "\n",
    "class_weight = {0: 3, 1: 1., 2: 1} \n",
    "    \n",
    "model1.fit(TrainDGen_1,\n",
    "          validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                              features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                              features_test['cqt1'], features_test['stft1']], \n",
    "                             mm_lbs_test), \n",
    "                             callbacks=[lr],\n",
    "                              steps_per_epoch=np.ceil(len(mm_lbs_trn)/64),\n",
    "                           class_weight=class_weight, \n",
    "                             epochs = n_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d23f4",
   "metadata": {},
   "source": [
    "트레이닝 accuracy 와 auc 가 validation accuracy, auc 와 거의 유사함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1630e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 9s 201ms/step - loss: 0.7671 - accuracy: 0.5098 - auc: 0.5344 - val_loss: 0.7761 - val_accuracy: 0.3994 - val_auc: 0.3535\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 0.7134 - accuracy: 0.5340 - auc: 0.5511 - val_loss: 0.7773 - val_accuracy: 0.5055 - val_auc: 0.5498\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 7s 174ms/step - loss: 0.7093 - accuracy: 0.4935 - auc: 0.5225 - val_loss: 1.3568 - val_accuracy: 0.5055 - val_auc: 0.5932\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 7s 175ms/step - loss: 0.7084 - accuracy: 0.4987 - auc: 0.5316 - val_loss: 0.8671 - val_accuracy: 0.5198 - val_auc: 0.5866\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 7s 171ms/step - loss: 0.7110 - accuracy: 0.5260 - auc: 0.5452 - val_loss: 0.7184 - val_accuracy: 0.5008 - val_auc: 0.5257\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 7s 173ms/step - loss: 0.6959 - accuracy: 0.5345 - auc: 0.5465 - val_loss: 1.3696 - val_accuracy: 0.5246 - val_auc: 0.5797\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 7s 175ms/step - loss: 0.7054 - accuracy: 0.5199 - auc: 0.5667 - val_loss: 0.6740 - val_accuracy: 0.5594 - val_auc: 0.6090\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 7s 179ms/step - loss: 0.6942 - accuracy: 0.5353 - auc: 0.5870 - val_loss: 0.6289 - val_accuracy: 0.6292 - val_auc: 0.6959\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 7s 175ms/step - loss: 0.7017 - accuracy: 0.5488 - auc: 0.5537 - val_loss: 0.6462 - val_accuracy: 0.6403 - val_auc: 0.6849\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 7s 170ms/step - loss: 0.6925 - accuracy: 0.5582 - auc: 0.5645 - val_loss: 0.7379 - val_accuracy: 0.4992 - val_auc: 0.5890\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 7s 172ms/step - loss: 0.7194 - accuracy: 0.5283 - auc: 0.5368 - val_loss: 0.6418 - val_accuracy: 0.6672 - val_auc: 0.7188\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 7s 176ms/step - loss: 0.6794 - accuracy: 0.5731 - auc: 0.6252 - val_loss: 0.7056 - val_accuracy: 0.5642 - val_auc: 0.6037\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 7s 170ms/step - loss: 0.6842 - accuracy: 0.5495 - auc: 0.6148 - val_loss: 0.6532 - val_accuracy: 0.5753 - val_auc: 0.6475\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 7s 173ms/step - loss: 0.6962 - accuracy: 0.5352 - auc: 0.5556 - val_loss: 0.7277 - val_accuracy: 0.5214 - val_auc: 0.5840\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 7s 174ms/step - loss: 0.7008 - accuracy: 0.5176 - auc: 0.5722 - val_loss: 0.6333 - val_accuracy: 0.6513 - val_auc: 0.7276\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 7s 170ms/step - loss: 0.6893 - accuracy: 0.5572 - auc: 0.5969 - val_loss: 0.6166 - val_accuracy: 0.6403 - val_auc: 0.7124\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 0.6896 - accuracy: 0.5399 - auc: 0.6028 - val_loss: 0.6342 - val_accuracy: 0.6719 - val_auc: 0.7197\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 7s 182ms/step - loss: 0.6937 - accuracy: 0.5478 - auc: 0.5903 - val_loss: 0.7053 - val_accuracy: 0.4897 - val_auc: 0.4899\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 7s 175ms/step - loss: 0.6880 - accuracy: 0.5416 - auc: 0.5941 - val_loss: 0.7272 - val_accuracy: 0.5468 - val_auc: 0.5753\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 7s 174ms/step - loss: 0.6808 - accuracy: 0.5700 - auc: 0.6173 - val_loss: 0.7893 - val_accuracy: 0.5293 - val_auc: 0.5966\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 7s 175ms/step - loss: 0.7002 - accuracy: 0.5446 - auc: 0.5780 - val_loss: 0.6700 - val_accuracy: 0.6117 - val_auc: 0.6320\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 0.6881 - accuracy: 0.5614 - auc: 0.6025 - val_loss: 0.6250 - val_accuracy: 0.6624 - val_auc: 0.7229\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 7s 179ms/step - loss: 0.6824 - accuracy: 0.5664 - auc: 0.6209 - val_loss: 0.6235 - val_accuracy: 0.6561 - val_auc: 0.7244\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 7s 184ms/step - loss: 0.6844 - accuracy: 0.5551 - auc: 0.6186 - val_loss: 0.7120 - val_accuracy: 0.5895 - val_auc: 0.6206\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 7s 181ms/step - loss: 0.6799 - accuracy: 0.5559 - auc: 0.6296 - val_loss: 0.6631 - val_accuracy: 0.6022 - val_auc: 0.6553\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 7s 171ms/step - loss: 0.6851 - accuracy: 0.5646 - auc: 0.6121 - val_loss: 0.6117 - val_accuracy: 0.6704 - val_auc: 0.7353\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 7s 179ms/step - loss: 0.6817 - accuracy: 0.5761 - auc: 0.6194 - val_loss: 0.6440 - val_accuracy: 0.6561 - val_auc: 0.6833\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 7s 174ms/step - loss: 0.6796 - accuracy: 0.5768 - auc: 0.6405 - val_loss: 0.6653 - val_accuracy: 0.5848 - val_auc: 0.6378\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 7s 182ms/step - loss: 0.6938 - accuracy: 0.5531 - auc: 0.5961 - val_loss: 0.6426 - val_accuracy: 0.6545 - val_auc: 0.7161\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 7s 181ms/step - loss: 0.6802 - accuracy: 0.5715 - auc: 0.6296 - val_loss: 0.6401 - val_accuracy: 0.6656 - val_auc: 0.7265\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 0.6821 - accuracy: 0.5634 - auc: 0.6259 - val_loss: 0.6180 - val_accuracy: 0.6624 - val_auc: 0.7222\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 0.6735 - accuracy: 0.5946 - auc: 0.6502 - val_loss: 0.6776 - val_accuracy: 0.5372 - val_auc: 0.5697\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 0.6814 - accuracy: 0.5703 - auc: 0.6239 - val_loss: 0.6178 - val_accuracy: 0.6688 - val_auc: 0.7284\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 7s 180ms/step - loss: 0.6694 - accuracy: 0.5891 - auc: 0.6503 - val_loss: 0.6775 - val_accuracy: 0.5911 - val_auc: 0.6173\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 0.6809 - accuracy: 0.5745 - auc: 0.6286 - val_loss: 0.6340 - val_accuracy: 0.6371 - val_auc: 0.7139\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 0.6709 - accuracy: 0.5940 - auc: 0.6693 - val_loss: 0.6786 - val_accuracy: 0.6022 - val_auc: 0.6339\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 7s 176ms/step - loss: 0.6829 - accuracy: 0.5631 - auc: 0.6417 - val_loss: 0.7020 - val_accuracy: 0.5198 - val_auc: 0.6148\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 7s 181ms/step - loss: 0.6932 - accuracy: 0.5317 - auc: 0.6027 - val_loss: 0.6334 - val_accuracy: 0.6561 - val_auc: 0.7169\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 7s 179ms/step - loss: 0.6844 - accuracy: 0.5771 - auc: 0.6454 - val_loss: 0.6769 - val_accuracy: 0.5895 - val_auc: 0.6150\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 7s 170ms/step - loss: 0.6700 - accuracy: 0.5835 - auc: 0.6635 - val_loss: 0.6451 - val_accuracy: 0.6307 - val_auc: 0.6679\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 0.6730 - accuracy: 0.5773 - auc: 0.6564 - val_loss: 0.6110 - val_accuracy: 0.6593 - val_auc: 0.7308\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 7s 180ms/step - loss: 0.6791 - accuracy: 0.5753 - auc: 0.6421 - val_loss: 0.6582 - val_accuracy: 0.6006 - val_auc: 0.6686\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 7s 176ms/step - loss: 0.6702 - accuracy: 0.5832 - auc: 0.6550 - val_loss: 0.6153 - val_accuracy: 0.6751 - val_auc: 0.7361\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6770 - accuracy: 0.5763 - auc: 0.6446 - val_loss: 0.6169 - val_accuracy: 0.6688 - val_auc: 0.7365\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 7s 174ms/step - loss: 0.6632 - accuracy: 0.6144 - auc: 0.6633 - val_loss: 0.6521 - val_accuracy: 0.5357 - val_auc: 0.6102\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 0.6649 - accuracy: 0.5946 - auc: 0.6633 - val_loss: 0.6163 - val_accuracy: 0.6561 - val_auc: 0.7233\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 7s 175ms/step - loss: 0.6632 - accuracy: 0.5938 - auc: 0.6781 - val_loss: 0.6260 - val_accuracy: 0.6545 - val_auc: 0.7154\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 7s 173ms/step - loss: 0.6735 - accuracy: 0.5809 - auc: 0.6511 - val_loss: 0.6512 - val_accuracy: 0.6450 - val_auc: 0.6632\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 7s 179ms/step - loss: 0.6731 - accuracy: 0.5879 - auc: 0.6656 - val_loss: 0.6522 - val_accuracy: 0.6323 - val_auc: 0.6624\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 7s 178ms/step - loss: 0.6706 - accuracy: 0.5908 - auc: 0.6618 - val_loss: 0.6386 - val_accuracy: 0.6323 - val_auc: 0.7073\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 7s 182ms/step - loss: 0.6758 - accuracy: 0.5818 - auc: 0.6608 - val_loss: 0.6314 - val_accuracy: 0.6751 - val_auc: 0.7251\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 7s 181ms/step - loss: 0.6745 - accuracy: 0.5873 - auc: 0.6492 - val_loss: 0.7205 - val_accuracy: 0.5705 - val_auc: 0.6212\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 7s 171ms/step - loss: 0.6665 - accuracy: 0.5972 - auc: 0.6710 - val_loss: 0.6553 - val_accuracy: 0.6181 - val_auc: 0.6591\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 7s 178ms/step - loss: 0.6709 - accuracy: 0.5679 - auc: 0.6669 - val_loss: 0.6319 - val_accuracy: 0.6387 - val_auc: 0.6972\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 7s 182ms/step - loss: 0.6734 - accuracy: 0.5776 - auc: 0.6635 - val_loss: 0.6382 - val_accuracy: 0.6529 - val_auc: 0.6957\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 8s 192ms/step - loss: 0.6646 - accuracy: 0.5950 - auc: 0.6695 - val_loss: 0.6371 - val_accuracy: 0.6292 - val_auc: 0.7095\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 6s 163ms/step - loss: 0.6648 - accuracy: 0.6143 - auc: 0.6755 - val_loss: 0.6404 - val_accuracy: 0.6482 - val_auc: 0.6904\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 7s 181ms/step - loss: 0.6680 - accuracy: 0.6070 - auc: 0.6690 - val_loss: 0.6315 - val_accuracy: 0.6593 - val_auc: 0.7090\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 7s 181ms/step - loss: 0.6618 - accuracy: 0.6095 - auc: 0.6965 - val_loss: 0.6297 - val_accuracy: 0.6688 - val_auc: 0.7062\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 7s 174ms/step - loss: 0.6646 - accuracy: 0.6038 - auc: 0.6838 - val_loss: 0.6425 - val_accuracy: 0.5927 - val_auc: 0.6546\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 7s 181ms/step - loss: 0.6689 - accuracy: 0.5951 - auc: 0.6761 - val_loss: 0.6435 - val_accuracy: 0.6133 - val_auc: 0.6610\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 7s 175ms/step - loss: 0.6710 - accuracy: 0.5872 - auc: 0.6872 - val_loss: 0.6259 - val_accuracy: 0.6529 - val_auc: 0.7118\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 0.6623 - accuracy: 0.6208 - auc: 0.6840 - val_loss: 0.6253 - val_accuracy: 0.6418 - val_auc: 0.7004\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 7s 181ms/step - loss: 0.6572 - accuracy: 0.6150 - auc: 0.6913 - val_loss: 0.6254 - val_accuracy: 0.6688 - val_auc: 0.7156\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 7s 182ms/step - loss: 0.6637 - accuracy: 0.6001 - auc: 0.7005 - val_loss: 0.6303 - val_accuracy: 0.6561 - val_auc: 0.7080\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 7s 170ms/step - loss: 0.6524 - accuracy: 0.6165 - auc: 0.7130 - val_loss: 0.6242 - val_accuracy: 0.6561 - val_auc: 0.7119\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 7s 177ms/step - loss: 0.6545 - accuracy: 0.6211 - auc: 0.7034 - val_loss: 0.6221 - val_accuracy: 0.6577 - val_auc: 0.7128\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 7s 184ms/step - loss: 0.6576 - accuracy: 0.6025 - auc: 0.6975 - val_loss: 0.6202 - val_accuracy: 0.6418 - val_auc: 0.7101\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 7s 177ms/step - loss: 0.6682 - accuracy: 0.5894 - auc: 0.6892 - val_loss: 0.6356 - val_accuracy: 0.6323 - val_auc: 0.6958\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 7s 171ms/step - loss: 0.6630 - accuracy: 0.6055 - auc: 0.6951 - val_loss: 0.6219 - val_accuracy: 0.6704 - val_auc: 0.7239\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 7s 179ms/step - loss: 0.6542 - accuracy: 0.6295 - auc: 0.6990 - val_loss: 0.6212 - val_accuracy: 0.6688 - val_auc: 0.7276\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 0.6540 - accuracy: 0.6078 - auc: 0.6989 - val_loss: 0.6278 - val_accuracy: 0.6545 - val_auc: 0.7104\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 7s 184ms/step - loss: 0.6674 - accuracy: 0.5764 - auc: 0.6836 - val_loss: 0.6395 - val_accuracy: 0.6260 - val_auc: 0.6836\n",
      "Epoch 74/100\n",
      "20/40 [==============>...............] - ETA: 3s - loss: 0.6530 - accuracy: 0.6195 - auc: 0.7198"
     ]
    }
   ],
   "source": [
    "n_epoch = 100\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "batch_size = 64\n",
    "params = {'batch_size': batch_size,\n",
    "#          'input_shape': (100, 313, 1),\n",
    "          'shuffle': True,\n",
    "          'beta_param': 0.7,\n",
    "          'mixup': True,\n",
    "#          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "          'highpass': [.5, [78,79,80,81,82,83,84,85]],\n",
    "          'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "#           'dropblock' : [30, 100]\n",
    "          #'device' : device\n",
    "}\n",
    "\n",
    "params_no_shuffle = {'batch_size': batch_size,\n",
    "#          'input_shape': (100, 313, 1),\n",
    "          'shuffle': False,\n",
    "          'beta_param': 0.7,\n",
    "          'mixup': False\n",
    "          #'device': device\n",
    "}\n",
    "\n",
    "TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], features_trn['preg'], features_trn['loc'], \n",
    "           features_trn['mel1'],features_trn['cqt1'],features_trn['stft1']], \n",
    "                        out_lbs_trn,  ## our Y\n",
    "                        **params)()\n",
    "\n",
    "#ValDGen_1 = Generator0([features_test[0]['age'],features_test[0]['sex'], features_test[0]['hw'], features_test[0]['preg'], features_test[0]['loc'], \n",
    "#           features_test[0]['mel1'],features_test[0]['cqt1'],features_test[0]['stft1']], \n",
    "#                        features_test[2],  ## our Y\n",
    "#                        **params_no_shuffle)()\n",
    "\n",
    "    \n",
    "model2.fit(TrainDGen_1,\n",
    "          validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                              features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                              features_test['cqt1'], features_test['stft1']], \n",
    "                             out_lbs_test), \n",
    "                             callbacks=[lr],\n",
    "                              steps_per_epoch=np.ceil(len(out_lbs_trn)/64),\n",
    "                             epochs = n_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5fa63b",
   "metadata": {},
   "source": [
    "### 팀코드 수정해야 할 함수들.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eacb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da9864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_feature['mel_shape'] = mel_input_shape\n",
    "params_feature['cqt_shape'] = cqt_input_shape\n",
    "params_feature['stft_shape'] = stft_input_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ba728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "def save_challenge_model(model_folder, model1, model2, m_name1, m_name2, param_feature) :\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    filename1 = os.path.join(model_folder, m_name1 + '_model1.hdf5')\n",
    "    filename2 = os.path.join(model_folder, m_name2 + '_model2.hdf5')\n",
    "    model1.save(filename1)\n",
    "    model2.save(filename2)\n",
    "    param_feature['model1'] = m_name1\n",
    "    param_feature['model2'] = m_name2\n",
    "    param_feature['model_fnm1'] = filename1\n",
    "    param_feature['model_fnm2'] = filename2\n",
    "\n",
    "    with open(info_fnm, 'wb') as f:\n",
    "        pk.dump(param_feature, f, pk.HIGHEST_PROTOCOL)\n",
    "    return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb83741",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_challenge_model(model_folder, model1, model2, m_name1 = 'lcnn1', m_name2 = 'lcnn2', param_feature = params_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67f2a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_challenge_model(model_folder, verbose):\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    with open(info_fnm, 'rb') as f:\n",
    "        info_m = pk.load(f)\n",
    "#    if info_m['model'] == 'toy' :\n",
    "#        model = get_toy(info_m['mel_shape'])\n",
    "#    filename = os.path.join(model_folder, info_m['model'] + '_model.hdf5')\n",
    "#    model.load_weights(filename)\n",
    "    return info_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_challenge_model(model, data, recordings, verbose):\n",
    "    \n",
    "    murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "    outcome_classes = ['Abnormal', 'Normal']\n",
    "    \n",
    "    if model['model1'] == 'toy1' :\n",
    "        model1 = get_toy5_1(model['mel_shape'],model['cqt_shape'],model['stft_shape'] )\n",
    "    if model['model2'] == 'toy2' :\n",
    "        model2 = get_toy5_2(model['mel_shape'],model['cqt_shape'],model['stft_shape'])\n",
    "    if model['model1'] == 'lcnn1' :\n",
    "        model1 = get_LCNN_1(model['mel_shape'],model['cqt_shape'],model['stft_shape'], use_mel = True, use_cqt = False, use_stft = False)\n",
    "    if model['model2'] == 'lcnn2' :\n",
    "        model2 = get_LCNN_2(model['mel_shape'],model['cqt_shape'],model['stft_shape'], use_mel = True, use_cqt = False, use_stft = False)\n",
    "    model1.load_weights(model['model_fnm1'])\n",
    "    model2.load_weights(model['model_fnm2'])\n",
    "    \n",
    "#    classes = model['classes']\n",
    "    # Load features.\n",
    "    features = get_feature_one(data, verbose = 0)\n",
    "\n",
    "    samp_sec = model['samp_sec'] \n",
    "    pre_emphasis = model['pre_emphasis']\n",
    "    hop_length = model['hop_length']\n",
    "    win_length = model['win_length']\n",
    "    n_mels = model['n_mels']\n",
    "    filter_scale = model['filter_scale']\n",
    "    n_bins = model['n_bins']\n",
    "    fmin = model['fmin']\n",
    "    \n",
    "    features['mel1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        mel1 = feature_extract_melspec(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length, \n",
    "                                           win_length = win_length, n_mels = n_mels)[0]\n",
    "        features['mel1'].append(mel1)\n",
    "    M, N = features['mel1'][0].shape\n",
    "    for i in range(len(features['mel1'])) :\n",
    "        features['mel1'][i] = features['mel1'][i].reshape(M,N,1)   \n",
    "    features['mel1'] = np.array(features['mel1'])\n",
    "\n",
    "    features['cqt1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        mel1 = feature_extract_cqt(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, filter_scale = filter_scale, \n",
    "                                        n_bins = n_bins, fmin = fmin)[0]\n",
    "        features['cqt1'].append(mel1)\n",
    "    M, N = features['cqt1'][0].shape\n",
    "    for i in range(len(features['cqt1'])) :\n",
    "        features['cqt1'][i] = features['cqt1'][i].reshape(M,N,1)   \n",
    "    features['cqt1'] = np.array(features['cqt1'])\n",
    "\n",
    "    features['stft1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        mel1 = feature_extract_stft(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length, \n",
    "                                       win_length = win_length)[0]\n",
    "        features['stft1'].append(mel1)\n",
    "    M, N = features['stft1'][0].shape\n",
    "    for i in range(len(features['stft1'])) :\n",
    "        features['stft1'][i] = features['stft1'][i].reshape(M,N,1)           \n",
    "    features['stft1'] = np.array(features['stft1'])\n",
    "\n",
    "    #    print(features)\n",
    "    # Impute missing data.\n",
    "    res1 = model1.predict([features['age'], features['sex'], features['hw'], features['preg'], features['loc'], features['mel1'], features['cqt1'], features['stft1']])\n",
    "    res2 = model2.predict([features['age'], features['sex'], features['hw'], features['preg'], features['loc'], features['mel1'], features['cqt1'], features['stft1']])\n",
    "\n",
    "    # Get classifier probabilities.\n",
    "    idx1 = res1.argmax(axis=0)[0]\n",
    "    murmur_probabilities = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "    outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "#    idx = np.argmax(prob1)\n",
    "\n",
    "    ## 이부분도 생각 필요.. rule 을 cost를 maximize 하는 기준으로 threshold 탐색 필요할지도..\n",
    "    # Choose label with highest probability.\n",
    "    murmur_labels = np.zeros(len(murmur_classes), dtype=np.int_)\n",
    "    idx = np.argmax(murmur_probabilities)\n",
    "    murmur_labels[idx] = 1\n",
    "    outcome_labels = np.zeros(len(outcome_classes), dtype=np.int_)\n",
    "    idx = np.argmax(outcome_probabilities)\n",
    "    outcome_labels[idx] = 1\n",
    "    \n",
    "    # Concatenate classes, labels, and probabilities.\n",
    "    classes = murmur_classes + outcome_classes\n",
    "    labels = np.concatenate((murmur_labels, outcome_labels))\n",
    "    probabilities = np.concatenate((murmur_probabilities, outcome_probabilities))\n",
    "    \n",
    "    return classes, labels, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b28b4c2",
   "metadata": {},
   "source": [
    "### 대회 평가용 run_model 함수 (수정 불필요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769299d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model.\n",
    "def run_model(model_folder, data_folder, output_folder, allow_failures, verbose):\n",
    "    # Load models.\n",
    "    if verbose >= 1:\n",
    "        print('Loading Challenge model...')\n",
    "\n",
    "    model = load_challenge_model(model_folder, verbose) ### Teams: Implement this function!!!\n",
    "\n",
    "    # Find the patient data files.\n",
    "    patient_files = find_patient_files(data_folder)\n",
    "    num_patient_files = len(patient_files)\n",
    "\n",
    "    if num_patient_files==0:\n",
    "        raise Exception('No data was provided.')\n",
    "\n",
    "    # Create a folder for the Challenge outputs if it does not already exist.\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Run the team's model on the Challenge data.\n",
    "    if verbose >= 1:\n",
    "        print('Running model on Challenge data...')\n",
    "\n",
    "    # Iterate over the patient files.\n",
    "    for i in range(num_patient_files):\n",
    "        if verbose >= 2:\n",
    "            print('    {}/{}...'.format(i+1, num_patient_files))\n",
    "\n",
    "        patient_data = load_patient_data(patient_files[i])\n",
    "        recordings = load_recordings(data_folder, patient_data)\n",
    "\n",
    "        # Allow or disallow the model to fail on parts of the data; helpful for debugging.\n",
    "        try:\n",
    "            classes, labels, probabilities = run_challenge_model(model, patient_data, recordings, verbose) ### Teams: Implement this function!!!\n",
    "        except:\n",
    "            if allow_failures:\n",
    "                if verbose >= 2:\n",
    "                    print('... failed.')\n",
    "                classes, labels, probabilities = list(), list(), list()\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # Save Challenge outputs.\n",
    "        head, tail = os.path.split(patient_files[i])\n",
    "        root, extension = os.path.splitext(tail)\n",
    "        output_file = os.path.join(output_folder, root + '.csv')\n",
    "        patient_id = get_patient_id(patient_data)\n",
    "        save_challenge_outputs(output_file, patient_id, classes, labels, probabilities)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba835b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 우리 모형 저장된 폴더이름\n",
    "model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a3344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 테스트 데이터 폴더\n",
    "test_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d52354",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 테스트 데이터에 모형 돌려서 스코어 저장할 폴더\n",
    "output_folder = '/home/ubuntu/hmd/notebooks/out_lcnn2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e825de39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_model(model_folder, test_folder, output_folder, allow_failures = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b11ec46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat /home/ubuntu/hmd/notebooks/out_lcnn1/49628.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d97d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "murmur_scores, outcome_scores = evaluate_model(test_folder, output_folder)\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "    + '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "\n",
    "if len(sys.argv) == 3:\n",
    "    print(output_string)\n",
    "elif len(sys.argv) == 4:\n",
    "    with open(sys.argv[3], 'w') as f:\n",
    "        f.write(output_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd3ae37",
   "metadata": {},
   "source": [
    "### Threshold 변경해가며 결과 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f7a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_folder = test_folder\n",
    "murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "# Load and parse label and model output files.\n",
    "label_files, output_files = find_challenge_files(label_folder, output_folder)\n",
    "murmur_labels = load_murmurs(label_files, murmur_classes)\n",
    "murmur_binary_outputs, murmur_scalar_outputs = load_classifier_outputs(output_files, murmur_classes)\n",
    "outcome_labels = load_outcomes(label_files, outcome_classes)\n",
    "outcome_binary_outputs, outcome_scalar_outputs = load_classifier_outputs(output_files, outcome_classes)\n",
    "\n",
    "\n",
    "print(np.mean(murmur_scalar_outputs[:,0]))\n",
    "print(np.mean(murmur_scalar_outputs[:,2]))\n",
    "print(np.mean(outcome_scalar_outputs[:,0]))\n",
    "print(np.mean(outcome_scalar_outputs[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b61ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## threshold 바꿔가면서 결과 출력\n",
    "\n",
    "for th1 in [0.01, 0.05, 0.1, 0.15,0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8] :\n",
    "    murmur_binary_outputs[:,0] = murmur_scalar_outputs[:,0] > th1\n",
    "    murmur_binary_outputs[:,2] = murmur_scalar_outputs[:,2] > 1 - th1\n",
    "    outcome_binary_outputs[:,0] = outcome_scalar_outputs[:,0] > th1\n",
    "    outcome_binary_outputs[:,1] = outcome_scalar_outputs[:,1] > 1 - th1\n",
    "    # For each patient, set the 'Present' or 'Abnormal' class to positive if no class is positive or if multiple classes are positive.\n",
    "    murmur_labels = enforce_positives(murmur_labels, murmur_classes, 'Present')\n",
    "    murmur_binary_outputs = enforce_positives(murmur_binary_outputs, murmur_classes, 'Present')\n",
    "    outcome_labels = enforce_positives(outcome_labels, outcome_classes, 'Abnormal')\n",
    "    outcome_binary_outputs = enforce_positives(outcome_binary_outputs, outcome_classes, 'Abnormal')\n",
    "    # Evaluate the murmur model by comparing the labels and model outputs.\n",
    "    murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes = compute_auc(murmur_labels, murmur_scalar_outputs)\n",
    "    murmur_f_measure, murmur_f_measure_classes = compute_f_measure(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_accuracy, murmur_accuracy_classes = compute_accuracy(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_weighted_accuracy = compute_weighted_accuracy(murmur_labels, murmur_binary_outputs, murmur_classes) # This is the murmur scoring metric.\n",
    "    murmur_cost = compute_cost(outcome_labels, murmur_binary_outputs, outcome_classes, murmur_classes) # Use *outcomes* to score *murmurs* for the Challenge cost metric, but this is not the actual murmur scoring metric.\n",
    "    murmur_scores = (murmur_classes, murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes, \\\n",
    "                 murmur_f_measure, murmur_f_measure_classes, murmur_accuracy, murmur_accuracy_classes, murmur_weighted_accuracy, murmur_cost)\n",
    "\n",
    "    # Evaluate the outcome model by comparing the labels and model outputs.\n",
    "    outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes = compute_auc(outcome_labels, outcome_scalar_outputs)\n",
    "    outcome_f_measure, outcome_f_measure_classes = compute_f_measure(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_accuracy, outcome_accuracy_classes = compute_accuracy(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_weighted_accuracy = compute_weighted_accuracy(outcome_labels, outcome_binary_outputs, outcome_classes)\n",
    "    outcome_cost = compute_cost(outcome_labels, outcome_binary_outputs, outcome_classes, outcome_classes) # This is the clinical outcomes scoring metric.\n",
    "    outcome_scores = (outcome_classes, outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes, \\\n",
    "                  outcome_f_measure, outcome_f_measure_classes, outcome_accuracy, outcome_accuracy_classes, outcome_weighted_accuracy, outcome_cost)\n",
    "\n",
    "\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "    murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "    outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "                + '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "    print(\"threshold: \", th1)\n",
    "    print(output_string)\n",
    "    print(\"-------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a223bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2.4_p37)",
   "language": "python",
   "name": "conda_tensorflow2.4_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

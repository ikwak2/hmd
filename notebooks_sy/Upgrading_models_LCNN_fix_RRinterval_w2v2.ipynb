{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1507a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39144/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cbc0e3",
   "metadata": {},
   "source": [
    "## RR interval 사용방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e1e2dd",
   "metadata": {},
   "source": [
    "1. [S1-S1-Phonocardiogram](https://github.com/kevinmgamboa/S1-S1-Phonocardiogram-Peak-Detection-Method-in-Python) git clone 진행\n",
    "2. [PeakUtils](https://bitbucket.org/lucashnegri/peakutils/src/master/) git clone 진행후 **python setup.py install**  \n",
    "\n",
    "\n",
    "* RR interval feature 추출할 때 오류가 발생한다면, Filters1.mat 파일 복사해서 현재 notebook 파일이 있는 위치에 Filters1.mat 이름으로 위치시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "147b01fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt                 # Librery to load .mat files\n",
    "import peakutils                                # Librery to help in peak detection\n",
    "from scipy import special\n",
    "\n",
    "##----------------------------------------------------------------------------\n",
    "# FUNCTIONS: \"execfile\"\n",
    "##----------------------------------------------------------------------------\n",
    "''' This Function creates a time-vector for any signal given the sampling frequency\n",
    "    and the duration of a signal'''\n",
    "def time_vector(sampling_frequency,duration): \n",
    "\tnumber_samples= sampling_frequency*duration;\n",
    "\tresult=np.arange(1,duration/number_samples,duration-duration/number_samples);\n",
    "\n",
    "\treturn result\n",
    "\n",
    "##----------------------------------------------------------------------------\n",
    "''' Derivate of an input signal as y[n]= x[n+1]- x[n-1] \n",
    "'''\n",
    "def derivate (x):\n",
    "\tlenght=np.shape(x)\t\t\t\t# Get the length of the vector\t\t\n",
    "\tlenght=lenght[0];\t\t\t\t# Get the value of the length\n",
    "\ty=np.zeros(lenght);\t\t\t\t# Initializate derivate vector\n",
    "\tfor i in range(lenght-1):\n",
    "\t\t\ty[i]=x[i+1]-x[i];\t\t\n",
    "\treturn y\n",
    "\n",
    "##----------------------------------------------------------------------------\n",
    "'''To normalized any vector\\0-dimentional array in [-1,1] range, by divided the \n",
    "   vector by the maximun value of itself, substracting the mean value to the vector\n",
    "   & dividing each value of the vector by the maximun value of itself \n",
    "'''\n",
    "def vec_nor(x):\n",
    "\tlenght=np.shape(x)\t\t\t\t# Get the length of the vector\t\t\n",
    "\tlenght=lenght[0];\t\t\t\t# Get the value of the length\n",
    "\txMax=max(x);\t\t\t\t\t# Get the maximun value of the vector\n",
    "\tnVec=np.zeros(lenght);\t\t\t        # Initializate derivate vector\n",
    "\tfor n in range(lenght):\n",
    "\t\tnVec[n]=x[n]/xMax;\t\t\t\n",
    "\tnVec=nVec-np.mean(nVec);\n",
    "\tnVec=np.divide(nVec,np.max(nVec));\n",
    "\treturn nVec\n",
    "##----------------------------------------------------------------------------\n",
    "'''\n",
    "  FpassBand is the function that develop a pass band filter of the signal 'x' through the\n",
    "  discrete convolution of this 'x' first with the coeficients of a High Pass Filter 'hp' and then\n",
    "  with the discrete convolution of this result with a Low Pass Filter 'lp'\n",
    "'''\n",
    "def FpassBand(X,hp,lp):\n",
    "        llp=np.shape(lp)\t  \t        # Get the length of the lowpass vector\t\t\n",
    "        llp=llp[0];\t\t\t\t# Get the value of the length\n",
    "        lhp=np.shape(hp)\t\t\t# Get the length of the highpass vector\t\t\n",
    "        lhp=lhp[0];\t\t\t\t# Get the value of the length\t\n",
    "\n",
    "        x=np.convolve(X,lp);\t\t        # Disrete convolution \n",
    "        x=x[round(llp/2):round(-1-(llp/2))];\n",
    "        x=x-(np.mean(x));\n",
    "        x=x/np.max(x);\n",
    "\t\n",
    "        y=np.convolve(x,hp);\t\t\t# Disrete onvolution\n",
    "        y=y[round((lhp/2)):round(-1-(lhp/2))];\n",
    "        y=y-np.mean(y);\n",
    "        y=y/np.max(y);\n",
    "\n",
    "        x=np.convolve(y,lp);\t\t        # Disrete convolution \n",
    "        x=x[round((llp/2)):round(-1-(llp/2))];\n",
    "        x=x-(np.mean(x));\n",
    "        x=x/np.max(x);\n",
    "\t\n",
    "        y=np.convolve(x,hp);\t\t\t# Disrete onvolution\n",
    "        y=y[round((lhp/2)):round(-1-(lhp/2))];\n",
    "        y=y-np.mean(y);\n",
    "        y=y/np.max(y);\n",
    "        \n",
    "        y=vec_nor(y);\t\t\t\t# Vector Normalizing\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a6057f",
   "metadata": {},
   "source": [
    "## LSTM_fix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eeb035e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Data2/hmd/hmd_sy/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a45589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import librosa\n",
    "import librosa.display\n",
    "import math\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0,'/Data2/hmd/hmd_sy/evaluation-2022')\n",
    "\n",
    "\n",
    "## RR interval\n",
    "sys.path.insert(0,'peakutils')\n",
    "sys.path.insert(0,'S1-S1-Phonocardiogram-Peak-Detection-Method-in-Python')\n",
    "import peakutils\n",
    "sys.path.insert(0,'utils')\n",
    "\n",
    "from helper_code import *\n",
    "from get_feature import *\n",
    "from models import *\n",
    "from Generator0 import *\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from evaluate_model import *\n",
    "from scipy import special\n",
    "import scipy.io as sio\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9edeaf74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Data2/hmd/hmd_sy/notebooks'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52a9d85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 12 02:08:50 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     On   | 00000000:1A:00.0 Off |                  Off |\n",
      "| 35%   60C    P2   192W / 260W |  12944MiB / 48601MiB |     67%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000     On   | 00000000:3D:00.0 Off |                  Off |\n",
      "| 48%   70C    P2   255W / 260W |  48122MiB / 48601MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Quadro RTX 8000     On   | 00000000:3E:00.0 Off |                  Off |\n",
      "| 43%   66C    P2   235W / 260W |  12944MiB / 48601MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro RTX 8000     On   | 00000000:40:00.0 Off |                  Off |\n",
      "| 33%   26C    P8     7W / 260W |      3MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Quadro RTX 8000     On   | 00000000:41:00.0 Off |                  Off |\n",
      "| 33%   31C    P8    13W / 260W |  48597MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb1c20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[3], 'GPU')\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b6c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder =  '/Data2/hmd/physionet.org/files/circor-heart-sound/1.0.3/training_data'\n",
    "train_folder =  '/Data2/hmd/data_split/murmur/train'\n",
    "test_folder = '/Data2/hmd/data_split/murmur/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a5342b",
   "metadata": {},
   "source": [
    "## wav2vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2492c0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-12 02:09:07.733529: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-12 02:09:08.454015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 47232 MB memory:  -> device: 3, name: Quadro RTX 8000, pci bus id: 0000:40:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 246000)]          0         \n",
      "_________________________________________________________________\n",
      "keras_layer (KerasLayer)     (None, 768, 32)           94396320  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 768, 10)           330       \n",
      "=================================================================\n",
      "Total params: 94,396,650\n",
      "Trainable params: 94,396,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from wav2vec2 import Wav2Vec2Config\n",
    "\n",
    "model_url = \"https://tfhub.dev/vasudevgupta7/wav2vec2-960h/1\"\n",
    "pretrained_layer = hub.KerasLayer(model_url, trainable=True)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(246000,))\n",
    "hidden_states = pretrained_layer(inputs)\n",
    "outputs = tf.keras.layers.Dense(10)(hidden_states)\n",
    "\n",
    "wav2vec2_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "wav2vec2_model.summary()\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def forward(speech):\n",
    "    return wav2vec2_model(speech, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ec7242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdd45d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidal_decay(e, start=0, end=100, lr_start=1e-3, lr_end=1e-5):\n",
    "    if e < start:\n",
    "        return lr_start\n",
    "    elif e > end:\n",
    "        return lr_end\n",
    "\n",
    "    middle = (start + end) / 2\n",
    "    s = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return s(13 * (-e + middle) / np.abs(end - start)) * np.abs(lr_start - lr_end) + lr_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1715f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model.\n",
    "def run_model(model_folder, data_folder, output_folder, allow_failures, verbose):\n",
    "    # Load models.\n",
    "    if verbose >= 1:\n",
    "        print('Loading Challenge model...')\n",
    "\n",
    "    model = load_challenge_model(model_folder, verbose) ### Teams: Implement this function!!!\n",
    "\n",
    "    # Find the patient data files.\n",
    "    patient_files = find_patient_files(data_folder)\n",
    "    num_patient_files = len(patient_files)\n",
    "\n",
    "    if num_patient_files==0:\n",
    "        raise Exception('No data was provided.')\n",
    "\n",
    "    # Create a folder for the Challenge outputs if it does not already exist.\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Run the team's model on the Challenge data.\n",
    "    if verbose >= 1:\n",
    "        print('Running model on Challenge data...')\n",
    "\n",
    "#    @tf.function\n",
    "    # Iterate over the patient files.\n",
    "    for i in range(num_patient_files):\n",
    "        if verbose >= 2:\n",
    "            print('    {}/{}...'.format(i+1, num_patient_files))\n",
    "\n",
    "        patient_data = load_patient_data(patient_files[i])\n",
    "        recordings = load_recordings(data_folder, patient_data)\n",
    "\n",
    "        # Allow or disallow the model to fail on parts of the data; helpful for debugging.\n",
    "        try:\n",
    "            classes, labels, probabilities = run_challenge_model(model, patient_data, recordings, verbose) ### Teams: Implement this function!!!\n",
    "        except:\n",
    "            if allow_failures:\n",
    "                if verbose >= 2:\n",
    "                    print('... failed.')\n",
    "                classes, labels, probabilities = list(), list(), list()\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # Save Challenge outputs.\n",
    "        head, tail = os.path.split(patient_files[i])\n",
    "        root, extension = os.path.splitext(tail)\n",
    "        output_file = os.path.join(output_folder, root + '.csv')\n",
    "        patient_id = get_patient_id(patient_data)\n",
    "        save_challenge_outputs(output_file, patient_id, classes, labels, probabilities)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print('Done.')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f82550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "def save_challenge_model(model_folder, model1, model2, m_name1, m_name2, param_feature) :\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    filename1 = os.path.join(model_folder, m_name1 + '_model1.hdf5')\n",
    "    filename2 = os.path.join(model_folder, m_name2 + '_model2.hdf5')\n",
    "    model1.save(filename1)\n",
    "    model2.save(filename2)\n",
    "    param_feature['model1'] = m_name1\n",
    "    param_feature['model2'] = m_name2\n",
    "    param_feature['model_fnm1'] = filename1\n",
    "    param_feature['model_fnm2'] = filename2\n",
    "    with open(info_fnm, 'wb') as f:\n",
    "        pk.dump(param_feature, f, pk.HIGHEST_PROTOCOL)\n",
    "    return 1\n",
    "\n",
    "def load_challenge_model(model_folder, verbose):\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    with open(info_fnm, 'rb') as f:\n",
    "        info_m = pk.load(f)\n",
    "#    if info_m['model'] == 'toy' :\n",
    "#        model = get_toy(info_m['mel_shape'])\n",
    "#    filename = os.path.join(model_folder, info_m['model'] + '_model.hdf5')\n",
    "#    model.load_weights(filename)\n",
    "    return info_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c177a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_challenge_model(model, data, recordings, verbose):\n",
    "    \n",
    "    murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "    outcome_classes = ['Abnormal', 'Normal']\n",
    "    \n",
    "    if model['model1'] == 'lcnn1_dr' :\n",
    "        model1 = get_LCNN_o_4_dr_1(model['mel_shape'],model['cqt_shape'],model['stft_shape'], \n",
    "                                 model['interval_input_shape'],model['wav2_input_shape'],use_wav2=model['use_wav2'],\n",
    "                                 use_mel = model['use_mel'],use_cqt = model['use_cqt'], use_stft = model['use_stft'], \n",
    "                                 ord1 = model['ord1'], dp = model['dp'], fc = model['fc'], ext = model['ext'])\n",
    "    if model['model2'] == 'lcnn2_dr' :\n",
    "        model2 = get_LCNN_o_4_dr_1(model['mel_shape'],model['cqt_shape'],model['stft_shape'], \n",
    "                               model['interval_input_shape'],model['wav2_input_shape'],use_wav2=model['use_wav2'],\n",
    "                               use_mel = model['use_mel'], use_cqt = model['use_cqt'], use_stft = model['use_stft'], \n",
    "                               dp = model['dp'], fc = model['fc'], ext = model['ext'])\n",
    "    model1.load_weights(model['model_fnm1'])\n",
    "    model2.load_weights(model['model_fnm2'])\n",
    "    \n",
    "\n",
    "    \n",
    "    maxlen1 = params_feature['maxlen1']\n",
    "    min_dist = params_feature['min_dist']\n",
    "    max_interval_len = params_feature['max_interval_len']\n",
    "    \n",
    "    \n",
    "    # Load features.\n",
    "    features = get_feature_one(data, verbose = 0)\n",
    "\n",
    "    samp_sec = model['samp_sec'] \n",
    "    pre_emphasis = model['pre_emphasis']\n",
    "    hop_length = model['hop_length']\n",
    "    win_length = model['win_length']\n",
    "    n_mels = model['n_mels']\n",
    "    filter_scale = model['filter_scale']\n",
    "    n_bins = model['n_bins']\n",
    "    fmin = model['fmin']\n",
    "    use_mel = model['use_mel']\n",
    "    use_cqt = model['use_cqt']\n",
    "    use_stft = model['use_stft']\n",
    "#    use_raw = model['use_raw']\n",
    "    trim = model['trim']\n",
    "    \n",
    "    # wav2vec2\n",
    "    use_wav2 = model['use_wav2']\n",
    "    maxlen1 = model['maxlen1']\n",
    "    features['wav2'] = []\n",
    "    tmp_wav=[] \n",
    "    tmp = []\n",
    "    \n",
    "    # RR interval\n",
    "    use_interval = model['use_interval']\n",
    "    min_dist = model['min_dist']\n",
    "    max_interval_len = model['max_interval_len']\n",
    "    per_sec = model['per_sec']\n",
    "\n",
    "    features['interval'] = []\n",
    "    tmp_total_interval = [] \n",
    "    \n",
    "    if use_wav2:\n",
    "\n",
    "        for i in range(len(recordings)):\n",
    "            data = recordings[i]\n",
    "            tmp_wav.append(data)\n",
    "\n",
    "        padded =pad_sequences(tmp_wav, maxlen=maxlen1, dtype='float64', padding='post', truncating='post', value=0.0)\n",
    "\n",
    "        padded=np.array(padded, dtype=np.float32)\n",
    "        for i in range(len(padded)):\n",
    "            features['wav2'].append(padded[i])\n",
    "\n",
    "        features['wav2'] = np.array(features['wav2'])\n",
    "    \n",
    "\n",
    "       \n",
    "    if use_interval:\n",
    "\n",
    "\n",
    "        for i in tqdm.tqdm(range(len(recordings))) :\n",
    "            \n",
    "            datos=recordings[i]\n",
    "            filtros=sio.loadmat('./Filters1')\n",
    "            tmp_interval = []\n",
    "            n_samp = len(datos)//per_sec\n",
    "\n",
    "\n",
    "            try:\n",
    "                for k in range(n_samp):\n",
    "                    \n",
    "                    X = datos[k*per_sec:(k+1)*per_sec]\n",
    "                    Fs= per_sec\n",
    "                    Fpa20=filtros['Fpa20'];\t\t\t        # High pass filter\n",
    "                    Fpa20=Fpa20[0];\t\t\t\t\t# High pass filter\n",
    "                    Fpb100=filtros['Fpb100'];\t\t        # Low-pass Filter\n",
    "                    Fpb100=Fpb100[0];\t\t\t\t# Low-pass Filter\n",
    "                    Xf=FpassBand(X,Fpa20,Fpb100); \t                # Apply a passband filter\n",
    "                    Xf=vec_nor(Xf);\t\t\t\n",
    "\n",
    "                # Derivate of the Signal\n",
    "                    dX=derivate(Xf);\t\t\t\t# Derivate of the signal\n",
    "                    dX=vec_nor(dX);\t\t\t\t\t# Vector Normalizing\n",
    "                # Square of the signal\n",
    "                    dy=np.square(Xf);\n",
    "                    dy=vec_nor(dy);\n",
    "\n",
    "                    size=np.shape(Xf)\t\t\t\t# Rank or dimension of the array\n",
    "                    fil=size[0];\t\t\t\t\t# Number of rows\n",
    "\n",
    "                    positive=np.zeros((1,fil+1));                   # Initializating Positives Values Vector \n",
    "                    positive=positive[0];                           # Getting the Vector\n",
    "\n",
    "                    points=np.zeros((1,fil));                       # Initializating the all Peak Points Vector\n",
    "                    points=points[0];                               # Getting the point vector\n",
    "\n",
    "                    peaks=np.zeros((1,fil));                        # Initializating the s1-s1 Peak Vector\n",
    "                    peaks=peaks[0];                                 # Getting the point vector\n",
    "\n",
    "\n",
    "                    for i in range(0,fil):\n",
    "                        if dX[i]>0:\n",
    "                            positive[i]=1;\n",
    "                        else:\n",
    "                            positive[i]=0;\n",
    "\n",
    "                    for i in range(0,fil):\n",
    "                        if (positive[i]==1 and positive[i+1]==0):\n",
    "                            points[i]=Xf[i];\n",
    "                        else:\n",
    "                            points[i]=0;\n",
    "\n",
    "                    indexes=peakutils.indexes(points,thres=0.5/max(points), min_dist=min_dist);\n",
    "                    lenght=np.shape(indexes)\t\t\t# Get the length of the index vector\t\t\n",
    "                    lenght=lenght[0];\t\t\t\t# Get the value of the index vector\n",
    "\n",
    "                    for i in range(0,lenght):\n",
    "                        p=indexes[i];\n",
    "                        peaks[p]=points[p];\n",
    "\n",
    "                    n=np.arange(0,fil);                            # Vector to the X axes (Number of Samples)\n",
    "                    indexes =indexes+(k*per_sec)    \n",
    "                    tmp_peaks = np.array(indexes)\n",
    "\n",
    "\n",
    "                    tmp_interval.extend(tmp_peaks)\n",
    "\n",
    "                tmp_interval = np.array(tmp_interval)\n",
    "                tmp_interval = np.diff(tmp_interval)\n",
    "\n",
    "                tmp_total_interval.append(tmp_interval)\n",
    "\n",
    "\n",
    "            except:\n",
    "                print(i)\n",
    "                tmp_peaks = np.zeros(max_interval_len)\n",
    "                tmp_total_interval.append(tmp_peaks)\n",
    "\n",
    "    else :\n",
    "\n",
    "        tmp_peaks = np.zeros(max_interval_len)\n",
    "        tmp_total_interval.append(tmp_peaks)       \n",
    "\n",
    "\n",
    "    if use_interval:\n",
    "          \n",
    "        padded =pad_sequences(tmp_total_interval, maxlen=max_interval_len, dtype='float64', padding='post', truncating='post', value=0.0)\n",
    "\n",
    "        for i in range(len(padded)):\n",
    "            features['interval'].append(padded[i])\n",
    "             \n",
    "        for i in range(len(features['interval'])):\n",
    "            features['interval'][i]= features['interval'][i].reshape(-1,1)\n",
    "    \n",
    "        features['interval']=np.array(features['interval'])\n",
    "    \n",
    "        \n",
    "    else:\n",
    "        for i in range(len(tmp_interval)):\n",
    "            features['interval'].append(tmp_total_interval[i])\n",
    "        \n",
    "        for i in range(len(features['interval'])):\n",
    "            features['interval'][i]= features['interval'][i].reshape(-1,1)\n",
    "        features['interval']=np.array(features['interval'])\n",
    "        \n",
    "        \n",
    "    \n",
    "    features['mel1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_mel :\n",
    "            mel1 = feature_extract_melspec(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length, \n",
    "                                           win_length = win_length, n_mels = n_mels, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1) )\n",
    "        features['mel1'].append(mel1)\n",
    "    M, N = features['mel1'][0].shape\n",
    "\n",
    "    if use_mel :\n",
    "        for i in range(len(features['mel1'])) :\n",
    "            features['mel1'][i] = features['mel1'][i].reshape(M,N,1)\n",
    "    features['mel1'] = np.array(features['mel1'])\n",
    "\n",
    "    features['cqt1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_cqt :\n",
    "            mel1 = feature_extract_cqt(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, filter_scale = filter_scale, \n",
    "                                        n_bins = n_bins, fmin = fmin, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1))\n",
    "        features['cqt1'].append(mel1)\n",
    "    M, N = features['cqt1'][0].shape\n",
    "    if use_cqt :\n",
    "        for i in range(len(features['cqt1'])) :\n",
    "            features['cqt1'][i] = features['cqt1'][i].reshape(M,N,1)   \n",
    "    features['cqt1'] = np.array(features['cqt1'])\n",
    "\n",
    "    features['stft1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_stft :\n",
    "            mel1 = feature_extract_stft(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length, \n",
    "                                        win_length = win_length, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1) )\n",
    "        features['stft1'].append(mel1)\n",
    "    M, N = features['stft1'][0].shape\n",
    "    if use_stft :\n",
    "        for i in range(len(features['stft1'])) :\n",
    "            features['stft1'][i] = features['stft1'][i].reshape(M,N,1)           \n",
    "    features['stft1'] = np.array(features['stft1'])\n",
    "\n",
    "    #    print(features)\n",
    "    # Impute missing data.\n",
    "    res1 = model1.predict([features['age'], features['sex'], features['hw'], features['preg'], features['loc'], \n",
    "#                            features['interval'],\n",
    "                           features['interval'],features['wav2'],\n",
    "                           features['mel1'], features['cqt1'], features['stft1']])\n",
    "    \n",
    "    res2 = model2.predict([features['age'], features['sex'], features['hw'], features['preg'], features['loc'], \n",
    "#                            features['interval'],\n",
    "                           features['interval'],features['wav2'],\n",
    "                           features['mel1'], features['cqt1'], features['stft1']])\n",
    "\n",
    "    # Get classifier probabilities.\n",
    "    if model['ord1'] :\n",
    "        idx1 = res1.argmax(axis=0)[0]\n",
    "        murmur_p = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "        murmur_probabilities = np.zeros((3,))\n",
    "        murmur_probabilities[0] = murmur_p[0]\n",
    "        murmur_probabilities[1] = 0\n",
    "        murmur_probabilities[2] = murmur_p[1]\n",
    "        outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "    else :\n",
    "        if model['mm_mean'] :\n",
    "            murmur_probabilities = res1.mean(axis = 0)\n",
    "        else :\n",
    "            idx1 = res1.argmax(axis=0)[0]\n",
    "            murmur_probabilities = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "        outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "\n",
    "    # Get classifier probabilities.\n",
    "#    idx1 = res1.argmax(axis=0)[0]\n",
    "#    murmur_probabilities = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "#    outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "#    idx = np.argmax(prob1)\n",
    "\n",
    "    ## 이부분도 생각 필요.. rule 을 cost를 maximize 하는 기준으로 threshold 탐색 필요할지도..\n",
    "    # Choose label with highest probability.\n",
    "    murmur_labels = np.zeros(len(murmur_classes), dtype=np.int_)\n",
    "#    idx = np.argmax(murmur_probabilities)\n",
    "    if murmur_probabilities[0] > 0.482 :\n",
    "        idx = 0\n",
    "    else :\n",
    "        idx = 2\n",
    "    murmur_labels[idx] = 1\n",
    "\n",
    "    outcome_labels = np.zeros(len(outcome_classes), dtype=np.int_)\n",
    "    if outcome_probabilities[0] > 0.607 :\n",
    "        idx = 0\n",
    "    else :\n",
    "        idx = 1    \n",
    "        # idx = np.argmax(outcome_probabilities)\n",
    "    outcome_labels[idx] = 1\n",
    "    \n",
    "    # Concatenate classes, labels, and probabilities.\n",
    "    classes = murmur_classes + outcome_classes\n",
    "    labels = np.concatenate((murmur_labels, outcome_labels))\n",
    "    probabilities = np.concatenate((murmur_probabilities, outcome_probabilities))\n",
    "    \n",
    "    return classes, labels, probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e0e6c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patient_files_trn = find_patient_files(train_folder)\n",
    "patient_files_test = find_patient_files(test_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f164cab",
   "metadata": {},
   "source": [
    "### 수정할 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b3414a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = 'lcnn_fix_rr_w2v2'\n",
    "output_folder = '/Data2/hmd/hmd_sy/notebooks/out_lcnn_fix_rr_w2v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58458fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_sec=16000 # 4초\n",
    "#-------------------------\n",
    "winlen = 512\n",
    "hoplen = 256\n",
    "nmel = 120 #np.random.choice([100, 120, 140])\n",
    "nsec = 20 #np.random.choice([10, 20, 30, 40, 50])\n",
    "trim = 0 #np.random.choice([0,2000, 4000])\n",
    "use_mel = True\n",
    "use_cqt = False #np.random.choice([True,False])\n",
    "use_stft = False#np.random.choice([True, False])\n",
    "use_raw=False\n",
    "use_interval=True\n",
    "use_wav2=True\n",
    "maxlen1 = 246000\n",
    "min_dist = 500\n",
    "max_interval_len = 192\n",
    "\n",
    "params_feature = {'samp_sec': nsec,\n",
    "              #### melspec, stft 피쳐 옵션들  \n",
    "              'pre_emphasis': 0,\n",
    "              'hop_length': hoplen,\n",
    "              'win_length':winlen,\n",
    "              'n_mels': nmel,\n",
    "              #### cqt 피쳐 옵션들  \n",
    "              'filter_scale': 1,\n",
    "              'n_bins': 80,\n",
    "              'fmin': 10,\n",
    "              ### RR interval 옵션\n",
    "                  'min_dist':min_dist,\n",
    "                  'max_interval_len' : max_interval_len,\n",
    "                  'per_sec' : per_sec,\n",
    "                  'use_interval' : use_interval,\n",
    "            ### wav2vec2 옵션\n",
    "                  'maxlen1': maxlen1,\n",
    "                  'use_wav2' : use_wav2,\n",
    "              ### 사용할 피쳐 지정\n",
    "                  'trim' : trim, # 앞뒤 얼마나 자를지? 4000 이면 1초\n",
    "                  'use_mel' : use_mel,\n",
    "                  'use_cqt' : use_cqt,\n",
    "                  'use_stft' : use_stft          \n",
    "}\n",
    "\n",
    "\n",
    "mm_weight = 3 #np.random.choice([2,3,4,5])\n",
    "oo_weight = 3 #np.random.choice([2,3,4,5,6])\n",
    "ord1 = True #np.random.choice([True,False])\n",
    "mm_mean = False #np.random.choice([True,False])\n",
    "dp = 0 #np.random.choice([0, .1, .2, .3])\n",
    "fc = False #np.random.choice([True,False])\n",
    "ext = False\n",
    "chaug = 10 #np.random.choice([0, 10])\n",
    "mixup = True #np.random.choice([True,False])\n",
    "cout = .8 #np.random.choice([0, 0.8])\n",
    "wunknown = 1 #np.random.choice([1, 0.7, .5, .2])\n",
    "n1 = 0 #np.random.choice([0,2])\n",
    "if n1 == 0 :\n",
    "    ranfil = False\n",
    "else :\n",
    "    ranfil = [n1, [18,19,20,21,22,23]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0edb56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_3lb_all_ord(data_folder, patient_files_trn, po = .3,\n",
    "                          samp_sec=20, pre_emphasis = 0, hop_length=256, win_length = 512, n_mels = 100,\n",
    "                             filter_scale = 1, n_bins = 80, fmin = 10, trim = 4000,\n",
    "                             use_mel = True, use_cqt = False, use_stft = False, use_raw = False,\n",
    "                             use_wav2=False, maxlen1=120000,\n",
    "                             use_interval=False,min_dist=500,per_sec=16000,max_interval_len=192\n",
    "                         ) :\n",
    "    features = dict()\n",
    "    features['id'] = []\n",
    "    features['age'] = []\n",
    "    features['sex'] = []\n",
    "    features['hw'] = []\n",
    "    features['preg'] = []\n",
    "    features['loc'] = []\n",
    "    features['mel1'] = []\n",
    "    features['cqt1'] = []\n",
    "    features['stft1'] = []\n",
    "    features['raw1'] = []\n",
    "    \n",
    "    features['interval'] = []\n",
    "    tmp_total_interval = [] \n",
    "    \n",
    "    features['wav2']=[]\n",
    "    tmp_wav=[]\n",
    "\n",
    "    \n",
    "    mm_labels = []\n",
    "    out_labels = []\n",
    "\n",
    "    age_classes = ['Neonate', 'Infant', 'Child', 'Adolescent', 'Young Adult']\n",
    "    recording_locations = ['AV', 'MV', 'PV', 'TV', 'PhC']\n",
    "\n",
    "    num_patient_files = len(patient_files_trn)\n",
    "\n",
    "    for i in tqdm.tqdm(range(num_patient_files)):\n",
    "\n",
    "        # Load the current patient data and recordings.\n",
    "        current_patient_data = load_patient_data(patient_files_trn[i])\n",
    "        num_locations = get_num_locations(current_patient_data)\n",
    "        recording_information = current_patient_data.split('\\n')[1:num_locations+1]\n",
    "        for j in range(num_locations) :\n",
    "            entries = recording_information[j].split(' ')\n",
    "            recording_file = entries[2]\n",
    "            filename = os.path.join(data_folder, recording_file)\n",
    "\n",
    "            # Extract id\n",
    "            id1 = recording_file.split('_')[0]\n",
    "            features['id'].append(id1)\n",
    "\n",
    "            # Extract melspec\n",
    "            if use_mel :\n",
    "                mel1 = feature_extract_melspec(filename, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length, \n",
    "                                               win_length = win_length, n_mels = n_mels, trim = trim)[0]\n",
    "            else :\n",
    "                mel1 = np.zeros( (1,1,1) )\n",
    "            features['mel1'].append(mel1)\n",
    "\n",
    "            if use_cqt :\n",
    "                mel2 = feature_extract_cqt(filename, samp_sec=samp_sec, pre_emphasis = pre_emphasis, filter_scale = filter_scale, \n",
    "                                           n_bins = n_bins, fmin = fmin, trim = trim)[0]\n",
    "            else :\n",
    "                mel2 = np.zeros( (1,1,1) )\n",
    "            features['cqt1'].append(mel2)\n",
    "\n",
    "            if use_stft :\n",
    "                mel3 = feature_extract_stft(filename, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length, \n",
    "                                            win_length = win_length, trim = trim)[0]\n",
    "            else :\n",
    "                mel3 = np.zeros( (1,1,1) )\n",
    "            features['stft1'].append(mel3)\n",
    "\n",
    "            if use_raw :\n",
    "                frequency1, recording1 = sp.io.wavfile.read(filename)\n",
    "            else :\n",
    "                recording1 = np.zeros((1))\n",
    "            features['raw1'].append(recording1)\n",
    "            \n",
    "            ## w2v2\n",
    "            if use_wav2:\n",
    "#                 frequency1, recording1 = sp.io.wavfile.read(filename)\n",
    "                recording1,frequency1 = librosa.load(filename)\n",
    "                \n",
    "            else :\n",
    "                recording1 = np.zeros( (1) )\n",
    "            tmp_wav.append(recording1) \n",
    "\n",
    "            ## RR interval\n",
    "            if use_interval :\n",
    "                \n",
    "                datos=sp.io.wavfile.read(filename)\n",
    "                filtros=sio.loadmat('./Filters1')\n",
    "                tmp_interval = []\n",
    "                n_samp = len(datos[1])//per_sec\n",
    "                               \n",
    "                \n",
    "                try:\n",
    "                    for k in range(n_samp):\n",
    "                        X = datos[1][k*per_sec:(k+1)*per_sec]\n",
    "                        Fs= datos[0]\n",
    "                        Fpa20=filtros['Fpa20'];\t\t\t        # High pass filter\n",
    "                        Fpa20=Fpa20[0];\t\t\t\t\t# High pass filter\n",
    "                        Fpb100=filtros['Fpb100'];\t\t        # Low-pass Filter\n",
    "                        Fpb100=Fpb100[0];\t\t\t\t# Low-pass Filter\n",
    "                        Xf=FpassBand(X,Fpa20,Fpb100); \t                # Apply a passband filter\n",
    "                        Xf=vec_nor(Xf);\t\t\t\n",
    "            \n",
    "            # Derivate of the Signal\n",
    "                        dX=derivate(Xf);\t\t\t\t# Derivate of the signal\n",
    "                        dX=vec_nor(dX);\t\t\t\t\t# Vector Normalizing\n",
    "            # Square of the signal\n",
    "                        dy=np.square(Xf);\n",
    "                        dy=vec_nor(dy);\n",
    "                    \n",
    "                        size=np.shape(Xf)\t\t\t\t# Rank or dimension of the array\n",
    "                        fil=size[0];\t\t\t\t\t# Number of rows\n",
    "\n",
    "                        positive=np.zeros((1,fil+1));                   # Initializating Positives Values Vector \n",
    "                        positive=positive[0];                           # Getting the Vector\n",
    "\n",
    "                        points=np.zeros((1,fil));                       # Initializating the all Peak Points Vector\n",
    "                        points=points[0];                               # Getting the point vector\n",
    "\n",
    "                        peaks=np.zeros((1,fil));                        # Initializating the s1-s1 Peak Vector\n",
    "                        peaks=peaks[0];                                 # Getting the point vector\n",
    "\n",
    "            \n",
    "                        for i in range(0,fil):\n",
    "                            if dX[i]>0:\n",
    "                                positive[i]=1;\n",
    "                            else:\n",
    "                                positive[i]=0;\n",
    "    \n",
    "                        for i in range(0,fil):\n",
    "                            if (positive[i]==1 and positive[i+1]==0):\n",
    "                                points[i]=Xf[i];\n",
    "                            else:\n",
    "                                points[i]=0;\n",
    "\n",
    "                        indexes=peakutils.indexes(points,thres=0.5/max(points), min_dist=min_dist);\n",
    "                        lenght=np.shape(indexes)\t\t\t# Get the length of the index vector\t\t\n",
    "                        lenght=lenght[0];\t\t\t\t# Get the value of the index vector\n",
    "\n",
    "                        for i in range(0,lenght):\n",
    "                            p=indexes[i];\n",
    "                            peaks[p]=points[p];\n",
    "        \n",
    "                        n=np.arange(0,fil);                            # Vector to the X axes (Number of Samples)\n",
    "                        indexes =indexes+(k*per_sec)    \n",
    "                        tmp_peaks = np.array(indexes)\n",
    "                    \n",
    "                    \n",
    "                        tmp_interval.extend(tmp_peaks)\n",
    "                    \n",
    "                    tmp_interval = np.array(tmp_interval)\n",
    "                    tmp_interval = np.diff(tmp_interval)\n",
    "                    \n",
    "                    tmp_total_interval.append(tmp_interval)\n",
    " \n",
    "           \n",
    "                except:\n",
    "                    print(filename)\n",
    "                    tmp_peaks = np.zeros(max_interval_len)\n",
    "                    tmp_total_interval.append(tmp_peaks)\n",
    "                    \n",
    "            else :\n",
    "                        \n",
    "                tmp_peaks = np.zeros(max_interval_len)\n",
    "                tmp_total_interval.append(tmp_peaks)               \n",
    "            \n",
    "            \n",
    "            \n",
    "            # Extract age_group\n",
    "            age_group = get_age(current_patient_data)\n",
    "            current_age_group = np.zeros(6, dtype=int)\n",
    "            if age_group in age_classes:\n",
    "                j = age_classes.index(age_group)\n",
    "                current_age_group[j] = 1\n",
    "            else :\n",
    "                current_age_group[5] = 1\n",
    "            features['age'].append(current_age_group)\n",
    "\n",
    "            # Extract sex\n",
    "            sex = get_sex(current_patient_data)\n",
    "            sex_features = np.zeros(2, dtype=int)\n",
    "            if compare_strings(sex, 'Female'):\n",
    "                sex_features[0] = 1\n",
    "            elif compare_strings(sex, 'Male'):\n",
    "                sex_features[1] = 1\n",
    "            features['sex'].append(sex_features)\n",
    "\n",
    "            # Extract height and weight.\n",
    "            height = get_height(current_patient_data)\n",
    "            weight = get_weight(current_patient_data)\n",
    "            ## simple impute\n",
    "            if math.isnan(height) :\n",
    "                height = 110.846\n",
    "            if math.isnan(weight) :\n",
    "                weight = 23.767\n",
    "                \n",
    "            features['hw'].append(np.array([height, weight]))\n",
    "\n",
    "            # Extract pregnancy\n",
    "            is_pregnant = get_pregnancy_status(current_patient_data)\n",
    "            features['preg'].append(is_pregnant)\n",
    "\n",
    "            # Extract location\n",
    "            locations = entries[0]\n",
    "            num_recording_locations = len(recording_locations)\n",
    "            loc_features = np.zeros(num_recording_locations)\n",
    "            if locations in recording_locations:\n",
    "                j = recording_locations.index(locations)\n",
    "                loc_features[j] = 1\n",
    "            features['loc'].append(loc_features)\n",
    "\n",
    "            # Extract labels \n",
    "            mm_label = get_murmur(current_patient_data)\n",
    "            out_label = get_outcome(current_patient_data)\n",
    "            if mm_label == 'Absent' :\n",
    "                current_mm_labels = np.array([0, 1])\n",
    "            elif mm_label == 'Unknown' :\n",
    "                current_mm_labels = np.array([po, 1-po])\n",
    "            else :\n",
    "                mm_loc = get_murmur_loc(current_patient_data)\n",
    "                if mm_loc == 'nan' :\n",
    "                    current_mm_labels = np.array([0.9, 0.1])\n",
    "                else :\n",
    "                    mm_loc = mm_loc.split('+')\n",
    "                    if locations in mm_loc :\n",
    "                        current_mm_labels = np.array([1, 0])\n",
    "                    else :\n",
    "                        current_mm_labels = np.array([0.8, 0.2])\n",
    "\n",
    "            if out_label == 'Normal' :\n",
    "                current_out_labels = np.array([0, 1])\n",
    "            else :\n",
    "                current_out_labels = np.array([1, 0])\n",
    "#                if mm_label == 'Absent' :\n",
    "#                    current_out_labels = np.array([0.8, 0.2])\n",
    "#                elif mm_label == 'unknown' :\n",
    "#                    current_out_labels = np.array([0.85, 0.15])\n",
    "#                else :\n",
    "#                    current_out_labels = np.array([1, 0])\n",
    "                \n",
    "            mm_labels.append(current_mm_labels)\n",
    "            out_labels.append(current_out_labels)\n",
    "\n",
    "    if use_mel : \n",
    "        M, N = features['mel1'][i].shape\n",
    "        for i in range(len(features['mel1'])) :\n",
    "            features['mel1'][i] = features['mel1'][i].reshape(M,N,1)\n",
    "        print(\"melspec: \", M,N)\n",
    "    else :\n",
    "        M, N, _ = features['mel1'][i].shape\n",
    "    mel_input_shape = (M,N,1)\n",
    "        \n",
    "    if use_cqt :\n",
    "        M, N = features['cqt1'][i].shape\n",
    "        for i in range(len(features['cqt1'])) :\n",
    "            features['cqt1'][i] = features['cqt1'][i].reshape(M,N,1)\n",
    "        print(\"cqt: \", M,N)\n",
    "    else :\n",
    "        M, N, _ = features['cqt1'][i].shape\n",
    "    cqt_input_shape = (M,N,1)\n",
    "\n",
    "    \n",
    "    if use_stft :\n",
    "        M, N = features['stft1'][i].shape\n",
    "        for i in range(len(features['stft1'])) :\n",
    "            features['stft1'][i] = features['stft1'][i].reshape(M,N,1)\n",
    "        print(\"stft: \", M,N)\n",
    "    else :\n",
    "        M, N, _ = features['stft1'][i].shape\n",
    "    stft_input_shape = (M,N,1)\n",
    "        \n",
    "    ## RR interval    \n",
    "    if use_interval:\n",
    "        padded =pad_sequences(tmp_total_interval, maxlen=max_interval_len, dtype='float32', padding='post', truncating='post', value=0.0)\n",
    "        \n",
    "        for i in range(len(padded)):\n",
    "            features['interval'].append(padded[i])\n",
    "        for i in range(len(features['interval'])):\n",
    "            features['interval'][i]= features['interval'][i].reshape(-1,1)\n",
    "        features['interval']= np.array(features['interval'])\n",
    "        \n",
    "    else:\n",
    "        for i in range(len(tmp_interval)):\n",
    "            features['interval'].append(tmp_total_interval[i])\n",
    "        for i in range(len(features['interval'])):\n",
    "            features['interval'][i]= features['interval'][i].reshape(-1,1)\n",
    "        features['interval']= np.array(features['interval'])\n",
    "\n",
    "    interval_input_shape = features['interval'].shape[1:]\n",
    "    M,N = interval_input_shape\n",
    "    print(\"interval: \", M,N)\n",
    "    \n",
    "    ## w2v2    \n",
    "    if use_wav2:\n",
    "        padded =pad_sequences(tmp_wav, maxlen=maxlen1, dtype='float64', padding='post', truncating='post', value=0.0)\n",
    "        padded=np.array(padded, dtype=np.float32)\n",
    "        features['wav2']=padded\n",
    "    \n",
    "    wav2_input_shape = features['wav2'].shape[1:]\n",
    "    M = wav2_input_shape\n",
    "    print(\"wav2: \", M)    \n",
    "    \n",
    "        \n",
    "    for k1 in features.keys() :\n",
    "        features[k1] = np.array(features[k1])\n",
    "    \n",
    "    mm_labels = np.array(mm_labels)\n",
    "    out_labels = np.array(out_labels)\n",
    "    \n",
    "#     return features, mm_labels, out_labels, mel_input_shape, cqt_input_shape, stft_input_shape,interval_input_shape\n",
    "\n",
    "    return features, mm_labels, out_labels, mel_input_shape, cqt_input_shape, stft_input_shape,interval_input_shape,wav2_input_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4069ad65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 751/751 [40:49<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melspec:  120 313\n",
      "interval:  192 1\n",
      "wav2:  (246000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 191/191 [08:09<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melspec:  120 313\n",
      "interval:  192 1\n",
      "wav2:  (246000,)\n"
     ]
    }
   ],
   "source": [
    "if ord1 :\n",
    "    features_trn, mm_lbs_trn, out_lbs_trn, mel_input_shape, cqt_input_shape, stft_input_shape,interval_input_shape, wav2_input_shape = get_features_3lb_all_ord(train_folder, patient_files_trn, **params_feature)\n",
    "    features_test, mm_lbs_test, out_lbs_test, _, _, _,interval_input_shape,wav2_input_shape = get_features_3lb_all_ord(test_folder, patient_files_test, **params_feature)\n",
    "else :\n",
    "    features_trn, mm_lbs_trn, out_lbs_trn, mel_input_shape, cqt_input_shape, stft_input_shape,interval_input_shape, wav2_input_shape = get_features_3lb_all(train_folder, patient_files_trn, **params_feature)\n",
    "    features_test, mm_lbs_test, out_lbs_test, _, _, _,interval_input_shape, wav2_input_shape = get_features_3lb_all(test_folder, patient_files_test, **params_feature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3d96000",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features_trn_fix_rr_w2v2.pkl','wb') as f:\n",
    "    pickle.dump(features_trn,f)\n",
    "    \n",
    "with open('mm_lbs_trn_fix_rr_w2v2.pkl','wb') as f:\n",
    "    pickle.dump(mm_lbs_trn,f)\n",
    "    \n",
    "with open('out_lbs_trn_fix_rr_w2v2.pkl','wb') as f:\n",
    "    pickle.dump(out_lbs_trn,f)\n",
    "    \n",
    "with open('mel_input_shape_fix_rr_w2v2.pkl','wb') as f:\n",
    "    pickle.dump(mel_input_shape,f)\n",
    "\n",
    "with open('cqt_input_shape_fix_rr_w2v2.pkl','wb') as f:\n",
    "    pickle.dump(cqt_input_shape,f)\n",
    "    \n",
    "with open('stft_input_shape_fix_rr_w2v2.pkl','wb') as f:\n",
    "    pickle.dump(stft_input_shape,f)\n",
    "    \n",
    "with open('features_test_fix_rr_w2v2.pkl','wb') as f:\n",
    "    pickle.dump(features_test,f)\n",
    "    \n",
    "with open('mm_lbs_test_fix_rr_w2v2.pkl','wb') as f:\n",
    "    pickle.dump(mm_lbs_test,f)\n",
    "\n",
    "with open('out_lbs_test_fix_rr_w2v2.pkl','wb') as f:\n",
    "    pickle.dump(out_lbs_test,f)\n",
    "    \n",
    "with open('interval_input_shape_fix_rr_w2v2.pkl','wb') as f:\n",
    "    pickle.dump(interval_input_shape,f)\n",
    "    \n",
    "with open('wav2_input_shape_fix_rr_w2v2.pkl','wb') as f:\n",
    "    pickle.dump(wav2_input_shape,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed6b92ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/Data2/hmd/hmd_sy/notebooks/features_trn_fix_rr_w2v2.pkl','rb') as f:\n",
    "    features_trn = pickle.load(f)\n",
    "    \n",
    "with open('/Data2/hmd/hmd_sy/notebooks/mm_lbs_trn_fix_rr_w2v2.pkl','rb') as f:\n",
    "    mm_lbs_trn = pickle.load(f)\n",
    "    \n",
    "with open('/Data2/hmd/hmd_sy/notebooks/out_lbs_trn_fix_rr_w2v2.pkl','rb') as f:\n",
    "    out_lbs_trn = pickle.load(f)\n",
    "    \n",
    "with open('/Data2/hmd/hmd_sy/notebooks/mel_input_shape_fix_rr_w2v2.pkl','rb') as f:\n",
    "    mel_input_shape = pickle.load(f)\n",
    "\n",
    "with open('/Data2/hmd/hmd_sy/notebooks/cqt_input_shape_fix_rr_w2v2.pkl','rb') as f:\n",
    "    cqt_input_shape = pickle.load(f)\n",
    "    \n",
    "with open('/Data2/hmd/hmd_sy/notebooks/stft_input_shape_fix_rr_w2v2.pkl','rb') as f:\n",
    "    stft_input_shape = pickle.load(f)\n",
    "    \n",
    "with open('/Data2/hmd/hmd_sy/notebooks/features_test_fix_rr_w2v2.pkl','rb') as f:\n",
    "    features_test = pickle.load(f)\n",
    "    \n",
    "with open('/Data2/hmd/hmd_sy/notebooks/mm_lbs_test_fix_rr_w2v2.pkl','rb') as f:\n",
    "    mm_lbs_test = pickle.load(f)\n",
    "\n",
    "with open('/Data2/hmd/hmd_sy/notebooks/out_lbs_test_fix_rr_w2v2.pkl','rb') as f:\n",
    "    out_lbs_test = pickle.load(f)\n",
    "    \n",
    "with open('/Data2/hmd/hmd_sy/notebooks/interval_input_shape_fix_rr_w2v2.pkl','rb') as f:\n",
    "    interval_input_shape = pickle.load(f)\n",
    "    \n",
    "with open('/Data2/hmd/hmd_sy/notebooks/wav2_input_shape_fix_rr_w2v2.pkl','rb') as f:\n",
    "    wav2_input_shape = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e724c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LCNN_o_4_dr_1(mel_input_shape, cqt_input_shape, stft_input_shape,\n",
    "                      interval_input_shape,wav2_input_shape,use_wav2 = True,\n",
    "                      use_mel = True, use_cqt = False, use_stft = False, ord1 = True, dp = .5, fc = False, ext = False):\n",
    "        # Create a towy model.\n",
    "    age = keras.Input(shape=(6,), name = 'age_cat')\n",
    "    sex = keras.Input(shape=(2,), name = 'sex_cat')\n",
    "    hw = keras.Input(shape=(2,), name = 'height_weight')\n",
    "    preg = keras.Input(shape=(1,), name = 'is_preg')\n",
    "    loc = keras.Input(shape=(5,), name = 'loc')\n",
    "    interval = keras.Input(shape=interval_input_shape, name = 'interval')\n",
    "    wav2 = keras.Input(shape=wav2_input_shape, name = 'wav2')\n",
    "    mel1 = keras.Input(shape=mel_input_shape, name = 'mel')\n",
    "    cqt1 = keras.Input(shape=cqt_input_shape, name = 'cqt')\n",
    "    stft1 = keras.Input(shape=stft_input_shape, name = 'stft')\n",
    "    \n",
    "       \n",
    "    ## age embeddig\n",
    "    age1 = layers.Dense(2, activation = None)(age)\n",
    "\n",
    "    ## sex embedding\n",
    "    sex1 = layers.Dense(1, activation = None)(sex)\n",
    "\n",
    "    ## hw embedding\n",
    "    hw1 = layers.Dense(1, activation = None)(hw)\n",
    "\n",
    "    ## loc embedding\n",
    "    loc1 = layers.Dense(3, activation = None)(loc)\n",
    "    \n",
    "    ## interval embedding\n",
    "    interval1_1 = tf.keras.layers.Conv1D(9, 3, padding=\"causal\",activation=None,dilation_rate=1)(interval)\n",
    "    interval1_2 = tf.keras.layers.Conv1D(9, 3, padding=\"causal\",activation=None,dilation_rate=1)(interval)\n",
    "    interval_mfm1 = tensorflow.keras.layers.maximum([interval1_1, interval1_2])\n",
    "    interval_maxpool_1 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm1)\n",
    "\n",
    "    interval2_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=2)(interval_maxpool_1)\n",
    "    interval2_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=2)(interval_maxpool_1)\n",
    "    interval_mfm2 = tensorflow.keras.layers.maximum([interval2_1, interval2_2])\n",
    "    interval_maxpool_2 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm2)\n",
    "    \n",
    "    \n",
    "    interval3_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=4)(interval_maxpool_2)\n",
    "    interval3_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=4)(interval_maxpool_2)\n",
    "    interval_mfm3 = tensorflow.keras.layers.maximum([interval3_1, interval3_2])\n",
    "    interval_maxpool_3 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm3)\n",
    "    \n",
    "    interval4_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=8)(interval_maxpool_3)\n",
    "    interval4_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=8)(interval_maxpool_3)\n",
    "    interval_mfm4 = tensorflow.keras.layers.maximum([interval4_1, interval4_2])\n",
    "    interval_maxpool_4 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm4)\n",
    "    \n",
    "    interval5_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=16)(interval_maxpool_4)\n",
    "    interval5_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=16)(interval_maxpool_4)\n",
    "    interval_mfm5 = tensorflow.keras.layers.maximum([interval5_1, interval5_1])\n",
    "    interval_maxpool_5 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm5)\n",
    "    \n",
    "    interval6_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=32)(interval_maxpool_5)\n",
    "    interval6_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=32)(interval_maxpool_5)\n",
    "    interval_mfm6 = tensorflow.keras.layers.maximum([interval6_1, interval6_2])\n",
    "    interval_maxpool_6 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm6)\n",
    "    \n",
    "    interval7_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=64)(interval_maxpool_6)\n",
    "    interval7_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=64)(interval_maxpool_6)\n",
    "    interval_mfm7 = tensorflow.keras.layers.maximum([interval7_1, interval7_2])\n",
    "    interval_maxpool_7 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm7)\n",
    "    \n",
    "    interval8_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=128)(interval_maxpool_7)\n",
    "    interval8_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=128)(interval_maxpool_7)\n",
    "    interval_mfm8 = tensorflow.keras.layers.maximum([interval8_1, interval8_2])\n",
    "    interval_maxpool_8 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm8)\n",
    "    \n",
    "    interval9_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=256)(interval_maxpool_8)\n",
    "    interval9_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=256)(interval_maxpool_8)\n",
    "    interval_mfm9 = tensorflow.keras.layers.maximum([interval9_1, interval9_2])\n",
    "    interval_maxpool_9 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm9)\n",
    "    \n",
    "    interval10_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=512)(interval_maxpool_9)\n",
    "    interval10_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=512)(interval_maxpool_9)\n",
    "    interval_mfm10 = tensorflow.keras.layers.maximum([interval10_1, interval10_2])\n",
    "    interval_maxpool_10 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm10)\n",
    "    \n",
    "    interval11_1 = tf.keras.layers.Conv1D(9, 3, padding=\"causal\",activation=None,dilation_rate=1)(interval_maxpool_10)\n",
    "    interval11_2 = tf.keras.layers.Conv1D(9, 3, padding=\"causal\",activation=None,dilation_rate=1)(interval_maxpool_10)\n",
    "    interval_mfm11 = tensorflow.keras.layers.maximum([interval11_1, interval11_2])\n",
    "    interval_maxpool_11 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm11)\n",
    "    \n",
    "    interval12_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=2)(interval_maxpool_11)\n",
    "    interval12_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=2)(interval_maxpool_11)\n",
    "    interval_mfm12 = tensorflow.keras.layers.maximum([interval12_1, interval12_2])\n",
    "    interval_maxpool_12 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm12)\n",
    "    \n",
    "    \n",
    "    interval13_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=4)(interval_maxpool_12)\n",
    "    interval13_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=4)(interval_maxpool_12)\n",
    "    interval_mfm13 = tensorflow.keras.layers.maximum([interval13_1, interval13_2])\n",
    "    interval_maxpool_13 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm13)\n",
    "    \n",
    "    interval14_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=8)(interval_maxpool_13)\n",
    "    interval14_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=8)(interval_maxpool_13)\n",
    "    interval_mfm14 = tensorflow.keras.layers.maximum([interval14_1, interval14_2])\n",
    "    interval_maxpool_14 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm14)\n",
    "    \n",
    "    interval15_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=16)(interval_maxpool_14)\n",
    "    interval15_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=16)(interval_maxpool_14)\n",
    "    interval_mfm15 = tensorflow.keras.layers.maximum([interval15_1, interval15_2])\n",
    "    interval_maxpool_15 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm15)\n",
    "    \n",
    "    interval16_1 = tf.keras.layers.Conv1D(64, 2, padding=\"causal\",activation=None,dilation_rate=32)(interval_maxpool_15)\n",
    "    interval16_2 = tf.keras.layers.Conv1D(64, 2, padding=\"causal\",activation=None,dilation_rate=32)(interval_maxpool_15)\n",
    "    interval_mfm16 = tensorflow.keras.layers.maximum([interval16_1, interval16_2])\n",
    "    interval_maxpool_16 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm16)\n",
    "    \n",
    "    interval17_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=64)(interval_maxpool_16)\n",
    "    interval17_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=64)(interval_maxpool_16)\n",
    "    interval_mfm17 = tensorflow.keras.layers.maximum([interval17_1, interval17_2])\n",
    "    interval_maxpool_17 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm17)\n",
    "    \n",
    "    interval18_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=128)(interval_maxpool_17)\n",
    "    interval18_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=128)(interval_maxpool_17)\n",
    "    interval_mfm18 = tensorflow.keras.layers.maximum([interval18_1, interval18_2])\n",
    "    interval_maxpool_18 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm18)\n",
    "    \n",
    "    interval19_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=256)(interval_maxpool_18)\n",
    "    interval19_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=256)(interval_maxpool_18)\n",
    "    interval_mfm19 = tensorflow.keras.layers.maximum([interval19_1, interval19_2])\n",
    "    interval_maxpool_19 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm19)\n",
    "    \n",
    "    interval20_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=512)(interval_maxpool_19)\n",
    "    interval20_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=512)(interval_maxpool_19)\n",
    "    interval_mfm20 = tensorflow.keras.layers.maximum([interval20_1, interval20_2])\n",
    "    interval_maxpool_20 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(interval_mfm20)    \n",
    "    \n",
    "    interval1 = layers.GlobalAveragePooling1D()(interval_maxpool_20)\n",
    "    interval1 = Dropout(dp)(interval1)\n",
    "    \n",
    "    ## wav2 embedding\n",
    "    \n",
    "    if use_wav2 :\n",
    "        \n",
    "        wav2vec2_layer = wav2vec2_model(wav2)\n",
    "        wav2vec2_layer = tf.keras.layers.Reshape((10,768,1))(wav2vec2_layer)\n",
    "        \n",
    "        \n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(wav2vec2_layer)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(wav2vec2_layer)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        wav2_1 = layers.GlobalAveragePooling2D()(max28)\n",
    "        wav2_1 = Dropout(dp)(wav2_1)\n",
    "        \n",
    "    \n",
    "#     if use_wav2:\n",
    "#         wav2_reshape = tf.keras.layers.Reshape((300,-1))(wav2)\n",
    "#         wav2_1_1 = tf.keras.layers.Conv1D(9, 3, padding=\"causal\",activation=None,dilation_rate=1)(wav2_reshape)\n",
    "#         wav2_1_2 = tf.keras.layers.Conv1D(9, 3, padding=\"causal\",activation=None,dilation_rate=1)(wav2_reshape)\n",
    "#         wav2_mfm1 = tensorflow.keras.layers.maximum([wav2_1_1, wav2_1_2])\n",
    "#         wav2_maxpool_1 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm1)\n",
    "\n",
    "#         wav2_2_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=2)(wav2_maxpool_1)\n",
    "#         wav2_2_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=2)(wav2_maxpool_1)\n",
    "#         wav2_mfm2 = tensorflow.keras.layers.maximum([wav2_2_1, wav2_2_2])\n",
    "#         wav2_maxpool_2 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm2)\n",
    "\n",
    "#         wav2_3_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=4)(wav2_maxpool_2)\n",
    "#         wav2_3_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=4)(wav2_maxpool_2)\n",
    "#         wav2_mfm3 = tensorflow.keras.layers.maximum([wav2_3_1, wav2_3_2])\n",
    "#         wav2_maxpool_3 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm3)\n",
    "\n",
    "#         wav2_4_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=8)(wav2_maxpool_3)\n",
    "#         wav2_4_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=8)(wav2_maxpool_3)\n",
    "#         wav2_mfm4 = tensorflow.keras.layers.maximum([wav2_4_1, wav2_4_2])\n",
    "#         wav2_maxpool_4 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm4)\n",
    "\n",
    "#         wav2_5_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=16)(wav2_maxpool_4)\n",
    "#         wav2_5_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=16)(wav2_maxpool_4)\n",
    "#         wav2_mfm5 = tensorflow.keras.layers.maximum([wav2_5_1, wav2_5_2])\n",
    "#         wav2_maxpool_5 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm5)\n",
    "\n",
    "#         wav2_6_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=32)(wav2_maxpool_5)\n",
    "#         wav2_6_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=32)(wav2_maxpool_5)\n",
    "#         wav2_mfm6 = tensorflow.keras.layers.maximum([wav2_6_1, wav2_6_2])\n",
    "#         wav2_maxpool_6 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm6)\n",
    "\n",
    "#         wav2_7_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=64)(wav2_maxpool_6)\n",
    "#         wav2_7_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=64)(wav2_maxpool_6)\n",
    "#         wav2_mfm7 = tensorflow.keras.layers.maximum([wav2_7_1, wav2_7_2])\n",
    "#         wav2_maxpool_7 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm7)\n",
    "\n",
    "#         wav2_8_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=128)(wav2_maxpool_7)\n",
    "#         wav2_8_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=128)(wav2_maxpool_7)\n",
    "#         wav2_mfm8 = tensorflow.keras.layers.maximum([wav2_8_1, wav2_8_2])\n",
    "#         wav2_maxpool_8 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm8)\n",
    "\n",
    "#         wav2_9_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=256)(wav2_maxpool_8)\n",
    "#         wav2_9_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=256)(wav2_maxpool_8)\n",
    "#         wav2_mfm9 = tensorflow.keras.layers.maximum([wav2_9_1, wav2_9_2])\n",
    "#         wav2_maxpool_9 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm9)\n",
    "\n",
    "#         wav2_10_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=512)(wav2_maxpool_9)\n",
    "#         wav2_10_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=512)(wav2_maxpool_9)\n",
    "#         wav2_mfm10 = tensorflow.keras.layers.maximum([wav2_10_1, wav2_10_2])\n",
    "#         wav2_maxpool_10 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm10)\n",
    "\n",
    "#         wav2_11_1 = tf.keras.layers.Conv1D(9, 3, padding=\"causal\",activation=None,dilation_rate=1)(wav2_maxpool_10)\n",
    "#         wav2_11_2 = tf.keras.layers.Conv1D(9, 3, padding=\"causal\",activation=None,dilation_rate=1)(wav2_maxpool_10)\n",
    "#         wav2_mfm11 = tensorflow.keras.layers.maximum([wav2_11_1, wav2_11_2])\n",
    "#         wav2_maxpool_11 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm11)\n",
    "\n",
    "#         wav2_12_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=2)(wav2_maxpool_11)\n",
    "#         wav2_12_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=2)(wav2_maxpool_11)\n",
    "#         wav2_mfm12 = tensorflow.keras.layers.maximum([wav2_12_1, wav2_12_2])\n",
    "#         wav2_maxpool_12 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm12)\n",
    "\n",
    "#         wav2_13_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=4)(wav2_maxpool_12)\n",
    "#         wav2_13_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=4)(wav2_maxpool_12)\n",
    "#         wav2_mfm13 = tensorflow.keras.layers.maximum([wav2_13_1, wav2_13_2])\n",
    "#         wav2_maxpool_13 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm13)\n",
    "\n",
    "#         wav2_14_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=8)(wav2_maxpool_13)\n",
    "#         wav2_14_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=8)(wav2_maxpool_13)\n",
    "#         wav2_mfm14 = tensorflow.keras.layers.maximum([wav2_14_1, wav2_14_2])\n",
    "#         wav2_maxpool_14 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm14)\n",
    "\n",
    "#         wav2_15_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=16)(wav2_maxpool_14)\n",
    "#         wav2_15_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=16)(wav2_maxpool_14)\n",
    "#         wav2_mfm15 = tensorflow.keras.layers.maximum([wav2_15_1, wav2_15_2])\n",
    "#         wav2_maxpool_15 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm15)\n",
    "\n",
    "#         wav2_16_1 = tf.keras.layers.Conv1D(64, 2, padding=\"causal\",activation=None,dilation_rate=32)(wav2_maxpool_15)\n",
    "#         wav2_16_2 = tf.keras.layers.Conv1D(64, 2, padding=\"causal\",activation=None,dilation_rate=32)(wav2_maxpool_15)\n",
    "#         wav2_mfm16 = tensorflow.keras.layers.maximum([wav2_16_1, wav2_16_2])\n",
    "#         wav2_maxpool_16 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm16)\n",
    "\n",
    "#         wav2_17_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=64)(wav2_maxpool_16)\n",
    "#         wav2_17_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=64)(wav2_maxpool_16)\n",
    "#         wav2_mfm17 = tensorflow.keras.layers.maximum([wav2_17_1, wav2_17_2])\n",
    "#         wav2_maxpool_17 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm17)\n",
    "\n",
    "#         wav2_18_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=128)(wav2_maxpool_17)\n",
    "#         wav2_18_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=128)(wav2_maxpool_17)\n",
    "#         wav2_mfm18 = tensorflow.keras.layers.maximum([wav2_18_1, wav2_18_2])\n",
    "#         wav2_maxpool_18 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm18)\n",
    "\n",
    "#         wav2_19_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=256)(wav2_maxpool_18)\n",
    "#         wav2_19_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=256)(wav2_maxpool_18)\n",
    "#         wav2_mfm19 = tensorflow.keras.layers.maximum([wav2_19_1, wav2_19_2])\n",
    "#         wav2_maxpool_19 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm19)\n",
    "\n",
    "#         wav2_20_1 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=512)(wav2_maxpool_19)\n",
    "#         wav2_20_2 = tf.keras.layers.Conv1D(9, 2, padding=\"causal\",activation=None,dilation_rate=512)(wav2_maxpool_19)\n",
    "#         wav2_mfm20 = tensorflow.keras.layers.maximum([wav2_20_1, wav2_20_2])\n",
    "#         wav2_maxpool_20 = tf.keras.layers.MaxPooling1D(pool_size=2,strides=1, padding='valid')(wav2_mfm20)    \n",
    "\n",
    "#         wav2_1 = layers.GlobalAveragePooling1D()(wav2_maxpool_20)\n",
    "#         wav2_1 = Dropout(dp)(wav2_1)\n",
    "    \n",
    "    \n",
    "    ## mel embedding\n",
    "    if use_mel :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        mel2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            mel2 = Dropout(dp)(mel2)\n",
    "\n",
    "    if use_cqt :\n",
    "        ## cqt embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "        \n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        cqt2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            cqt2 = Dropout(dp)(cqt2)\n",
    "\n",
    "    if use_stft :\n",
    "        ## stft embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        \n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "        \n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        stft2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            stft2 = Dropout(dp)(stft2)\n",
    "    \n",
    "    \n",
    "    if use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2, stft2])\n",
    "    if not use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([cqt2, stft2])\n",
    "    if use_mel and not use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, stft2])\n",
    "    if use_mel and use_cqt and not use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2])\n",
    "    if not use_mel and not use_cqt and use_stft :  ## stft 만\n",
    "        concat2 = stft2\n",
    "    if use_mel and not use_cqt and not use_stft :  ### mel만\n",
    "        concat2 = mel2\n",
    "    if not use_mel and use_cqt and not use_stft :  ### cqt만\n",
    "        concat2 = cqt2\n",
    "\n",
    "    if ext :\n",
    "        concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg])\n",
    "        d1 = layers.Dense(2, activation = 'relu')(concat1)\n",
    "#         concat2 = layers.Concatenate()([concat2, d1, interval1])\n",
    "        concat2 = layers.Concatenate()([concat2, d1, interval1, wav2_1])\n",
    "        \n",
    "    if fc :\n",
    "        concat2 = layers.Dense(10, activation = \"relu\")(concat2)\n",
    "        concat2 = Dropout(dp)(concat2)\n",
    "        \n",
    "        \n",
    "    if ord1 :\n",
    "        res1 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "    else :\n",
    "        res1 = layers.Dense(3, activation = \"softmax\")(concat2)\n",
    "\n",
    "        \n",
    "#     res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "\n",
    "#     model = keras.Model(inputs = [age,sex,hw,preg,loc,interval, mel1, cqt1,stft1] , outputs = res1 )\n",
    "    model = keras.Model(inputs = [age,sex,hw,preg,loc,interval, wav2, mel1,cqt1,stft1] , outputs = res1 )\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy','AUC'])\n",
    "    return(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e48388",
   "metadata": {},
   "source": [
    "## Generator0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "316f436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Conv2D, maximum, add, SeparableConv2D\n",
    "from sklearn.metrics import roc_curve\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import soundfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "\n",
    "\n",
    "class Generator0():\n",
    "    def __init__(self, X_train, y_train, batch_size=32, beta_param=0.2, mixup = True, lowpass = False, highpass = False, ranfilter2 = False, shuffle=True, datagen=None, chaug = False, cout = False):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = beta_param\n",
    "        self.mixup = mixup\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_num = len(y_train)\n",
    "        self.datagen = datagen\n",
    "\n",
    "        ## ffm \n",
    "        \n",
    "        self.lowpass = lowpass\n",
    "        self.highpass = highpass\n",
    "        self.ranfilter = ranfilter2\n",
    "        self.chaug = chaug\n",
    "        self.cutout = cout        \n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            indexes = self.__get_exploration_order()\n",
    "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
    "\n",
    "            for i in range(itr_num):\n",
    "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
    "                X, y = self.__data_generation(batch_ids)\n",
    "\n",
    "                yield X, y\n",
    "\n",
    "    def __get_exploration_order(self):\n",
    "        indexes = np.arange(self.sample_num)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def __data_generation(self, batch_ids):\n",
    "        \n",
    "        \n",
    "        def get_box(lambda_value, nf, nt):\n",
    "            cut_rat = np.sqrt(1.0 - lambda_value)\n",
    "\n",
    "            cut_w = int(nf * cut_rat)  # rw\n",
    "            cut_h = int(nt * cut_rat)  # rh\n",
    "\n",
    "            cut_x = int(np.random.uniform(low=0, high=nf))  # rx\n",
    "            cut_y = int(np.random.uniform(low=0, high=nt))  # ry\n",
    "\n",
    "            boundaryx1 = np.minimum(np.maximum(cut_x - cut_w // 2, 0), nf) #tf.clip_by_value(cut_x - cut_w // 2, 0, IMG_SIZE_x)\n",
    "            boundaryy1 = np.minimum(np.maximum(cut_y - cut_h // 2, 0), nt) #tf.clip_by_value(cut_y - cut_h // 2, 0, IMG_SIZE_y)\n",
    "            bbx2 = np.minimum(np.maximum(cut_x + cut_w // 2, 0), nf) #tf.clip_by_value(cut_x + cut_w // 2, 0, IMG_SIZE_x)\n",
    "            bby2 = np.minimum(np.maximum(cut_y + cut_h // 2, 0), nt) #tf.clip_by_value(cut_y + cut_h // 2, 0, IMG_SIZE_y)\n",
    "\n",
    "            target_h = bby2 - boundaryy1\n",
    "            if target_h == 0:\n",
    "                target_h += 1\n",
    "\n",
    "            target_w = bbx2 - boundaryx1\n",
    "            if target_w == 0:\n",
    "                target_w += 1\n",
    "\n",
    "            return boundaryx1, boundaryy1, target_h, target_w           \n",
    "        \n",
    "        \n",
    "        if isinstance(self.X_train, list):\n",
    "            X = []\n",
    "            for X_temp in self.X_train:\n",
    "                if len(X_temp.shape) == 4: \n",
    "                    _, h, w, c = X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                elif len(X_temp.shape) == 3:\n",
    "                    _, h, w = X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size, 1, 1)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                elif len(X_temp.shape) == 2:\n",
    "                    _, h = X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size, 1)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                elif len(X_temp.shape) == 1:\n",
    "                    _= X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size,)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                \n",
    "                X1 = X_temp[batch_ids[:self.batch_size]].copy()\n",
    "                X2 = X_temp[batch_ids[self.batch_size:]].copy()\n",
    "                \n",
    "                if self.mixup :\n",
    "                    Xn = X1 * X_l + X2 * (1 - X_l)\n",
    "                else :\n",
    "                    Xn = X1\n",
    "                if len(X_temp.shape) == 4: \n",
    "                    _, h, w, c = X_temp.shape\n",
    "                    if h != 1 :\n",
    "                        if self.lowpass :\n",
    "                            uv, lp = self.lowpass\n",
    "                            dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                            for i in range(self.batch_size) :\n",
    "                                loc1 = np.random.choice(lp, size = 1)[0]\n",
    "                                Xn[i,:loc1,:,:] = 0\n",
    "                        if self.highpass :\n",
    "                            uv, hp = self.highpass\n",
    "                            dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                            for i in range(self.batch_size) :\n",
    "                                loc1 = np.random.choice(hp, size = 1)[0]\n",
    "                                Xn[i,loc1:,:,:] = 0\n",
    "                        if self.ranfilter :                \n",
    "                            raniter, ranf = self.ranfilter\n",
    "                            dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "                            for i in range(self.batch_size) :\n",
    "                                if dec1[i] > 0 :\n",
    "                                    for j in range(dec1[i]) :\n",
    "                                        b1 = np.random.choice(ranf, size = 1)[0]\n",
    "                                        loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "                                        Xn[i, loc1:(loc1 + b1 - 1), :] = 0\n",
    "                        if self.chaug :\n",
    "                            for i in range(self.batch_size) :\n",
    "                                noiselv = np.random.uniform(low= - self.chaug, high= self.chaug)\n",
    "                                Xn[i,:] += noiselv\n",
    "                        if self.cutout :\n",
    "                            lambda1 = np.random.beta(self.cutout, self.cutout, size = self.batch_size)   ## beta_param default : 0.7  STC페이퍼 추천은 0.6~0.8\n",
    "                            for i in range(self.batch_size) :\n",
    "                                boundaryx1, boundaryy1, target_h, target_w = get_box(lambda1[i], h, w)\n",
    "                                Xn[i, boundaryx1:(boundaryx1+target_h), boundaryy1:(boundaryy1+target_w),: ] = 0\n",
    "                \n",
    "#                 if len(X_temp.shape) == 3: \n",
    "                    \n",
    "#                     _, h, w = X_temp.shape\n",
    "                    \n",
    "#                     if h != 1 :\n",
    "                        \n",
    "#                         if self.lowpass :\n",
    "#                             uv, lp = self.lowpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(lp, size = 1)[0]\n",
    "#                                 Xn[i,:loc1,:] = 0\n",
    "#                         if self.highpass :\n",
    "#                             uv, hp = self.highpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(hp, size = 1)[0]\n",
    "#                                 Xn[i,loc1:,:] = 0\n",
    "#                         if self.ranfilter :                \n",
    "#                             raniter, ranf = self.ranfilter\n",
    "#                             dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 if dec1[i] > 0 :\n",
    "#                                     for j in range(dec1[i]) :\n",
    "#                                         b1 = np.random.choice(ranf, size = 1)[0]\n",
    "#                                         loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "#                                         Xn[i, loc1:(loc1 + b1 - 1), :] = 0                    \n",
    "                X.append(Xn)\n",
    "        else:\n",
    "            if len(self.X_train.shape) == 4: \n",
    "                _, h, w, c = self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "            elif len(self.X_train.shape) == 3:\n",
    "                _, h, w = self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size, 1, 1)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "            elif len(self.X_train.shape) == 2:\n",
    "                _, h = self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size, 1)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "            elif len(self.X_train.shape) == 1:\n",
    "                _= self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size,)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "\n",
    "            X1 = self.X_train[batch_ids[:self.batch_size]].copy()\n",
    "            X2 = self.X_train[batch_ids[self.batch_size:]].copy()\n",
    "            if self.mixup :\n",
    "                Xn = X1 * X_l + X2 * (1 - X_l)\n",
    "            else :\n",
    "                Xn = X1\n",
    "\n",
    "            if len(self.X_train.shape) == 4: \n",
    "                _, h, w, c = X_temp.shape\n",
    "                if self.lowpass :\n",
    "                    uv, lp = self.lowpass\n",
    "                    dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                    for i in range(self.batch_size) :\n",
    "                        loc1 = np.random.choice(lp, size = 1)[0]\n",
    "                        Xn[i,:loc1,:,:] = 0\n",
    "                if self.highpass :\n",
    "                    uv, hp = self.highpass\n",
    "                    dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                    for i in range(self.batch_size) :\n",
    "                        loc1 = np.random.choice(hp, size = 1)[0]\n",
    "                        Xn[i,loc1:,:,:] = 0\n",
    "                if self.ranfilter :                \n",
    "                    raniter, ranf = self.ranfilter\n",
    "                    dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "                    for i in range(self.batch_size) :\n",
    "                        if dec1[i] > 0 :\n",
    "                            for j in range(dec1[i]) :\n",
    "                                b1 = np.random.choice(ranf, size = 1)[0]\n",
    "                                loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "                                Xn[i, loc1:(loc1 + b1 - 1), :] = 0\n",
    "                \n",
    "#                 if len(self.X_train.shape) == 3:\n",
    "#                     _, h, w = X_temp.shape\n",
    "                    \n",
    "#                     if h != 1 :\n",
    "#                         if self.lowpass :\n",
    "#                             uv, lp = self.lowpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(lp, size = 1)[0]\n",
    "#                                 Xn[i,:loc1,:] = 0\n",
    "#                         if self.highpass :\n",
    "#                             uv, hp = self.highpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(hp, size = 1)[0]\n",
    "#                                 Xn[i,loc1:,:] = 0\n",
    "#                         if self.ranfilter :                \n",
    "#                             raniter, ranf = self.ranfilter\n",
    "#                             dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 if dec1[i] > 0 :\n",
    "#                                     for j in range(dec1[i]) :\n",
    "#                                         b1 = np.random.choice(ranf, size = 1)[0]\n",
    "#                                         loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "#                                         Xn[i, loc1:(loc1 + b1 - 1), :] = 0\n",
    "                \n",
    "            X.append(Xn)\n",
    "\n",
    "                \n",
    "        if self.datagen:\n",
    "            for i in range(self.batch_size):\n",
    "                X[i] = self.datagen.random_transform(X[i])\n",
    "                X[i] = self.datagen.standardize(X[i])\n",
    "\n",
    "        if isinstance(self.y_train, list):\n",
    "            y = []\n",
    "\n",
    "            for y_train_ in self.y_train:\n",
    "                y1 = y_train_[batch_ids[:self.batch_size]].copy()\n",
    "                y2 = y_train_[batch_ids[self.batch_size:]].copy()\n",
    "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
    "        else:\n",
    "            y1 = self.y_train[batch_ids[:self.batch_size]].copy()\n",
    "            y2 = self.y_train[batch_ids[self.batch_size:]].copy()\n",
    "            y = y1 * y_l + y2 * (1 - y_l)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cceb654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "500e8b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = get_LCNN_o_4_dr_1(mel_input_shape, cqt_input_shape, stft_input_shape, interval_input_shape,wav2_input_shape,\n",
    "                           use_mel = use_mel, use_cqt = use_cqt, use_stft = use_stft, ord1 = ord1, dp = dp, fc = fc, ext = ext)\n",
    "model2 = get_LCNN_o_4_dr_1(mel_input_shape, cqt_input_shape, stft_input_shape, interval_input_shape,wav2_input_shape,\n",
    "                           use_mel = use_mel, use_cqt = use_cqt, use_stft = use_stft, ord1 = ord1, dp = dp, fc = fc, ext = ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea4b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-12 02:09:51.686655: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-12 02:09:53.890874: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 16s 302ms/step - loss: 0.9119 - accuracy: 0.7504 - auc: 0.8197 - val_loss: 10.4440 - val_accuracy: 0.2108 - val_auc: 0.2085\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 12s 307ms/step - loss: 0.8606 - accuracy: 0.8078 - auc: 0.8563 - val_loss: 4.9760 - val_accuracy: 0.2108 - val_auc: 0.1893\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 0.8604 - accuracy: 0.8191 - auc: 0.8582 - val_loss: 0.6684 - val_accuracy: 0.5547 - val_auc: 0.6194\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 13s 328ms/step - loss: 0.8277 - accuracy: 0.8172 - auc: 0.8724 - val_loss: 2.0089 - val_accuracy: 0.2203 - val_auc: 0.1684\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 12s 315ms/step - loss: 0.8599 - accuracy: 0.8105 - auc: 0.8683 - val_loss: 0.6967 - val_accuracy: 0.4437 - val_auc: 0.4702\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 0.8401 - accuracy: 0.8078 - auc: 0.8728 - val_loss: 0.7027 - val_accuracy: 0.4992 - val_auc: 0.5004\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 12s 305ms/step - loss: 0.8350 - accuracy: 0.8160 - auc: 0.8664 - val_loss: 0.7259 - val_accuracy: 0.4897 - val_auc: 0.5119\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 12s 314ms/step - loss: 0.8209 - accuracy: 0.8035 - auc: 0.8799 - val_loss: 0.7243 - val_accuracy: 0.3994 - val_auc: 0.4300\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 13s 319ms/step - loss: 0.8239 - accuracy: 0.8230 - auc: 0.8712 - val_loss: 0.7447 - val_accuracy: 0.4010 - val_auc: 0.4286\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 13s 340ms/step - loss: 0.8344 - accuracy: 0.8098 - auc: 0.8738 - val_loss: 0.4706 - val_accuracy: 0.8558 - val_auc: 0.8871\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.8342 - accuracy: 0.8160 - auc: 0.8709 - val_loss: 0.5883 - val_accuracy: 0.7195 - val_auc: 0.7874\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 12s 307ms/step - loss: 0.8329 - accuracy: 0.8031 - auc: 0.8630 - val_loss: 1.2818 - val_accuracy: 0.2171 - val_auc: 0.3210\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 0.8392 - accuracy: 0.8082 - auc: 0.8773 - val_loss: 0.7832 - val_accuracy: 0.3582 - val_auc: 0.3678\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 12s 315ms/step - loss: 0.8132 - accuracy: 0.8211 - auc: 0.8827 - val_loss: 0.9563 - val_accuracy: 0.2203 - val_auc: 0.2643\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 0.7951 - accuracy: 0.8316 - auc: 0.8801 - val_loss: 0.4566 - val_accuracy: 0.8082 - val_auc: 0.8769\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 13s 328ms/step - loss: 0.8163 - accuracy: 0.8102 - auc: 0.8778 - val_loss: 0.4402 - val_accuracy: 0.8225 - val_auc: 0.8880\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 12s 314ms/step - loss: 0.8237 - accuracy: 0.8113 - auc: 0.8800 - val_loss: 0.6166 - val_accuracy: 0.6418 - val_auc: 0.7289\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 14s 359ms/step - loss: 0.7858 - accuracy: 0.8277 - auc: 0.8827 - val_loss: 0.3915 - val_accuracy: 0.8938 - val_auc: 0.9255\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 0.8150 - accuracy: 0.8203 - auc: 0.8812 - val_loss: 0.5128 - val_accuracy: 0.8320 - val_auc: 0.8709\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.8210 - accuracy: 0.8215 - auc: 0.8853 - val_loss: 0.3997 - val_accuracy: 0.8780 - val_auc: 0.9236\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 0.8228 - accuracy: 0.8102 - auc: 0.8844 - val_loss: 0.7842 - val_accuracy: 0.4580 - val_auc: 0.4838\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 13s 342ms/step - loss: 0.8112 - accuracy: 0.8012 - auc: 0.8776 - val_loss: 0.6107 - val_accuracy: 0.6593 - val_auc: 0.7211\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 12s 313ms/step - loss: 0.8043 - accuracy: 0.8141 - auc: 0.8763 - val_loss: 0.4746 - val_accuracy: 0.8637 - val_auc: 0.9230\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 0.8018 - accuracy: 0.8180 - auc: 0.8772 - val_loss: 0.4775 - val_accuracy: 0.8368 - val_auc: 0.9154\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.7998 - accuracy: 0.8215 - auc: 0.8867 - val_loss: 0.6118 - val_accuracy: 0.6656 - val_auc: 0.7262\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 0.8316 - accuracy: 0.8047 - auc: 0.8838 - val_loss: 0.4525 - val_accuracy: 0.8796 - val_auc: 0.9471\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 0.7951 - accuracy: 0.8172 - auc: 0.8862 - val_loss: 0.4160 - val_accuracy: 0.8796 - val_auc: 0.9363\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.8006 - accuracy: 0.8293 - auc: 0.8852 - val_loss: 0.4530 - val_accuracy: 0.8700 - val_auc: 0.9295\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 13s 328ms/step - loss: 0.8046 - accuracy: 0.8242 - auc: 0.8827 - val_loss: 0.3844 - val_accuracy: 0.8748 - val_auc: 0.9117\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 12s 315ms/step - loss: 0.7839 - accuracy: 0.8270 - auc: 0.8953 - val_loss: 0.3845 - val_accuracy: 0.8954 - val_auc: 0.9256\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 13s 326ms/step - loss: 0.7909 - accuracy: 0.8176 - auc: 0.8824 - val_loss: 0.3793 - val_accuracy: 0.8875 - val_auc: 0.9397\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 13s 343ms/step - loss: 0.8135 - accuracy: 0.8148 - auc: 0.8759 - val_loss: 0.3723 - val_accuracy: 0.8986 - val_auc: 0.9327\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 12s 294ms/step - loss: 0.8175 - accuracy: 0.8031 - auc: 0.8753 - val_loss: 0.3794 - val_accuracy: 0.8922 - val_auc: 0.9346\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 12s 308ms/step - loss: 0.7703 - accuracy: 0.8160 - auc: 0.8954 - val_loss: 0.3469 - val_accuracy: 0.8859 - val_auc: 0.9340\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 12s 318ms/step - loss: 0.8077 - accuracy: 0.8055 - auc: 0.8899 - val_loss: 0.4224 - val_accuracy: 0.8193 - val_auc: 0.9212\n",
      "Epoch 36/100\n",
      "29/40 [====================>.........] - ETA: 3s - loss: 0.8094 - accuracy: 0.8211 - auc: 0.8756"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_epoch = 100\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "batch_size = 64\n",
    "\n",
    "if mixup :\n",
    "    beta_param = .7\n",
    "else :\n",
    "    beta_param = 0\n",
    "\n",
    "params = {'batch_size': batch_size,\n",
    "          #          'input_shape': (100, 313, 1),\n",
    "          'shuffle': True,\n",
    "          'chaug': chaug,\n",
    "          'beta_param': beta_param,\n",
    "          'cout': cout\n",
    "#              'mixup': mixup,\n",
    "          #          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "          #          'highpass': [.5, [78,79,80,81,82,83,84,85]]\n",
    "#              'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "          #           'dropblock' : [30, 100]\n",
    "          #'device' : device\n",
    "}\n",
    "\n",
    "if mixup :\n",
    "    params['mixup'] = mixup\n",
    "    params['ranfilter2'] = ranfil\n",
    "else :\n",
    "    params['cutout'] = cout\n",
    "\n",
    "params_no_shuffle = {'batch_size': batch_size,\n",
    "                     #          'input_shape': (100, 313, 1),\n",
    "                     'shuffle': False,\n",
    "                     'beta_param': 0.7,\n",
    "                     'mixup': False\n",
    "                     #'device': device\n",
    "}\n",
    "\n",
    "if ord1 :\n",
    "    class_weight = {0: mm_weight, 1: 1.}\n",
    "else :\n",
    "    class_weight = {0: mm_weight, 1: wunknown, 2:1.}\n",
    "\n",
    "if mixup :\n",
    "    TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], features_trn['preg'], features_trn['loc'], \n",
    "                                features_trn['interval'], features_trn['wav2'],\n",
    "                          features_trn['mel1'],features_trn['cqt1'],features_trn['stft1']], \n",
    "                         mm_lbs_trn,  ## our Y\n",
    "                             **params)()\n",
    "    model1.fit(TrainDGen_1,\n",
    "               validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                                   features_test['preg'], features_test['loc'], \n",
    "                                   features_test['interval'],features_test['wav2'],\n",
    "                                   features_test['mel1'], features_test['cqt1'], features_test['stft1']], \n",
    "                                  mm_lbs_test), \n",
    "               callbacks=[lr],\n",
    "               steps_per_epoch=np.ceil(len(mm_lbs_trn)/batch_size),\n",
    "               class_weight=class_weight, \n",
    "               epochs = n_epoch)\n",
    "\n",
    "else :\n",
    "    TrainGen = DataGenerator([features_trn['age'],features_trn['sex'], features_trn['hw'], features_trn['preg'], features_trn['loc'], \n",
    "                                features_trn['interval'], features_trn['wav2'],\n",
    "                      features_trn['mel1'],features_trn['cqt1'],features_trn['stft1']], \n",
    "                     mm_lbs_trn,  ## our Y\n",
    "                     **params)\n",
    "    model1.fit(TrainGen,\n",
    "               validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                                   features_test['preg'], features_test['loc'], \n",
    "                                   features_test['interval'],features_test['wav2'],\n",
    "                                   features_test['mel1'], features_test['cqt1'], features_test['stft1']], \n",
    "                                  mm_lbs_test), \n",
    "               callbacks=[lr],\n",
    "               #        steps_per_epoch=np.ceil(len(mm_lbs_trn)/64),\n",
    "               class_weight=class_weight, \n",
    "               epochs = n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3048bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 100\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "batch_size = 64\n",
    "params = {'batch_size': batch_size,\n",
    "          #          'input_shape': (100, 313, 1),\n",
    "          'shuffle': True,\n",
    "          'chaug': 0,\n",
    "          'beta_param': beta_param,\n",
    "          'cout': cout,\n",
    "#              'mixup': True,\n",
    "          #          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "#            'highpass': [.5, [78,79,80,81,82,83,84,85]],\n",
    "#              'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "        #           'dropblock' : [30, 100]\n",
    "          #'device' : device\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "if mixup :\n",
    "    params['mixup'] = mixup\n",
    "    params['ranfilter2'] = ranfil\n",
    "else :\n",
    "    params['cutout'] = cout\n",
    "\n",
    "\n",
    "params_no_shuffle = {'batch_size': batch_size,\n",
    "                     #          'input_shape': (100, 313, 1),\n",
    "                     'shuffle': False,\n",
    "                     'beta_param': 0.7,\n",
    "                     'mixup': False\n",
    "                     #'device': device\n",
    "}\n",
    "\n",
    "class_weight = {0: oo_weight, 1: 1.}\n",
    "\n",
    "if mixup :\n",
    "    TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], features_trn['preg'], features_trn['loc'],\n",
    "                              features_trn['interval'], features_trn['wav2'],\n",
    "                          features_trn['mel1'],features_trn['cqt1'],features_trn['stft1']], \n",
    "                         out_lbs_trn,  ## our Y\n",
    "                         **params)()\n",
    "\n",
    "    model2.fit(TrainDGen_1,\n",
    "               validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                                   features_test['preg'], features_test['loc'], \n",
    "                                   features_test['interval'],features_test['wav2'],\n",
    "                                   features_test['mel1'], \n",
    "                                   features_test['cqt1'], features_test['stft1']], \n",
    "                                  out_lbs_test), \n",
    "               callbacks=[lr],\n",
    "               steps_per_epoch=np.ceil(len(out_lbs_trn)/batch_size),\n",
    "               class_weight=class_weight, \n",
    "               epochs = n_epoch)\n",
    "else :\n",
    "    TrainGen = DataGenerator([features_trn['age'],features_trn['sex'], features_trn['hw'], features_trn['preg'], features_trn['loc'],\n",
    "                              features_trn['interval'], features_trn['wav2'],\n",
    "                      features_trn['mel1'],features_trn['cqt1'],features_trn['stft1']], \n",
    "                     out_lbs_trn,  ## our Y\n",
    "                     **params)\n",
    "    model2.fit(TrainGen,\n",
    "               validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                                   features_test['preg'], features_test['loc'], \n",
    "                                   features_test['interval'],features_test['wav2'],\n",
    "                                   features_test['mel1'], \n",
    "                                   features_test['cqt1'], features_test['stft1']], \n",
    "                                  out_lbs_test), \n",
    "               callbacks=[lr],\n",
    "               class_weight=class_weight, \n",
    "               epochs = n_epoch)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dfe4f273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'samp_sec': 20, 'pre_emphasis': 0, 'hop_length': 256, 'win_length': 512, 'n_mels': 120, 'filter_scale': 1, 'n_bins': 80, 'fmin': 10, 'min_dist': 500, 'max_interval_len': 192, 'per_sec': 16000, 'use_interval': True, 'maxlen1': 246000, 'use_wav2': True, 'trim': 0, 'use_mel': True, 'use_cqt': False, 'use_stft': False, 'ord1': True, 'mm_mean': False, 'dp': 0, 'fc': False, 'ext': False, 'oo_weight': 3, 'mm_weight': 3, 'chaug': 10, 'cout': 0.8, 'wunknown': 1, 'mixup': True, 'n1': 0, 'mel_shape': (120, 313, 1), 'cqt_shape': (1, 1, 1), 'stft_shape': (1, 1, 1), 'model1': 'lcnn1_dr', 'model2': 'lcnn2_dr', 'model_fnm1': 'lcnn_fix_rr_w2v2/lcnn1_dr_model1.hdf5', 'model_fnm2': 'lcnn_fix_rr_w2v2/lcnn2_dr_model2.hdf5', 'mm_weighted_accuracy': 0.7897574123989218, 'interval_input_shape': (192, 1), 'wav2_input_shape': (246000,)}\n"
     ]
    }
   ],
   "source": [
    "params_feature['ord1'] = ord1\n",
    "params_feature['mm_mean'] = mm_mean\n",
    "params_feature['dp'] = dp\n",
    "params_feature['fc'] = fc\n",
    "params_feature['ext'] = ext\n",
    "params_feature['oo_weight'] = oo_weight\n",
    "params_feature['mm_weight'] = mm_weight\n",
    "params_feature['chaug'] = chaug\n",
    "params_feature['cout'] = cout\n",
    "params_feature['wunknown'] = wunknown\n",
    "params_feature['mixup'] = mixup\n",
    "params_feature['n1'] = n1\n",
    "\n",
    "params_feature['mel_shape'] = mel_input_shape\n",
    "params_feature['cqt_shape'] = cqt_input_shape\n",
    "params_feature['stft_shape'] = stft_input_shape\n",
    "\n",
    "params_feature['use_mel'] = use_mel\n",
    "params_feature['use_cqt'] = use_cqt\n",
    "params_feature['use_stft'] = use_stft\n",
    "\n",
    "params_feature['interval_input_shape'] = interval_input_shape\n",
    "# params_feature['raw1_input_shape'] = raw1_input_shape\n",
    "params_feature['wav2_input_shape'] = wav2_input_shape\n",
    "params_feature['max_interval_len'] = max_interval_len\n",
    "\n",
    "print(params_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed2650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "80f929a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model.\n",
    "def run_model(model_folder, data_folder, output_folder, allow_failures, verbose):\n",
    "    # Load models.\n",
    "    if verbose >= 1:\n",
    "        print('Loading Challenge model...')\n",
    "\n",
    "    model = load_challenge_model(model_folder, verbose) ### Teams: Implement this function!!!\n",
    "\n",
    "    # Find the patient data files.\n",
    "    patient_files = find_patient_files(data_folder)\n",
    "    num_patient_files = len(patient_files)\n",
    "\n",
    "    if num_patient_files==0:\n",
    "        raise Exception('No data was provided.')\n",
    "\n",
    "    # Create a folder for the Challenge outputs if it does not already exist.\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Run the team's model on the Challenge data.\n",
    "    if verbose >= 1:\n",
    "        print('Running model on Challenge data...')\n",
    "\n",
    "#    @tf.function\n",
    "    # Iterate over the patient files.\n",
    "    for i in range(num_patient_files):\n",
    "        if verbose >= 2:\n",
    "            print('    {}/{}...'.format(i+1, num_patient_files))\n",
    "\n",
    "        patient_data = load_patient_data(patient_files[i])\n",
    "        recordings = load_recordings(data_folder, patient_data)\n",
    "\n",
    "        # Allow or disallow the model to fail on parts of the data; helpful for debugging.\n",
    "        try:\n",
    "            classes, labels, probabilities = run_challenge_model(model, patient_data, recordings, verbose) ### Teams: Implement this function!!!\n",
    "        except:\n",
    "            if allow_failures:\n",
    "                if verbose >= 2:\n",
    "                    print('... failed.')\n",
    "                classes, labels, probabilities = list(), list(), list()\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # Save Challenge outputs.\n",
    "        head, tail = os.path.split(patient_files[i])\n",
    "        root, extension = os.path.splitext(tail)\n",
    "        output_file = os.path.join(output_folder, root + '.csv')\n",
    "        patient_id = get_patient_id(patient_data)\n",
    "        save_challenge_outputs(output_file, patient_id, classes, labels, probabilities)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print('Done.')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4bc2c548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Challenge model...\n",
      "Running model on Challenge data...\n",
      "    1/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    5/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    6/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    7/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    10/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    11/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    12/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    13/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    14/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    15/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    16/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    17/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:01<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    18/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    19/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    20/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    21/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    22/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    23/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    24/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    25/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    26/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    27/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    28/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    29/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    30/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    31/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    32/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    33/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    34/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    35/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    36/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    37/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    38/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:01<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    39/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    40/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    41/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    42/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    43/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    44/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    45/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    46/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    47/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    48/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    49/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    50/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    51/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    52/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:01<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    53/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    54/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    55/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    56/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    57/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    58/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    59/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    60/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    61/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    62/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:02<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    63/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    64/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    65/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    66/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    67/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    68/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    69/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    70/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:04<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    71/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    72/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    73/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    74/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    75/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    76/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    77/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    78/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    79/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    80/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    81/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    82/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    83/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    84/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    85/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    86/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    87/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    88/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    89/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    90/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    91/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    92/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    93/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    94/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    95/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    97/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    98/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    99/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    100/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    101/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    102/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    103/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    104/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  8.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    105/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    106/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    107/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    108/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    109/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    110/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    111/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    112/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    113/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    114/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    115/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    116/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    117/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    118/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    119/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    120/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    121/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    122/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    123/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    124/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    125/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    126/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    127/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    128/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    129/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    130/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    131/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    132/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    133/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    134/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    135/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    136/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    137/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    138/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    139/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    140/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    141/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    142/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    143/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    144/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    145/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    146/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    147/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    148/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    149/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    150/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    151/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    152/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    153/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    154/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    155/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    156/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    158/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    159/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    160/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    161/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    162/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    163/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    164/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    165/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    166/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    167/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    168/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    169/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    170/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    171/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    172/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    173/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    174/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    175/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    176/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    177/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    178/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    179/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    180/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    181/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    182/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    183/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    184/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    185/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    186/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    187/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    188/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    189/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    190/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    191/191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "save_challenge_model(model_folder, model1, model2, m_name1 = 'lcnn1_dr', m_name2 = 'lcnn2_dr', param_feature = params_feature)\n",
    "\n",
    "run_model(model_folder, test_folder, output_folder, allow_failures = True, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7a7e81db",
   "metadata": {},
   "outputs": [],
   "source": [
    "murmur_scores, outcome_scores = evaluate_model(test_folder, output_folder)\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "','.join(classes),\n",
    "','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "params_feature['mm_weighted_accuracy'] = weighted_accuracy\n",
    "\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "','.join(classes),\n",
    "','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "+ '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "de12683a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.526,0.801,0.768,17574.098\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.611,0.623,0.726,11426.775\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.702,0.000,0.876\n",
      "Accuracy,0.868,0.000,0.863\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.679,0.544\n",
      "Accuracy,0.776,0.462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## sy 내가 동일한 코드로 돌렸을 때 결과 1\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0d1d6b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.545,0.822,0.790,17566.761\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.589,0.607,0.734,11453.603\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.747,0.000,0.888\n",
      "Accuracy,0.895,0.000,0.885\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.675,0.503\n",
      "Accuracy,0.796,0.409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## sy 내가 동일한 코드로 돌렸을 때 결과 2\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "de5c3a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4094367159910851\n",
      "0.5905632875196597\n",
      "0.6565561481795386\n",
      "0.34344385377087516\n"
     ]
    }
   ],
   "source": [
    "label_folder = test_folder\n",
    "murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "# Load and parse label and model output files.\n",
    "label_files, output_files = find_challenge_files(label_folder, output_folder)\n",
    "murmur_labels = load_murmurs(label_files, murmur_classes)\n",
    "murmur_binary_outputs, murmur_scalar_outputs = load_classifier_outputs(output_files, murmur_classes)\n",
    "outcome_labels = load_outcomes(label_files, outcome_classes)\n",
    "outcome_binary_outputs, outcome_scalar_outputs = load_classifier_outputs(output_files, outcome_classes)\n",
    "\n",
    "\n",
    "print(np.mean(murmur_scalar_outputs[:,0]))\n",
    "print(np.mean(murmur_scalar_outputs[:,2]))\n",
    "print(np.mean(outcome_scalar_outputs[:,0]))\n",
    "print(np.mean(outcome_scalar_outputs[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fbb3e11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold:  0.01\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.111,0.199,0.512,15140.890\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.332,0.000,0.000\n",
      "Accuracy,1.000,0.000,0.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.05\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.111,0.199,0.512,15140.890\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.332,0.000,0.000\n",
      "Accuracy,1.000,0.000,0.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.1\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.111,0.199,0.512,15131.464\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.333,0.000,0.000\n",
      "Accuracy,1.000,0.000,0.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.15\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.132,0.220,0.523,14509.033\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.341,0.000,0.055\n",
      "Accuracy,1.000,0.000,0.029\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.2\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.184,0.277,0.553,13557.314\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.358,0.000,0.192\n",
      "Accuracy,1.000,0.000,0.108\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.25\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.272,0.393,0.612,12169.412\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.402,0.000,0.413\n",
      "Accuracy,1.000,0.000,0.266\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.3\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.354,0.518,0.677,12779.504\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.469,0.000,0.592\n",
      "Accuracy,1.000,0.000,0.439\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.35\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.426,0.639,0.728,13658.301\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.544,0.000,0.733\n",
      "Accuracy,0.974,0.000,0.612\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.4\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.467,0.712,0.744,15011.943\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.334,0.503,0.823,15125.621\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.593,0.000,0.808\n",
      "Accuracy,0.921,0.000,0.727\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.669,0.000\n",
      "Accuracy,0.980,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.45\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.513,0.780,0.768,16966.900\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.369,0.518,0.828,14500.929\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.680,0.000,0.858\n",
      "Accuracy,0.895,0.000,0.827\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.676,0.061\n",
      "Accuracy,0.980,0.032\n",
      "\n",
      "-------------\n",
      "threshold:  0.5\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.543,0.822,0.779,17981.697\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.406,0.518,0.794,13683.005\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.742,0.000,0.889\n",
      "Accuracy,0.868,0.000,0.892\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.664,0.148\n",
      "Accuracy,0.929,0.086\n",
      "\n",
      "-------------\n",
      "threshold:  0.55\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.556,0.838,0.776,18602.007\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.490,0.555,0.779,12286.387\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.771,0.000,0.898\n",
      "Accuracy,0.842,0.000,0.921\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.672,0.309\n",
      "Accuracy,0.888,0.204\n",
      "\n",
      "-------------\n",
      "threshold:  0.6\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.577,0.859,0.765,19853.373\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.593,0.607,0.720,11608.121\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.822,0.000,0.908\n",
      "Accuracy,0.789,0.000,0.964\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.670,0.516\n",
      "Accuracy,0.776,0.430\n",
      "\n",
      "-------------\n",
      "threshold:  0.65\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.558,0.848,0.706,20898.997\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.623,0.623,0.630,13239.453\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.769,0.000,0.904\n",
      "Accuracy,0.658,0.000,0.986\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.633,0.613\n",
      "Accuracy,0.633,0.613\n",
      "\n",
      "-------------\n",
      "threshold:  0.7\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.543,0.838,0.679,21107.966\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.671,0.681,0.559,15496.747\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.730,0.000,0.898\n",
      "Accuracy,0.605,0.000,0.986\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.616,0.726\n",
      "Accuracy,0.500,0.871\n",
      "\n",
      "-------------\n",
      "threshold:  0.75\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.520,0.827,0.631,21733.990\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.630,0.654,0.475,17763.431\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.667,0.000,0.894\n",
      "Accuracy,0.500,0.000,1.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.535,0.725\n",
      "Accuracy,0.388,0.935\n",
      "\n",
      "-------------\n",
      "threshold:  0.8\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.762,0.613,0.416,0.775,0.496,23819.581\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.683,0.518,0.586,0.329,21525.500\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.947,0.500,0.840\n",
      "AUPRC,0.883,0.073,0.884\n",
      "F-measure,0.383,0.000,0.866\n",
      "Accuracy,0.237,0.000,1.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.758,0.609\n",
      "F-measure,0.336,0.700\n",
      "Accuracy,0.204,0.989\n",
      "\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "# test 3\n",
    "for th1 in [0.01, 0.05, 0.1, 0.15,0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8] :\n",
    "    murmur_binary_outputs[:,0] = murmur_scalar_outputs[:,0] > th1\n",
    "    murmur_binary_outputs[:,2] = murmur_scalar_outputs[:,2] > 1 - th1\n",
    "    outcome_binary_outputs[:,0] = outcome_scalar_outputs[:,0] > th1\n",
    "    outcome_binary_outputs[:,1] = outcome_scalar_outputs[:,1] > 1 - th1\n",
    "    # For each patient, set the 'Present' or 'Abnormal' class to positive if no class is positive or if multiple classes are positive.\n",
    "    murmur_labels = enforce_positives(murmur_labels, murmur_classes, 'Present')\n",
    "    murmur_binary_outputs = enforce_positives(murmur_binary_outputs, murmur_classes, 'Present')\n",
    "    outcome_labels = enforce_positives(outcome_labels, outcome_classes, 'Abnormal')\n",
    "    outcome_binary_outputs = enforce_positives(outcome_binary_outputs, outcome_classes, 'Abnormal')\n",
    "    # Evaluate the murmur model by comparing the labels and model outputs.\n",
    "    murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes = compute_auc(murmur_labels, murmur_scalar_outputs)\n",
    "    murmur_f_measure, murmur_f_measure_classes = compute_f_measure(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_accuracy, murmur_accuracy_classes = compute_accuracy(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_weighted_accuracy = compute_weighted_accuracy(murmur_labels, murmur_binary_outputs, murmur_classes) # This is the murmur scoring metric.\n",
    "    murmur_cost = compute_cost(outcome_labels, murmur_binary_outputs, outcome_classes, murmur_classes) # Use *outcomes* to score *murmurs* for the Challenge cost metric, but this is not the actual murmur scoring metric.\n",
    "    murmur_scores = (murmur_classes, murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes, \\\n",
    "                 murmur_f_measure, murmur_f_measure_classes, murmur_accuracy, murmur_accuracy_classes, murmur_weighted_accuracy, murmur_cost)\n",
    "\n",
    "    # Evaluate the outcome model by comparing the labels and model outputs.\n",
    "    outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes = compute_auc(outcome_labels, outcome_scalar_outputs)\n",
    "    outcome_f_measure, outcome_f_measure_classes = compute_f_measure(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_accuracy, outcome_accuracy_classes = compute_accuracy(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_weighted_accuracy = compute_weighted_accuracy(outcome_labels, outcome_binary_outputs, outcome_classes)\n",
    "    outcome_cost = compute_cost(outcome_labels, outcome_binary_outputs, outcome_classes, outcome_classes) # This is the clinical outcomes scoring metric.\n",
    "    outcome_scores = (outcome_classes, outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes, \\\n",
    "                  outcome_f_measure, outcome_f_measure_classes, outcome_accuracy, outcome_accuracy_classes, outcome_weighted_accuracy, outcome_cost)\n",
    "\n",
    "\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "    murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "    outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "                + '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "    print(\"threshold: \", th1)\n",
    "    print(output_string)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "46859b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold:  0.01\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.764,0.605,0.111,0.199,0.512,15140.890\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.676,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.953,0.500,0.838\n",
      "AUPRC,0.847,0.073,0.895\n",
      "F-measure,0.332,0.000,0.000\n",
      "Accuracy,1.000,0.000,0.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.669,0.669\n",
      "AUPRC,0.740,0.612\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.05\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.764,0.605,0.111,0.199,0.512,15140.890\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.676,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.953,0.500,0.838\n",
      "AUPRC,0.847,0.073,0.895\n",
      "F-measure,0.332,0.000,0.000\n",
      "Accuracy,1.000,0.000,0.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.669,0.669\n",
      "AUPRC,0.740,0.612\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.1\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.764,0.605,0.116,0.204,0.515,15131.464\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.676,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.953,0.500,0.838\n",
      "AUPRC,0.847,0.073,0.895\n",
      "F-measure,0.333,0.000,0.014\n",
      "Accuracy,1.000,0.000,0.007\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.669,0.669\n",
      "AUPRC,0.740,0.612\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.15\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.764,0.605,0.121,0.209,0.518,15125.621\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.676,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.953,0.500,0.838\n",
      "AUPRC,0.847,0.073,0.895\n",
      "F-measure,0.335,0.000,0.028\n",
      "Accuracy,1.000,0.000,0.014\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.669,0.669\n",
      "AUPRC,0.740,0.612\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.2\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.764,0.605,0.142,0.230,0.528,14535.378\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.676,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.953,0.500,0.838\n",
      "AUPRC,0.847,0.073,0.895\n",
      "F-measure,0.344,0.000,0.082\n",
      "Accuracy,1.000,0.000,0.043\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.669,0.669\n",
      "AUPRC,0.740,0.612\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.25\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.764,0.605,0.222,0.325,0.577,12474.372\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.676,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.953,0.500,0.838\n",
      "AUPRC,0.847,0.073,0.895\n",
      "F-measure,0.376,0.000,0.289\n",
      "Accuracy,1.000,0.000,0.173\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.669,0.669\n",
      "AUPRC,0.740,0.612\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.3\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.764,0.605,0.300,0.435,0.633,12360.361\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.676,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.953,0.500,0.838\n",
      "AUPRC,0.847,0.073,0.895\n",
      "F-measure,0.422,0.000,0.479\n",
      "Accuracy,1.000,0.000,0.324\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.669,0.669\n",
      "AUPRC,0.740,0.612\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.35\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.764,0.605,0.392,0.581,0.709,13263.930\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.676,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.953,0.500,0.838\n",
      "AUPRC,0.847,0.073,0.895\n",
      "F-measure,0.507,0.000,0.670\n",
      "Accuracy,1.000,0.000,0.525\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.669,0.669\n",
      "AUPRC,0.740,0.612\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.4\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.764,0.605,0.443,0.665,0.752,14403.648\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.676,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.953,0.500,0.838\n",
      "AUPRC,0.847,0.073,0.895\n",
      "F-measure,0.576,0.000,0.754\n",
      "Accuracy,1.000,0.000,0.640\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.669,0.669\n",
      "AUPRC,0.740,0.612\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.45\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.764,0.605,0.489,0.743,0.760,15134.689\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.676,0.349,0.513,0.834,14916.196\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.953,0.500,0.838\n",
      "AUPRC,0.847,0.073,0.895\n",
      "F-measure,0.642,0.000,0.826\n",
      "Accuracy,0.921,0.000,0.770\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.669,0.669\n",
      "AUPRC,0.740,0.612\n",
      "F-measure,0.676,0.021\n",
      "Accuracy,0.990,0.011\n",
      "\n",
      "-------------\n",
      "threshold:  0.5\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.764,0.605,0.537,0.812,0.774,17564.714\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.676,0.393,0.534,0.840,14090.185\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.953,0.500,0.838\n",
      "AUPRC,0.847,0.073,0.895\n",
      "F-measure,0.733,0.000,0.878\n",
      "Accuracy,0.868,0.000,0.878\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.669,0.669\n",
      "AUPRC,0.740,0.612\n",
      "F-measure,0.686,0.101\n",
      "Accuracy,0.990,0.054\n",
      "\n",
      "-------------\n",
      "threshold:  0.55\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.764,0.605,0.556,0.838,0.776,18183.159\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.676,0.468,0.550,0.798,12622.594\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.953,0.500,0.838\n",
      "AUPRC,0.847,0.073,0.895\n",
      "F-measure,0.771,0.000,0.898\n",
      "Accuracy,0.842,0.000,0.921\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.669,0.669\n",
      "AUPRC,0.740,0.612\n",
      "F-measure,0.677,0.259\n",
      "Accuracy,0.918,0.161\n",
      "\n",
      "-------------\n",
      "threshold:  0.6\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.764,0.605,0.559,0.843,0.768,18599.669\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.676,0.539,0.565,0.714,12111.304\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.953,0.500,0.838\n",
      "AUPRC,0.847,0.073,0.895\n",
      "F-measure,0.775,0.000,0.903\n",
      "Accuracy,0.816,0.000,0.935\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.669,0.669\n",
      "AUPRC,0.740,0.612\n",
      "F-measure,0.650,0.428\n",
      "Accuracy,0.786,0.333\n",
      "\n",
      "-------------\n",
      "threshold:  0.65\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.764,0.605,0.552,0.843,0.714,20480.865\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.676,0.592,0.592,0.585,14194.224\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.953,0.500,0.838\n",
      "AUPRC,0.847,0.073,0.895\n",
      "F-measure,0.754,0.000,0.903\n",
      "Accuracy,0.684,0.000,0.971\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.669,0.669\n",
      "AUPRC,0.740,0.612\n",
      "F-measure,0.594,0.589\n",
      "Accuracy,0.582,0.602\n",
      "\n",
      "-------------\n",
      "threshold:  0.7\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.764,0.605,0.556,0.848,0.695,21317.390\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.676,0.618,0.628,0.508,16543.868\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.953,0.500,0.838\n",
      "AUPRC,0.847,0.073,0.895\n",
      "F-measure,0.762,0.000,0.905\n",
      "Accuracy,0.632,0.000,0.993\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.669,0.669\n",
      "AUPRC,0.740,0.612\n",
      "F-measure,0.553,0.682\n",
      "Accuracy,0.449,0.817\n",
      "\n",
      "-------------\n",
      "threshold:  0.75\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.764,0.605,0.515,0.822,0.628,21943.908\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.676,0.614,0.644,0.451,18389.128\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.953,0.500,0.838\n",
      "AUPRC,0.847,0.073,0.895\n",
      "F-measure,0.655,0.000,0.890\n",
      "Accuracy,0.500,0.000,0.993\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.669,0.669\n",
      "AUPRC,0.740,0.612\n",
      "F-measure,0.507,0.721\n",
      "Accuracy,0.357,0.946\n",
      "\n",
      "-------------\n",
      "threshold:  0.8\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.764,0.605,0.413,0.770,0.493,23820.794\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.669,0.676,0.546,0.602,0.362,20689.358\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.953,0.500,0.838\n",
      "AUPRC,0.847,0.073,0.895\n",
      "F-measure,0.375,0.000,0.863\n",
      "Accuracy,0.237,0.000,0.993\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.669,0.669\n",
      "AUPRC,0.740,0.612\n",
      "F-measure,0.387,0.705\n",
      "Accuracy,0.245,0.978\n",
      "\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "for th1 in [0.01, 0.05, 0.1, 0.15,0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8] :\n",
    "    murmur_binary_outputs[:,0] = murmur_scalar_outputs[:,0] > th1\n",
    "    murmur_binary_outputs[:,2] = murmur_scalar_outputs[:,2] > 1 - th1\n",
    "    outcome_binary_outputs[:,0] = outcome_scalar_outputs[:,0] > th1\n",
    "    outcome_binary_outputs[:,1] = outcome_scalar_outputs[:,1] > 1 - th1\n",
    "    # For each patient, set the 'Present' or 'Abnormal' class to positive if no class is positive or if multiple classes are positive.\n",
    "    murmur_labels = enforce_positives(murmur_labels, murmur_classes, 'Present')\n",
    "    murmur_binary_outputs = enforce_positives(murmur_binary_outputs, murmur_classes, 'Present')\n",
    "    outcome_labels = enforce_positives(outcome_labels, outcome_classes, 'Abnormal')\n",
    "    outcome_binary_outputs = enforce_positives(outcome_binary_outputs, outcome_classes, 'Abnormal')\n",
    "    # Evaluate the murmur model by comparing the labels and model outputs.\n",
    "    murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes = compute_auc(murmur_labels, murmur_scalar_outputs)\n",
    "    murmur_f_measure, murmur_f_measure_classes = compute_f_measure(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_accuracy, murmur_accuracy_classes = compute_accuracy(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_weighted_accuracy = compute_weighted_accuracy(murmur_labels, murmur_binary_outputs, murmur_classes) # This is the murmur scoring metric.\n",
    "    murmur_cost = compute_cost(outcome_labels, murmur_binary_outputs, outcome_classes, murmur_classes) # Use *outcomes* to score *murmurs* for the Challenge cost metric, but this is not the actual murmur scoring metric.\n",
    "    murmur_scores = (murmur_classes, murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes, \\\n",
    "                 murmur_f_measure, murmur_f_measure_classes, murmur_accuracy, murmur_accuracy_classes, murmur_weighted_accuracy, murmur_cost)\n",
    "\n",
    "    # Evaluate the outcome model by comparing the labels and model outputs.\n",
    "    outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes = compute_auc(outcome_labels, outcome_scalar_outputs)\n",
    "    outcome_f_measure, outcome_f_measure_classes = compute_f_measure(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_accuracy, outcome_accuracy_classes = compute_accuracy(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_weighted_accuracy = compute_weighted_accuracy(outcome_labels, outcome_binary_outputs, outcome_classes)\n",
    "    outcome_cost = compute_cost(outcome_labels, outcome_binary_outputs, outcome_classes, outcome_classes) # This is the clinical outcomes scoring metric.\n",
    "    outcome_scores = (outcome_classes, outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes, \\\n",
    "                  outcome_f_measure, outcome_f_measure_classes, outcome_accuracy, outcome_accuracy_classes, outcome_weighted_accuracy, outcome_cost)\n",
    "\n",
    "\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "    murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "    outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "                + '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "    print(\"threshold: \", th1)\n",
    "    print(output_string)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fd5c6552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold:  0.01\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.111,0.199,0.512,15140.890\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.332,0.000,0.000\n",
      "Accuracy,1.000,0.000,0.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.05\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.111,0.199,0.512,15140.890\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.332,0.000,0.000\n",
      "Accuracy,1.000,0.000,0.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.1\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.116,0.204,0.515,15131.464\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.333,0.000,0.014\n",
      "Accuracy,1.000,0.000,0.007\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.15\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.137,0.225,0.526,14927.881\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.341,0.000,0.069\n",
      "Accuracy,1.000,0.000,0.036\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.2\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.175,0.267,0.547,13683.005\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.355,0.000,0.169\n",
      "Accuracy,1.000,0.000,0.094\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.25\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.290,0.419,0.625,11998.784\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.415,0.000,0.454\n",
      "Accuracy,1.000,0.000,0.302\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.3\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.347,0.508,0.671,11904.515\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.461,0.000,0.581\n",
      "Accuracy,1.000,0.000,0.424\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.35\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.424,0.639,0.717,14238.709\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.537,0.000,0.735\n",
      "Accuracy,0.947,0.000,0.619\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.4\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.475,0.723,0.749,16015.647\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.614,0.000,0.811\n",
      "Accuracy,0.921,0.000,0.741\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.45\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.516,0.785,0.771,16962.716\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.337,0.508,0.832,15131.464\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.687,0.000,0.862\n",
      "Accuracy,0.895,0.000,0.835\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.674,0.000\n",
      "Accuracy,0.990,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.5\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.555,0.832,0.795,17770.578\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.360,0.518,0.835,14704.473\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.773,0.000,0.893\n",
      "Accuracy,0.895,0.000,0.899\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.678,0.042\n",
      "Accuracy,0.990,0.022\n",
      "\n",
      "-------------\n",
      "threshold:  0.55\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.546,0.827,0.749,18809.093\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.451,0.550,0.818,12929.042\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.750,0.000,0.889\n",
      "Accuracy,0.789,0.000,0.921\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.684,0.218\n",
      "Accuracy,0.949,0.129\n",
      "\n",
      "-------------\n",
      "threshold:  0.6\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.561,0.848,0.739,20062.573\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.554,0.581,0.732,11773.563\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.778,0.000,0.905\n",
      "Accuracy,0.737,0.000,0.964\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.664,0.444\n",
      "Accuracy,0.806,0.344\n",
      "\n",
      "-------------\n",
      "threshold:  0.65\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.543,0.838,0.679,20898.542\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.644,0.644,0.657,12662.580\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.730,0.000,0.898\n",
      "Accuracy,0.605,0.000,0.986\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.657,0.630\n",
      "Accuracy,0.663,0.624\n",
      "\n",
      "-------------\n",
      "threshold:  0.7\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.543,0.838,0.679,20898.542\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.661,0.670,0.549,15706.171\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.730,0.000,0.898\n",
      "Accuracy,0.605,0.000,0.986\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.604,0.717\n",
      "Accuracy,0.490,0.860\n",
      "\n",
      "-------------\n",
      "threshold:  0.75\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.519,0.822,0.639,21316.468\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.583,0.623,0.410,19434.301\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.667,0.000,0.890\n",
      "Accuracy,0.526,0.000,0.986\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.455,0.712\n",
      "Accuracy,0.306,0.957\n",
      "\n",
      "-------------\n",
      "threshold:  0.8\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.760,0.610,0.468,0.796,0.561,22778.525\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.716,0.712,0.492,0.571,0.304,22152.287\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.941,0.500,0.839\n",
      "AUPRC,0.860,0.073,0.897\n",
      "F-measure,0.528,0.000,0.876\n",
      "Accuracy,0.368,0.000,0.993\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.716,0.716\n",
      "AUPRC,0.770,0.653\n",
      "F-measure,0.293,0.692\n",
      "Accuracy,0.173,0.989\n",
      "\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "# test 2\n",
    "for th1 in [0.01, 0.05, 0.1, 0.15,0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8] :\n",
    "    murmur_binary_outputs[:,0] = murmur_scalar_outputs[:,0] > th1\n",
    "    murmur_binary_outputs[:,2] = murmur_scalar_outputs[:,2] > 1 - th1\n",
    "    outcome_binary_outputs[:,0] = outcome_scalar_outputs[:,0] > th1\n",
    "    outcome_binary_outputs[:,1] = outcome_scalar_outputs[:,1] > 1 - th1\n",
    "    # For each patient, set the 'Present' or 'Abnormal' class to positive if no class is positive or if multiple classes are positive.\n",
    "    murmur_labels = enforce_positives(murmur_labels, murmur_classes, 'Present')\n",
    "    murmur_binary_outputs = enforce_positives(murmur_binary_outputs, murmur_classes, 'Present')\n",
    "    outcome_labels = enforce_positives(outcome_labels, outcome_classes, 'Abnormal')\n",
    "    outcome_binary_outputs = enforce_positives(outcome_binary_outputs, outcome_classes, 'Abnormal')\n",
    "    # Evaluate the murmur model by comparing the labels and model outputs.\n",
    "    murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes = compute_auc(murmur_labels, murmur_scalar_outputs)\n",
    "    murmur_f_measure, murmur_f_measure_classes = compute_f_measure(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_accuracy, murmur_accuracy_classes = compute_accuracy(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_weighted_accuracy = compute_weighted_accuracy(murmur_labels, murmur_binary_outputs, murmur_classes) # This is the murmur scoring metric.\n",
    "    murmur_cost = compute_cost(outcome_labels, murmur_binary_outputs, outcome_classes, murmur_classes) # Use *outcomes* to score *murmurs* for the Challenge cost metric, but this is not the actual murmur scoring metric.\n",
    "    murmur_scores = (murmur_classes, murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes, \\\n",
    "                 murmur_f_measure, murmur_f_measure_classes, murmur_accuracy, murmur_accuracy_classes, murmur_weighted_accuracy, murmur_cost)\n",
    "\n",
    "    # Evaluate the outcome model by comparing the labels and model outputs.\n",
    "    outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes = compute_auc(outcome_labels, outcome_scalar_outputs)\n",
    "    outcome_f_measure, outcome_f_measure_classes = compute_f_measure(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_accuracy, outcome_accuracy_classes = compute_accuracy(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_weighted_accuracy = compute_weighted_accuracy(outcome_labels, outcome_binary_outputs, outcome_classes)\n",
    "    outcome_cost = compute_cost(outcome_labels, outcome_binary_outputs, outcome_classes, outcome_classes) # This is the clinical outcomes scoring metric.\n",
    "    outcome_scores = (outcome_classes, outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes, \\\n",
    "                  outcome_f_measure, outcome_f_measure_classes, outcome_accuracy, outcome_accuracy_classes, outcome_weighted_accuracy, outcome_cost)\n",
    "\n",
    "\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "    murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "    outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "                + '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "    print(\"threshold: \", th1)\n",
    "    print(output_string)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8009deb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e3e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c49fd1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.767,0.619,0.544,0.817,0.798,17571.442\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.713,0.709,0.514,0.581,0.815,11846.100\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.958,0.500,0.842\n",
      "AUPRC,0.886,0.073,0.897\n",
      "F-measure,0.753,0.000,0.880\n",
      "Accuracy,0.921,0.000,0.871\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.713,0.713\n",
      "AUPRC,0.751,0.667\n",
      "F-measure,0.695,0.333\n",
      "Accuracy,0.929,0.215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 교수님 출력 결과\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655fa704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8f947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6eda80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84920/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a6057f",
   "metadata": {},
   "source": [
    "## LCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ed092a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Data2/hmd/hmd_sy/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a45589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import librosa\n",
    "import librosa.display\n",
    "import math\n",
    "import sys\n",
    "#sys.path.insert(0,'/home/ikwak2/hmd/notebooks')\n",
    "#sys.path.insert(0,'/home/ikwak2/hmd/iy_classifier')\n",
    "sys.path.insert(0,'/Data2/hmd/hmd_sy/evaluation-2022')\n",
    "sys.path.insert(0,'utils')\n",
    "from helper_code import *\n",
    "from get_feature import *\n",
    "from models import *\n",
    "from Generator0 import *\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from evaluate_model import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3bbd292",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'physionet.org/files/circor-heart-sound/1.0.3'\n",
    "training_data_file = root_dir + '/' + 'training_data.csv'\n",
    "training_data_dir = root_dir + '/' + 'training_data'\n",
    "model_dir = root_dir + '/' + 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52a9d85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 31 13:28:08 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     On   | 00000000:1A:00.0 Off |                  Off |\n",
      "| 33%   28C    P8    24W / 260W |   4819MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000     On   | 00000000:3D:00.0 Off |                  Off |\n",
      "| 33%   28C    P8     6W / 260W |   3310MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Quadro RTX 8000     On   | 00000000:3E:00.0 Off |                  Off |\n",
      "| 46%   69C    P2   247W / 260W |  41282MiB / 48601MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro RTX 8000     On   | 00000000:40:00.0 Off |                  Off |\n",
      "| 47%   69C    P2   254W / 260W |  42074MiB / 48601MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Quadro RTX 8000     On   | 00000000:41:00.0 Off |                  Off |\n",
      "| 33%   31C    P8    13W / 260W |  48121MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb1c20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[4], 'GPU')\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b6c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder =  '/Data2/hmd/physionet.org/files/circor-heart-sound/1.0.3/training_data'\n",
    "train_folder =  '/Data2/hmd/data_split/murmur/train'\n",
    "test_folder = '/Data2/hmd/data_split/murmur/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0afaa",
   "metadata": {},
   "source": [
    "## QRS_Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2e0c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## detect QRS complex from ECG time series\n",
    "\n",
    "import numpy as np \n",
    "import math\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_ecg(file_name):\n",
    "\treturn genfromtxt(file_name, delimiter=',')\n",
    "\n",
    "def lgth_transform(ecg, ws):\n",
    "\tlgth=ecg.shape[0]\n",
    "\tsqr_diff=np.zeros(lgth)\n",
    "\tdiff=np.zeros(lgth)\n",
    "\tecg=np.pad(ecg, ws, 'edge')\n",
    "\tfor i in range(lgth):\n",
    "\t\ttemp=ecg[i:i+ws+ws+1]\n",
    "\t\tleft=temp[ws]-temp[0]\n",
    "\t\tright=temp[ws]-temp[-1]\n",
    "\t\tdiff[i]=min(left, right)\n",
    "\t\tdiff[diff<0]=0\n",
    "\t# sqr_diff=np.multiply(diff, diff)\n",
    "\t# diff=ecg[:-1]-ecg[1:]\n",
    "\t# sqr_diff[:-1]=np.multiply(diff, diff)\n",
    "\t# sqr_diff[-1]=sqr_diff[-2]\n",
    "\treturn np.multiply(diff, diff)\n",
    "\n",
    "def integrate(ecg, ws):\n",
    "\tlgth=ecg.shape[0]\n",
    "\tintegrate_ecg=np.zeros(lgth)\n",
    "\tecg=np.pad(ecg, math.ceil(ws/2), mode='symmetric')\n",
    "\tfor i in range(lgth):\n",
    "\t\tintegrate_ecg[i]=np.sum(ecg[i:i+ws])/ws\n",
    "\treturn integrate_ecg\n",
    "\n",
    "def find_peak(data, ws):\n",
    "\tlgth=data.shape[0]\n",
    "\ttrue_peaks=list()\n",
    "\tfor i in range(lgth-ws+1):\n",
    "\t\ttemp=data[i:i+ws]\n",
    "\t\tif np.var(temp)<5:\n",
    "\t\t\tcontinue\n",
    "\t\tindex=int((ws-1)/2)\n",
    "\t\tpeak=True\n",
    "\t\tfor j in range(index):\n",
    "\t\t\tif temp[index-j]<=temp[index-j-1] or temp[index+j]<=temp[index+j+1]:\n",
    "\t\t\t\tpeak=False\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\tif peak is True:\n",
    "\t\t\ttrue_peaks.append(int(i+(ws-1)/2))\n",
    "\treturn np.asarray(true_peaks)\n",
    "\n",
    "def find_R_peaks(ecg, peaks, ws):\n",
    "\tnum_peak=peaks.shape[0]\n",
    "\tR_peaks=list()\n",
    "\tfor index in range(num_peak):\n",
    "\t\ti=peaks[index]\n",
    "\t\tif i-2*ws>0 and i<ecg.shape[0]:\n",
    "\t\t\ttemp_ecg=ecg[i-2*ws:i]\n",
    "\t\t\tR_peaks.append(int(np.argmax(temp_ecg)+i-2*ws))\n",
    "\treturn np.asarray(R_peaks)\n",
    "\n",
    "def find_S_point(ecg, R_peaks):\n",
    "\tnum_peak=R_peaks.shape[0]\n",
    "\tS_point=list()\n",
    "\tfor index in range(num_peak):\n",
    "\t\ti=R_peaks[index]\n",
    "\t\tcnt=i\n",
    "\t\tif cnt+1>=ecg.shape[0]:\n",
    "\t\t\tbreak\n",
    "\t\twhile ecg[cnt]>ecg[cnt+1]:\n",
    "\t\t\tcnt+=1\n",
    "\t\t\tif cnt>=ecg.shape[0]:\n",
    "\t\t\t\tbreak\n",
    "\t\tS_point.append(cnt)\n",
    "\treturn np.asarray(S_point)\n",
    "\n",
    "\n",
    "def find_Q_point(ecg, R_peaks):\n",
    "\tnum_peak=R_peaks.shape[0]\n",
    "\tQ_point=list()\n",
    "\tfor index in range(num_peak):\n",
    "\t\ti=R_peaks[index]\n",
    "\t\tcnt=i\n",
    "\t\tif cnt-1<0:\n",
    "\t\t\tbreak\n",
    "\t\twhile ecg[cnt]>ecg[cnt-1]:\n",
    "\t\t\tcnt-=1\n",
    "\t\t\tif cnt<0:\n",
    "\t\t\t\tbreak\n",
    "\t\tQ_point.append(cnt)\n",
    "\treturn np.asarray(Q_point)\n",
    "\n",
    "# QRS 계산하는 함수\n",
    "def EKG_QRS_detect1(ecg, fs, QS, plot=False):\n",
    "\tsig_lgth=ecg.shape[0]\n",
    "\tecg=ecg-np.mean(ecg)\n",
    "\tecg_lgth_transform=lgth_transform(ecg, int(fs/222))\n",
    "\t# ecg_lgth_transform=lgth_transform(ecg_lgth_transform, int(fs/40))\n",
    "\n",
    "\tws=int(fs/89)\n",
    "\tecg_integrate=integrate(ecg_lgth_transform, ws)/ws\n",
    "\tws=int(fs/67)\n",
    "\tecg_integrate=integrate(ecg_integrate, ws)\n",
    "\tws=int(fs/400)\n",
    "\tecg_integrate=integrate(ecg_integrate, ws)\n",
    "\tws=int(fs/800)\n",
    "\tecg_integrate=integrate(ecg_integrate, ws)\n",
    "\n",
    "\tpeaks=find_peak(ecg_integrate, int(fs/111))\n",
    "\tR_peaks=find_R_peaks(ecg, peaks, int(fs/444))\n",
    "\tif QS:\n",
    "\t\tS_point=find_S_point(ecg, R_peaks)\n",
    "\t\tQ_point=find_Q_point(ecg, R_peaks)\n",
    "\telse:\n",
    "\t\tS_point=None\n",
    "\t\tQ_point=None\n",
    "\tif plot:\n",
    "\t\tindex=np.arange(sig_lgth)/fs\n",
    "\t\tfig, ax=plt.subplots()\n",
    "\t\tax.plot(index, ecg, 'b', label='EKG')\n",
    "\t\tax.plot(R_peaks/fs, ecg[R_peaks], 'ro', label='R peaks')\n",
    "\t\tif QS:\n",
    "\t\t\tax.plot(S_point/fs, ecg[S_point], 'go', label='S')\n",
    "\t\t\tax.plot(Q_point/fs, ecg[Q_point], 'yo', label='Q')\n",
    "\t\tax.set_xlim([0, sig_lgth/fs])\n",
    "\t\tax.set_xlabel('Time [sec]')\n",
    "\t\tax.legend()\n",
    "\t\t# ax[1].plot(ecg_integrate)\n",
    "\t\t# ax[1].set_xlim([0, ecg_integrate.shape[0]])\n",
    "\t\t# ax[2].plot(ecg_lgth_transform)\n",
    "\t\t# ax[2].set_xlim([0, ecg_lgth_transform.shape[0]])\n",
    "\t\tplt.figure(figsize=(20,45))\n",
    "\t\tplt.show()\n",
    "\treturn R_peaks, S_point, Q_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f7665",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7cdbbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3438eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidal_decay(e, start=0, end=100, lr_start=1e-3, lr_end=1e-5):\n",
    "    if e < start:\n",
    "        return lr_start\n",
    "    elif e > end:\n",
    "        return lr_end\n",
    "\n",
    "    middle = (start + end) / 2\n",
    "    s = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return s(13 * (-e + middle) / np.abs(end - start)) * np.abs(lr_start - lr_end) + lr_end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ac699",
   "metadata": {},
   "source": [
    "### get feature 함수확장: 음성피쳐 옵션들과, 추가 음성들 고려한 피쳐추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c63a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_files_trn = find_patient_files(train_folder)\n",
    "patient_files_test = find_patient_files(test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c65f40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Data2/hmd/hmd_sy/notebooks'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9bfc11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = 'lcnn__rr_qrs3'\n",
    "output_folder = '/Data2/hmd/hmd_sy/notebooks/out_lcnn__rr_qrs3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "278cf4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "751"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_patient_files = len(patient_files_trn)\n",
    "num_patient_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6f0a1432",
   "metadata": {},
   "outputs": [],
   "source": [
    "        i = 1\n",
    "        current_patient_data = load_patient_data(patient_files_trn[i])\n",
    "        num_locations = get_num_locations(current_patient_data)\n",
    "        recording_information = current_patient_data.split('\\n')[1:num_locations+1]\n",
    "        for j in range(num_locations) :\n",
    "            entries = recording_information[j].split(' ')\n",
    "            recording_file = entries[2]\n",
    "            filename = os.path.join(data_folder, recording_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f186c13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Data2/hmd/physionet.org/files/circor-heart-sound/1.0.3/training_data/9979_MV.wav'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e553b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    features = dict()\n",
    "    features['id'] = []\n",
    "    features['age'] = []\n",
    "    features['sex'] = []\n",
    "    features['hw'] = []\n",
    "    features['preg'] = []\n",
    "    features['loc'] = []\n",
    "    features['mel1'] = []\n",
    "    features['cqt1'] = []\n",
    "    features['stft1'] = []\n",
    "    features['raw1'] = []\n",
    "    features['rr1'] = []\n",
    "    features['qrs1'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e4a17e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_qrs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bac6e9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "422bbb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "            # Extract id\n",
    "            id1 = recording_file.split('_')[0]\n",
    "            features['id'].append(id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0d09a92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrs/4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2a95b312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "38d8d897",
   "metadata": {},
   "outputs": [],
   "source": [
    "            if use_qrs :\n",
    "                ____, recording1 = sp.io.wavfile.read(filename)\n",
    "                R_peaks, S_point, Q_point=EKG_QRS_detect1(recording1, 4000, True, False)\n",
    "                qrs = (S_point-Q_point)/4000\n",
    "            else :\n",
    "                qrs = np.zeros((1))\n",
    "            features['qrs1'].append(qrs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e712b017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "217ffd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dee1d431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6255625"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "965d1589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['9979'],\n",
       " 'age': [],\n",
       " 'sex': [],\n",
       " 'hw': [],\n",
       " 'preg': [],\n",
       " 'loc': [],\n",
       " 'mel1': [array([[-6.89922784e+00,  1.90494826e-02, -1.23030720e+01, ...,\n",
       "           6.09699964e-02, -1.09735494e+01, -1.72198673e+01],\n",
       "         [ 1.78912823e+00,  7.82302352e+00, -1.27943020e+01, ...,\n",
       "           7.50267197e+00, -6.79463069e+00, -1.58918579e+01],\n",
       "         [ 5.38332830e+00,  7.76012088e+00, -9.81395234e+00, ...,\n",
       "           8.51253609e+00, -9.02300278e+00, -1.47872245e+01],\n",
       "         ...,\n",
       "         [-3.46921457e+01, -4.77105074e+01, -4.81707076e+01, ...,\n",
       "          -5.27193494e+01, -5.85358574e+01, -4.67985346e+01],\n",
       "         [-3.62466147e+01, -4.40756167e+01, -4.61119167e+01, ...,\n",
       "          -5.56958625e+01, -5.73739322e+01, -4.69605721e+01],\n",
       "         [-3.54969086e+01, -4.60599571e+01, -5.33656002e+01, ...,\n",
       "          -5.90770233e+01, -6.06032706e+01, -4.73134410e+01]])],\n",
       " 'cqt1': [],\n",
       " 'stft1': [],\n",
       " 'raw1': [],\n",
       " 'rr1': [0.6255625, 0.6255625],\n",
       " 'qrs1': [array([ 6,  4,  4,  6, 14,  5,  5,  5, 26, 21,  5,  4,  1, 11,  7,  4,  7,\n",
       "          7,  2,  4,  4, 10, 30, 19,  1, 14,  5, 25, 29, 24,  4, 11,  6, 20,\n",
       "         10, 12,  5,  4, 50, 29, 26, 30, 13,  8, 28, 28, 21,  8,  7,  4,  5,\n",
       "          7, 11, 15,  6, 10,  4,  7,  8,  2, 15, 10, 28, 15,  3, 16,  3, 24,\n",
       "          9, 31,  5,  5, 15, 17, 13,  4, 17,  3,  5,  8, 13,  5, 25, 18,  3,\n",
       "         23, 15, 12,  6,  9, 23,  6, 19,  5,  4, 10,  3, 10, 18,  8,  2,  5,\n",
       "          3,  7, 26, 22, 12,  9, 20, 25, 11,  8,  3, 10,  3,  3, 19, 26,  7,\n",
       "          5,  3,  5,  7,  8, 13, 11, 18, 28, 16, 22, 28,  9,  3,  6,  3,  4,\n",
       "          5,  8, 14,  3,  3,  8,  2,  6,  3, 15,  7, 12, 27, 10,  8,  7,  3,\n",
       "         13, 18,  1,  7,  3,  5, 15, 12, 21, 13,  3, 10,  7, 15, 41, 17, 19,\n",
       "         11,  7, 14, 19,  7, 52, 26, 17, 12, 10, 15, 21,  5, 12,  9, 16,  7,\n",
       "          4, 15, 16,  8,  6,  1,  8,  2,  3,  9, 23,  3, 10, 14, 22, 22,  9,\n",
       "          4,  6, 20, 14,  3,  3,  2, 16, 12,  8, 13, 30, 49, 16,  7, 11, 19,\n",
       "          7, 20,  6,  5,  5,  8, 22,  7,  8, 30, 26, 18,  2, 25, 39, 22,  3,\n",
       "          1,  8,  8, 35, 19,  8,  3, 28,  9, 27, 21, 14, 10, 12, 30, 19,  5,\n",
       "          6,  9, 27, 28, 15,  3,  3,  5,  3,  4,  8, 22, 35, 15, 10,  5, 25,\n",
       "          4,  9,  6,  8,  7, 18,  8,  7,  3, 11, 42, 21,  7, 24,  7, 13, 24,\n",
       "          2, 23,  9,  6,  8,  6,  7, 18, 17, 25,  4, 34,  4, 27,  5, 20, 12,\n",
       "         26, 17, 24, 30,  8,  6,  6,  5,  6,  4, 13, 52, 16,  6, 15,  1,  9,\n",
       "         29, 16,  5,  8,  8,  4, 28,  4, 14,  8, 10, 26,  3,  1,  8, 12, 35,\n",
       "          3,  6, 13, 18, 25,  5,  3,  4, 19, 17,  5, 18, 12,  7, 14, 21, 13,\n",
       "         26, 24, 16,  7,  5,  1,  3,  3,  3, 10,  7, 10,  6, 14, 13, 25,  5,\n",
       "          9, 11,  8,  8,  1, 20,  6,  3,  5, 13,  7, 27, 31,  4,  2,  7, 23,\n",
       "          3, 21,  4,  9,  5,  7,  7,  3,  8,  4,  9, 10, 13, 11, 28, 22, 39,\n",
       "          7,  7,  5, 18,  8,  6, 20,  6, 33, 27, 24, 27, 12,  2, 28,  6, 13,\n",
       "          3, 12, 16, 15,  4,  3,  5,  5, 22, 11, 16,  3,  9, 11, 28,  6, 10,\n",
       "         10,  9, 15,  4,  3,  4,  5,  7, 15, 20, 10,  9,  5, 21, 23,  8,  5,\n",
       "          7, 26,  4,  5,  3, 16, 24, 13, 27, 34, 21, 19, 24,  3, 13,  3, 14,\n",
       "          5, 39, 13, 10,  4,  6,  8,  5, 14,  5,  5,  8, 12,  9,  6, 22, 24,\n",
       "         10,  6,  3,  4, 14, 24,  4,  2,  6, 14, 24, 12, 15,  6, 12, 13, 21,\n",
       "         15,  8,  7, 28, 11,  2, 10,  9, 23,  6, 20, 29,  9,  9,  6, 25, 17,\n",
       "          6,  2,  8,  6, 15,  7,  4, 31, 19, 24, 13,  8, 24, 23,  5,  3, 13,\n",
       "         15, 27,  9,  5,  3, 17, 11, 11, 16, 17, 22, 21, 26,  5, 12, 15,  5,\n",
       "         11,  8, 10,  8,  9,  5,  4, 29, 10, 23, 14,  3, 22, 10, 21,  8,  6,\n",
       "          9,  6, 13, 11,  1, 12,  4,  6,  5,  5,  9,  3,  4,  3,  3,  7,  3,\n",
       "         27,  5,  7]),\n",
       "  array([ 6,  4,  4,  6, 14,  5,  5,  5, 26, 21,  5,  4,  1, 11,  7,  4,  7,\n",
       "          7,  2,  4,  4, 10, 30, 19,  1, 14,  5, 25, 29, 24,  4, 11,  6, 20,\n",
       "         10, 12,  5,  4, 50, 29, 26, 30, 13,  8, 28, 28, 21,  8,  7,  4,  5,\n",
       "          7, 11, 15,  6, 10,  4,  7,  8,  2, 15, 10, 28, 15,  3, 16,  3, 24,\n",
       "          9, 31,  5,  5, 15, 17, 13,  4, 17,  3,  5,  8, 13,  5, 25, 18,  3,\n",
       "         23, 15, 12,  6,  9, 23,  6, 19,  5,  4, 10,  3, 10, 18,  8,  2,  5,\n",
       "          3,  7, 26, 22, 12,  9, 20, 25, 11,  8,  3, 10,  3,  3, 19, 26,  7,\n",
       "          5,  3,  5,  7,  8, 13, 11, 18, 28, 16, 22, 28,  9,  3,  6,  3,  4,\n",
       "          5,  8, 14,  3,  3,  8,  2,  6,  3, 15,  7, 12, 27, 10,  8,  7,  3,\n",
       "         13, 18,  1,  7,  3,  5, 15, 12, 21, 13,  3, 10,  7, 15, 41, 17, 19,\n",
       "         11,  7, 14, 19,  7, 52, 26, 17, 12, 10, 15, 21,  5, 12,  9, 16,  7,\n",
       "          4, 15, 16,  8,  6,  1,  8,  2,  3,  9, 23,  3, 10, 14, 22, 22,  9,\n",
       "          4,  6, 20, 14,  3,  3,  2, 16, 12,  8, 13, 30, 49, 16,  7, 11, 19,\n",
       "          7, 20,  6,  5,  5,  8, 22,  7,  8, 30, 26, 18,  2, 25, 39, 22,  3,\n",
       "          1,  8,  8, 35, 19,  8,  3, 28,  9, 27, 21, 14, 10, 12, 30, 19,  5,\n",
       "          6,  9, 27, 28, 15,  3,  3,  5,  3,  4,  8, 22, 35, 15, 10,  5, 25,\n",
       "          4,  9,  6,  8,  7, 18,  8,  7,  3, 11, 42, 21,  7, 24,  7, 13, 24,\n",
       "          2, 23,  9,  6,  8,  6,  7, 18, 17, 25,  4, 34,  4, 27,  5, 20, 12,\n",
       "         26, 17, 24, 30,  8,  6,  6,  5,  6,  4, 13, 52, 16,  6, 15,  1,  9,\n",
       "         29, 16,  5,  8,  8,  4, 28,  4, 14,  8, 10, 26,  3,  1,  8, 12, 35,\n",
       "          3,  6, 13, 18, 25,  5,  3,  4, 19, 17,  5, 18, 12,  7, 14, 21, 13,\n",
       "         26, 24, 16,  7,  5,  1,  3,  3,  3, 10,  7, 10,  6, 14, 13, 25,  5,\n",
       "          9, 11,  8,  8,  1, 20,  6,  3,  5, 13,  7, 27, 31,  4,  2,  7, 23,\n",
       "          3, 21,  4,  9,  5,  7,  7,  3,  8,  4,  9, 10, 13, 11, 28, 22, 39,\n",
       "          7,  7,  5, 18,  8,  6, 20,  6, 33, 27, 24, 27, 12,  2, 28,  6, 13,\n",
       "          3, 12, 16, 15,  4,  3,  5,  5, 22, 11, 16,  3,  9, 11, 28,  6, 10,\n",
       "         10,  9, 15,  4,  3,  4,  5,  7, 15, 20, 10,  9,  5, 21, 23,  8,  5,\n",
       "          7, 26,  4,  5,  3, 16, 24, 13, 27, 34, 21, 19, 24,  3, 13,  3, 14,\n",
       "          5, 39, 13, 10,  4,  6,  8,  5, 14,  5,  5,  8, 12,  9,  6, 22, 24,\n",
       "         10,  6,  3,  4, 14, 24,  4,  2,  6, 14, 24, 12, 15,  6, 12, 13, 21,\n",
       "         15,  8,  7, 28, 11,  2, 10,  9, 23,  6, 20, 29,  9,  9,  6, 25, 17,\n",
       "          6,  2,  8,  6, 15,  7,  4, 31, 19, 24, 13,  8, 24, 23,  5,  3, 13,\n",
       "         15, 27,  9,  5,  3, 17, 11, 11, 16, 17, 22, 21, 26,  5, 12, 15,  5,\n",
       "         11,  8, 10,  8,  9,  5,  4, 29, 10, 23, 14,  3, 22, 10, 21,  8,  6,\n",
       "          9,  6, 13, 11,  1, 12,  4,  6,  5,  5,  9,  3,  4,  3,  3,  7,  3,\n",
       "         27,  5,  7])]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b99968c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_mel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3dcd81fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_sec=20; pre_emphasis = 0; hop_length=256; win_length = 512; n_mels = 100;\n",
    "filter_scale = 1; n_bins = 80; fmin = 10; trim = 4000;\n",
    "use_mel = True; use_cqt = False; use_stft = False; use_raw = False;\n",
    "use_rr = False; use_qrs=False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f1959fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "            if use_mel :\n",
    "                mel1 = feature_extract_melspec(filename, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length, \n",
    "                                               win_length = win_length, n_mels = n_mels, trim = trim)[0]\n",
    "            else :\n",
    "                mel1 = np.zeros( (1,1,1) )\n",
    "            features['mel1'].append(mel1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d547af86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 313)\n",
      "(598,)\n"
     ]
    }
   ],
   "source": [
    "print(mel1.shape)\n",
    "print(qrs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3772aff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2519, 2698, 2699, 2648, 2574, 2596, 2680, 2561, 2562, 2527, 2550,\n",
       "       2489, 2536, 2476, 2483, 2459, 2449, 2443, 2422, 2436, 2417, 2453,\n",
       "       2424, 2455, 2459, 2431, 2455, 2420, 2440, 2454, 2423, 2434])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diff(info['ECG_R_Peaks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08ba04e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "                    ____, recording1 = sp.io.wavfile.read(filename)\n",
    "                    ____, info = nk.ecg_process(recording1, sampling_rate=4000)\n",
    "                    rr = np.mean(np.diff(info['ECG_R_Peaks'])/4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7547eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5039, 5952, 5754, ...,  344,  229,   87], dtype=int16)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recording1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "97d33759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrs = np.zeros((1))\n",
    "qrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "213a6f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "            use_rr = True\n",
    "            if use_rr :\n",
    "                try:\n",
    "                    ____, recording1 = sp.io.wavfile.read(filename)\n",
    "                    ____, info = nk.ecg_process(recording1, sampling_rate=4000)\n",
    "                    rr = (np.diff(info['ECG_R_Peaks'])/4000)\n",
    "                    current_rr = rr\n",
    "                except:\n",
    "                    print(filename)\n",
    "                    current_rr=np.zeros((1))\n",
    "            else :\n",
    "                current_rr = np.zeros((1))\n",
    "            features['rr1'].append(current_rr)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fcfcb658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_rr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0173ef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_3lb_all_ord_seqrr_seqqrs(data_folder, patient_files_trn, po = .3,\n",
    "                          samp_sec=20, pre_emphasis = 0, hop_length=256, win_length = 512, n_mels = 100,\n",
    "                             filter_scale = 1, n_bins = 80, fmin = 10, trim = 4000,\n",
    "                             use_mel = True, use_cqt = False, use_stft = False, use_raw = False,\n",
    "                             use_rr = False, use_qrs=False\n",
    "                         ) :\n",
    "    features = dict()\n",
    "    features['id'] = []\n",
    "    features['age'] = []\n",
    "    features['sex'] = []\n",
    "    features['hw'] = []\n",
    "    features['preg'] = []\n",
    "    features['loc'] = []\n",
    "    features['mel1'] = []\n",
    "    features['cqt1'] = []\n",
    "    features['stft1'] = []\n",
    "    features['raw1'] = []\n",
    "    features['rr1'] = []\n",
    "    features['qrs1'] = []\n",
    "    \n",
    "    mm_labels = []\n",
    "    out_labels = []\n",
    "\n",
    "    age_classes = ['Neonate', 'Infant', 'Child', 'Adolescent', 'Young Adult']\n",
    "    recording_locations = ['AV', 'MV', 'PV', 'TV', 'PhC']\n",
    "\n",
    "    num_patient_files = len(patient_files_trn)\n",
    "\n",
    "    for i in tqdm.tqdm(range(num_patient_files)) :\n",
    "\n",
    "        # Load the current patient data and recordings.\n",
    "        current_patient_data = load_patient_data(patient_files_trn[i])\n",
    "        num_locations = get_num_locations(current_patient_data)\n",
    "        recording_information = current_patient_data.split('\\n')[1:num_locations+1]\n",
    "        for j in range(num_locations) :\n",
    "            entries = recording_information[j].split(' ')\n",
    "            recording_file = entries[2]\n",
    "            filename = os.path.join(data_folder, recording_file)\n",
    "\n",
    "            # Extract id\n",
    "            id1 = recording_file.split('_')[0]\n",
    "            features['id'].append(id1)\n",
    "\n",
    "            # Extract melspec\n",
    "            if use_mel :\n",
    "                mel1 = feature_extract_melspec(filename, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length, \n",
    "                                               win_length = win_length, n_mels = n_mels, trim = trim)[0]\n",
    "            else :\n",
    "                mel1 = np.zeros( (1,1,1) )\n",
    "            features['mel1'].append(mel1)\n",
    "\n",
    "            if use_cqt :\n",
    "                mel2 = feature_extract_cqt(filename, samp_sec=samp_sec, pre_emphasis = pre_emphasis, filter_scale = filter_scale, \n",
    "                                           n_bins = n_bins, fmin = fmin, trim = trim)[0]\n",
    "            else :\n",
    "                mel2 = np.zeros( (1,1,1) )\n",
    "            features['cqt1'].append(mel2)\n",
    "\n",
    "            if use_stft :\n",
    "                mel3 = feature_extract_stft(filename, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length, \n",
    "                                            win_length = win_length, trim = trim)[0]\n",
    "            else :\n",
    "                mel3 = np.zeros( (1,1,1) )\n",
    "            features['stft1'].append(mel3)\n",
    "\n",
    "            if use_raw :\n",
    "                frequency1, recording1 = sp.io.wavfile.read(filename)\n",
    "            else :\n",
    "                recording1 = np.zeros((1))\n",
    "            features['raw1'].append(recording1)\n",
    "            \n",
    "            \n",
    "            if use_rr :\n",
    "                try:\n",
    "                    ____, recording1 = sp.io.wavfile.read(filename)\n",
    "                    ____, info = nk.ecg_process(recording1, sampling_rate=4000)\n",
    "                    rr = (np.diff(info['ECG_R_Peaks'])/4000)\n",
    "                    current_rr = rr\n",
    "                except:\n",
    "                    print(filename)\n",
    "                    current_rr=np.zeros((1))\n",
    "            else :\n",
    "                current_rr = np.zeros((1))\n",
    "            features['rr1'].append(current_rr)        \n",
    "            \n",
    "            \n",
    "            if use_qrs :\n",
    "                ____, recording1 = sp.io.wavfile.read(filename)\n",
    "                R_peaks, S_point, Q_point=EKG_QRS_detect1(recording1, 4000, True, False)\n",
    "                qrs = (S_point-Q_point)/4000\n",
    "            else :\n",
    "                qrs = np.zeros((1))\n",
    "            features['qrs1'].append(qrs)   \n",
    "            \n",
    "            \n",
    "            # Extract age_group\n",
    "            age_group = get_age(current_patient_data)\n",
    "            current_age_group = np.zeros(6, dtype=int)\n",
    "            if age_group in age_classes:\n",
    "                j = age_classes.index(age_group)\n",
    "                current_age_group[j] = 1\n",
    "            else :\n",
    "                current_age_group[5] = 1\n",
    "            features['age'].append(current_age_group)\n",
    "\n",
    "            # Extract sex\n",
    "            sex = get_sex(current_patient_data)\n",
    "            sex_features = np.zeros(2, dtype=int)\n",
    "            if compare_strings(sex, 'Female'):\n",
    "                sex_features[0] = 1\n",
    "            elif compare_strings(sex, 'Male'):\n",
    "                sex_features[1] = 1\n",
    "            features['sex'].append(sex_features)\n",
    "\n",
    "            # Extract height and weight.\n",
    "            height = get_height(current_patient_data)\n",
    "            weight = get_weight(current_patient_data)\n",
    "            ## simple impute\n",
    "            if math.isnan(height) :\n",
    "                height = 110.846\n",
    "            if math.isnan(weight) :\n",
    "                weight = 23.767\n",
    "                \n",
    "            features['hw'].append(np.array([height, weight]))\n",
    "\n",
    "            # Extract pregnancy\n",
    "            is_pregnant = get_pregnancy_status(current_patient_data)\n",
    "            features['preg'].append(is_pregnant)\n",
    "\n",
    "            # Extract location\n",
    "            locations = entries[0]\n",
    "            num_recording_locations = len(recording_locations)\n",
    "            loc_features = np.zeros(num_recording_locations)\n",
    "            if locations in recording_locations:\n",
    "                j = recording_locations.index(locations)\n",
    "                loc_features[j] = 1\n",
    "            features['loc'].append(loc_features)\n",
    "\n",
    "            # Extract labels \n",
    "            mm_label = get_murmur(current_patient_data)\n",
    "            out_label = get_outcome(current_patient_data)\n",
    "            if mm_label == 'Absent' :\n",
    "                current_mm_labels = np.array([0, 1])\n",
    "            elif mm_label == 'Unknown' :\n",
    "                current_mm_labels = np.array([po, 1-po])\n",
    "            else :\n",
    "                mm_loc = get_murmur_loc(current_patient_data)\n",
    "                if mm_loc == 'nan' :\n",
    "                    current_mm_labels = np.array([0.9, 0.1])\n",
    "                else :\n",
    "                    mm_loc = mm_loc.split('+')\n",
    "                    if locations in mm_loc :\n",
    "                        current_mm_labels = np.array([1, 0])\n",
    "                    else :\n",
    "                        current_mm_labels = np.array([0.8, 0.2])\n",
    "\n",
    "            if out_label == 'Normal' :\n",
    "                current_out_labels = np.array([0, 1])\n",
    "            else :\n",
    "                current_out_labels = np.array([1, 0])\n",
    "#                if mm_label == 'Absent' :\n",
    "#                    current_out_labels = np.array([0.8, 0.2])\n",
    "#                elif mm_label == 'unknown' :\n",
    "#                    current_out_labels = np.array([0.85, 0.15])\n",
    "#                else :\n",
    "#                    current_out_labels = np.array([1, 0])\n",
    "                \n",
    "            mm_labels.append(current_mm_labels)\n",
    "            out_labels.append(current_out_labels)\n",
    "\n",
    "    if use_mel : \n",
    "        M, N = features['mel1'][i].shape\n",
    "        for i in range(len(features['mel1'])) :\n",
    "            features['mel1'][i] = features['mel1'][i].reshape(M,N,1)\n",
    "        print(\"melspec: \", M,N)\n",
    "    else :\n",
    "        M, N, _ = features['mel1'][i].shape\n",
    "    mel_input_shape = (M,N,1)\n",
    "        \n",
    "    if use_cqt :\n",
    "        M, N = features['cqt1'][i].shape\n",
    "        for i in range(len(features['cqt1'])) :\n",
    "            features['cqt1'][i] = features['cqt1'][i].reshape(M,N,1)\n",
    "        print(\"cqt: \", M,N)\n",
    "    else :\n",
    "        M, N, _ = features['cqt1'][i].shape\n",
    "    cqt_input_shape = (M,N,1)\n",
    "\n",
    "    \n",
    "    if use_stft :\n",
    "        M, N = features['stft1'][i].shape\n",
    "        for i in range(len(features['stft1'])) :\n",
    "            features['stft1'][i] = features['stft1'][i].reshape(M,N,1)\n",
    "        print(\"stft: \", M,N)\n",
    "    else :\n",
    "        M, N, _ = features['stft1'][i].shape\n",
    "    stft_input_shape = (M,N,1)\n",
    "        \n",
    "    for k1 in features.keys() :\n",
    "        features[k1] = np.array(features[k1])\n",
    "    \n",
    "    mm_labels = np.array(mm_labels)\n",
    "    out_labels = np.array(out_labels)\n",
    "    return features, mm_labels, out_labels, mel_input_shape, cqt_input_shape, stft_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98be9d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trim = 0\n",
    "use_mel = True\n",
    "use_cqt = True #np.random.choice([True,False])\n",
    "use_stft = True #np.random.choice([True,False])\n",
    "use_rr = True\n",
    "use_qrs = True\n",
    "\n",
    "params_feature = {'samp_sec': 20,\n",
    "                  #### melspec, stft 피쳐 옵션들\n",
    "                  'pre_emphasis': 0,\n",
    "                  'hop_length': 128,\n",
    "                  'win_length':256,\n",
    "                  'n_mels': 100,\n",
    "                  #### cqt 피쳐 옵션들\n",
    "                  'filter_scale': 1,\n",
    "                  'n_bins': 80,\n",
    "                  'fmin': 10,\n",
    "                  ### 사용할 피쳐 지정\n",
    "                      'trim' : trim, # 앞뒤 얼마나 자를지? 4000 이면 1초\n",
    "                      'use_mel' : use_mel,\n",
    "                      'use_cqt' : use_cqt,\n",
    "                      'use_stft' : use_stft,\n",
    "                      'use_rr': use_rr,\n",
    "                      'use_qrs': use_qrs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6d334c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                        | 3/751 [01:33<6:35:23, 31.72s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  1%|▍                                        | 7/751 [03:42<6:56:17, 33.57s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  2%|▋                                       | 12/751 [05:31<4:54:45, 23.93s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  4%|█▍                                      | 27/751 [12:12<7:07:32, 35.43s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  5%|██▏                                     | 40/751 [21:27<8:18:20, 42.05s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  5%|██▏                                     | 41/751 [22:23<9:05:40, 46.11s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  6%|██▏                                     | 42/751 [22:51<8:00:56, 40.70s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  6%|██▌                                     | 48/751 [27:15<9:25:15, 48.24s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  7%|██▉                                     | 56/751 [33:40<7:47:33, 40.36s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  8%|███▎                                    | 63/751 [38:07<7:03:55, 36.97s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  9%|███▌                                    | 66/751 [40:20<8:01:10, 42.15s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  9%|███▌                                    | 67/751 [41:17<8:51:54, 46.66s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  9%|███▌                                    | 68/751 [41:39<7:28:27, 39.40s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 11%|████▌                                   | 85/751 [48:54<3:38:33, 19.69s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 12%|████▋                                   | 87/751 [49:44<4:04:05, 22.06s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 12%|████▋                                   | 89/751 [51:40<7:20:51, 39.96s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 12%|████▊                                   | 90/751 [52:39<8:23:03, 45.66s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 13%|█████▏                                  | 98/751 [56:51<6:17:23, 34.68s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 14%|█████                                | 104/751 [1:00:43<6:25:37, 35.76s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▎                               | 109/751 [1:03:51<7:35:23, 42.56s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 15%|█████▌                               | 114/751 [1:07:41<8:03:35, 45.55s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 15%|█████▋                               | 115/751 [1:08:12<7:16:14, 41.16s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 15%|█████▋                               | 116/751 [1:09:08<8:01:43, 45.52s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 16%|█████▊                               | 117/751 [1:09:53<8:01:31, 45.57s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 17%|██████▍                              | 131/751 [1:19:33<7:48:50, 45.37s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 18%|██████▌                              | 134/751 [1:22:33<9:44:16, 56.82s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 19%|██████▉                              | 140/751 [1:26:05<6:14:08, 36.74s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 20%|███████▎                             | 148/751 [1:30:27<5:13:25, 31.19s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 20%|███████▍                             | 152/751 [1:32:19<4:39:47, 28.03s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 21%|███████▌                             | 154/751 [1:33:26<5:19:19, 32.09s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 21%|███████▋                             | 155/751 [1:34:16<6:13:08, 37.56s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 22%|████████▏                            | 166/751 [1:39:31<4:27:43, 27.46s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 22%|████████▏                            | 167/751 [1:40:24<5:41:35, 35.10s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 23%|████████▍                            | 171/751 [1:42:51<5:36:20, 34.79s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 24%|████████▊                            | 179/751 [1:48:49<6:53:33, 43.38s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 24%|████████▉                            | 181/751 [1:49:56<5:47:30, 36.58s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 24%|█████████                            | 183/751 [1:51:37<6:46:36, 42.95s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 25%|█████████▏                           | 186/751 [1:53:01<5:22:31, 34.25s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▍                           | 191/751 [1:56:42<6:14:24, 40.12s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 26%|█████████▍                           | 192/751 [1:57:07<5:31:01, 35.53s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 26%|█████████▌                           | 193/751 [1:57:53<5:58:48, 38.58s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 26%|█████████▌                           | 194/751 [1:58:45<6:36:54, 42.76s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 26%|█████████▋                           | 197/751 [2:00:58<6:30:49, 42.33s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 27%|█████████▊                           | 200/751 [2:03:13<6:39:49, 43.54s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 27%|██████████                           | 203/751 [2:04:47<5:15:21, 34.53s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 28%|██████████▍                          | 211/751 [2:10:02<4:58:27, 33.16s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 28%|██████████▍                          | 213/751 [2:11:32<5:55:39, 39.66s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 28%|██████████▌                          | 214/751 [2:12:26<6:32:10, 43.82s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 29%|██████████▊                          | 219/751 [2:16:17<7:32:34, 51.04s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 30%|███████████                          | 225/751 [2:21:02<6:53:20, 47.15s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 30%|███████████▏                         | 227/751 [2:22:32<6:28:01, 44.43s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 30%|███████████▏                         | 228/751 [2:23:10<6:09:05, 42.34s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 30%|███████████▎                         | 229/751 [2:24:24<5:29:09, 37.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m features_trn, mm_lbs_trn, out_lbs_trn, mel_input_shape, cqt_input_shape, stft_input_shape \u001b[38;5;241m=\u001b[39m \u001b[43mget_features_3lb_all_ord_seqrr_seqqrs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_files_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams_feature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m features_test, mm_lbs_test, out_lbs_test, _, _, _ \u001b[38;5;241m=\u001b[39m get_features_3lb_all_ord_seqrr_seqqrs(test_folder, patient_files_test, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams_feature)\n",
      "Input \u001b[0;32mIn [64]\u001b[0m, in \u001b[0;36mget_features_3lb_all_ord_seqrr_seqqrs\u001b[0;34m(data_folder, patient_files_trn, po, samp_sec, pre_emphasis, hop_length, win_length, n_mels, filter_scale, n_bins, fmin, trim, use_mel, use_cqt, use_stft, use_raw, use_rr, use_qrs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_qrs :\n\u001b[1;32m     88\u001b[0m     ____, recording1 \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mwavfile\u001b[38;5;241m.\u001b[39mread(filename)\n\u001b[0;32m---> 89\u001b[0m     R_peaks, S_point, Q_point\u001b[38;5;241m=\u001b[39m\u001b[43mEKG_QRS_detect1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecording1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     qrs \u001b[38;5;241m=\u001b[39m (S_point\u001b[38;5;241m-\u001b[39mQ_point)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4000\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mEKG_QRS_detect1\u001b[0;34m(ecg, fs, QS, plot)\u001b[0m\n\u001b[1;32m    108\u001b[0m ws\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(fs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m800\u001b[39m)\n\u001b[1;32m    109\u001b[0m ecg_integrate\u001b[38;5;241m=\u001b[39mintegrate(ecg_integrate, ws)\n\u001b[0;32m--> 111\u001b[0m peaks\u001b[38;5;241m=\u001b[39m\u001b[43mfind_peak\u001b[49m\u001b[43m(\u001b[49m\u001b[43mecg_integrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m111\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m R_peaks\u001b[38;5;241m=\u001b[39mfind_R_peaks(ecg, peaks, \u001b[38;5;28mint\u001b[39m(fs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m444\u001b[39m))\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m QS:\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mfind_peak\u001b[0;34m(data, ws)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(lgth\u001b[38;5;241m-\u001b[39mws\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     40\u001b[0m \ttemp\u001b[38;5;241m=\u001b[39mdata[i:i\u001b[38;5;241m+\u001b[39mws]\n\u001b[0;32m---> 41\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m     42\u001b[0m \t\t\u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \tindex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m((ws\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvar\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3757\u001b[0m, in \u001b[0;36mvar\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m   3754\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3755\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m var(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, ddof\u001b[38;5;241m=\u001b[39mddof, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3757\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3758\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:212\u001b[0m, in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    207\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Compute the mean.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Note that if dtype is not of inexact type then arraymean will\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# not be either.\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m arrmean \u001b[38;5;241m=\u001b[39m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# The shape of rcount has to match arrmean to not change the shape of out\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# in broadcasting. Otherwise, it cannot be stored back to arrmean.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rcount\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# fast-path for default case when where is True\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features_trn, mm_lbs_trn, out_lbs_trn, mel_input_shape, cqt_input_shape, stft_input_shape = get_features_3lb_all_ord_seqrr_seqqrs(train_folder, patient_files_trn, **params_feature)\n",
    "features_test, mm_lbs_test, out_lbs_test, _, _, _ = get_features_3lb_all_ord_seqrr_seqqrs(test_folder, patient_files_test, **params_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a148e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/Data2/hmd/hmd_sy/notebooks/features_trn_rr_qrs2.pkl','rb') as f:\n",
    "    features_trn = pickle.load(f)\n",
    "    \n",
    "with open('/Data2/hmd/hmd_sy/notebooks/mm_lbs_trn_rr_qrs2.pkl','rb') as f:\n",
    "    mm_lbs_trn = pickle.load(f)\n",
    "    \n",
    "with open('/Data2/hmd/hmd_sy/notebooks/out_lbs_trn_rr_qrs2.pkl','rb') as f:\n",
    "    out_lbs_trn = pickle.load(f)\n",
    "    \n",
    "with open('/Data2/hmd/hmd_sy/notebooks/mel_input_shape_rr_qrs2.pkl','rb') as f:\n",
    "    mel_input_shape = pickle.load(f)\n",
    "\n",
    "with open('/Data2/hmd/hmd_sy/notebooks/cqt_input_shape_rr_qrs2.pkl','rb') as f:\n",
    "    cqt_input_shape = pickle.load(f)\n",
    "    \n",
    "with open('/Data2/hmd/hmd_sy/notebooks/stft_input_shape_rr_qrs2.pkl','rb') as f:\n",
    "    stft_input_shape = pickle.load(f)\n",
    "    \n",
    "with open('/Data2/hmd/hmd_sy/notebooks/features_test_rr_qrs2.pkl','rb') as f:\n",
    "    features_test = pickle.load(f)\n",
    "    \n",
    "with open('/Data2/hmd/hmd_sy/notebooks/mm_lbs_test_rr_qrs2.pkl','rb') as f:\n",
    "    mm_lbs_test = pickle.load(f)\n",
    "\n",
    "with open('/Data2/hmd/hmd_sy/notebooks/out_lbs_test_rr_qrs2.pkl','rb') as f:\n",
    "    out_lbs_test = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aa5d2d",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d361916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LCNN_o_5_dr_qrs2_1_1(mel_input_shape, cqt_input_shape, stft_input_shape, use_mel = True, use_cqt = True, \n",
    "                        use_stft = True, ord1 = True, dp = .5, fc = False, ext = False):\n",
    "        # Create a towy model.\n",
    "    age = keras.Input(shape=(6,), name = 'age_cat')\n",
    "    sex = keras.Input(shape=(2,), name = 'sex_cat')\n",
    "    hw = keras.Input(shape=(2,), name = 'height_weight')\n",
    "    preg = keras.Input(shape=(1,), name = 'is_preg')\n",
    "    loc = keras.Input(shape=(5,), name = 'loc')\n",
    "    rr = keras.Input(shape=(1,), name = 'rr')\n",
    "    qrs = keras.Input(shape=(1,), name = 'qrs') \n",
    "    mel1 = keras.Input(shape=mel_input_shape, name = 'mel')\n",
    "    cqt1 = keras.Input(shape=cqt_input_shape, name = 'cqt')\n",
    "    stft1 = keras.Input(shape=stft_input_shape, name = 'stft')\n",
    "    \n",
    "    \n",
    "    ## age embeddig\n",
    "    age1 = layers.Dense(2, activation = None)(age)\n",
    "\n",
    "    ## sex embedding\n",
    "    sex1 = layers.Dense(1, activation = None)(sex)\n",
    "\n",
    "    ## hw embedding\n",
    "    hw1 = layers.Dense(1, activation = None)(hw)\n",
    "\n",
    "    ## loc embedding\n",
    "    loc1 = layers.Dense(3, activation = None)(loc)\n",
    "    \n",
    "    ## rr interval embedding\n",
    "    \n",
    "    rr1 = layers.Dense(10, activation = \"relu\")(rr)\n",
    "    rr1 = BatchNormalization()(rr1)\n",
    "    rr1 = layers.Dense(10, activation = \"relu\")(rr1)\n",
    "    rr1 = BatchNormalization()(rr1)\n",
    "    rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "    rr1 = BatchNormalization()(rr1)\n",
    "    rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "    rr1 = BatchNormalization()(rr1)\n",
    "    rr1 = layers.Dense(20, activation = None)(rr1)\n",
    "    rr1 = BatchNormalization()(rr1)\n",
    "\n",
    "    ## qrs interval embedding    \n",
    "    qrs1 = layers.Dense(10, activation = \"relu\")(qrs)\n",
    "    qrs1 = BatchNormalization()(qrs1)\n",
    "    qrs1 = layers.Dense(10, activation = \"relu\")(qrs1)\n",
    "    qrs1 = BatchNormalization()(qrs1)\n",
    "    qrs1 = layers.Dense(20, activation = \"relu\")(qrs1)\n",
    "    qrs1 = BatchNormalization()(qrs1)\n",
    "    qrs1 = layers.Dense(20, activation = \"relu\")(qrs1)\n",
    "    qrs1 = BatchNormalization()(qrs1)\n",
    "    qrs1 = layers.Dense(20, activation = None)(qrs1)\n",
    "    qrs1 = BatchNormalization()(qrs1)\n",
    "\n",
    "    ## mel embedding\n",
    "    if use_mel :\n",
    "        \n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        mel2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        mel2 = Dropout(dp)(mel2)\n",
    "\n",
    "    if use_cqt :\n",
    "        \n",
    "        ## cqt embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        \n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm27)\n",
    "        cqt2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            cqt2 = Dropout(dp)(cqt2)\n",
    "            \n",
    "    if use_stft :\n",
    "        ## stft embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        \n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "        \n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        stft2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        stft2 = Dropout(dp)(stft2)\n",
    "    \n",
    "    \n",
    "    if use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2, stft2])\n",
    "    if not use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([cqt2, stft2])\n",
    "    if use_mel and not use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, stft2])\n",
    "    if use_mel and use_cqt and not use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2])\n",
    "    if not use_mel and not use_cqt and use_stft :  ## stft 만\n",
    "        concat2 = stft2\n",
    "    if use_mel and not use_cqt and not use_stft :  ### mel만\n",
    "        concat2 = mel2\n",
    "    if not use_mel and use_cqt and not use_stft :  ### cqt만\n",
    "        concat2 = cqt2\n",
    "\n",
    "    if ext :\n",
    "        concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg])\n",
    "        d1 = layers.Dense(5, activation = 'relu')(concat1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1,rr1,qrs1]) # rr1,qrs1 따로 concat\n",
    "        \n",
    "    if fc :\n",
    "        concat2 = layers.Dense(10, activation = \"relu\")(concat2)\n",
    "        concat2 = Dropout(dp)(concat2)\n",
    "        \n",
    "    if ord1 :\n",
    "        res1 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "    else :\n",
    "        res1 = layers.Dense(3, activation = \"softmax\")(concat2)\n",
    "\n",
    "        \n",
    "#     res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "\n",
    "    model = keras.Model(inputs = [age,sex,hw,preg,loc,rr,qrs, mel1,cqt1, stft1] , outputs = res1 )\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy','AUC'])\n",
    "    return(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_LCNN_o_5_dr_qrs2_1_2(mel_input_shape, cqt_input_shape, stft_input_shape, use_mel = True, use_cqt = True, \n",
    "                        use_stft = True, ord1 = True, dp = .5, fc = False, ext = False):\n",
    "        # Create a towy model.\n",
    "    age = keras.Input(shape=(6,), name = 'age_cat')\n",
    "    sex = keras.Input(shape=(2,), name = 'sex_cat')\n",
    "    hw = keras.Input(shape=(2,), name = 'height_weight')\n",
    "    preg = keras.Input(shape=(1,), name = 'is_preg')\n",
    "    loc = keras.Input(shape=(5,), name = 'loc')\n",
    "    rr = keras.Input(shape=(1,), name = 'rr')\n",
    "    qrs = keras.Input(shape=(1,), name = 'qrs') \n",
    "    mel1 = keras.Input(shape=mel_input_shape, name = 'mel')\n",
    "    cqt1 = keras.Input(shape=cqt_input_shape, name = 'cqt')\n",
    "    stft1 = keras.Input(shape=stft_input_shape, name = 'stft')\n",
    "    \n",
    "    \n",
    "    ## age embeddig\n",
    "    age1 = layers.Dense(2, activation = None)(age)\n",
    "\n",
    "    ## sex embedding\n",
    "    sex1 = layers.Dense(1, activation = None)(sex)\n",
    "\n",
    "    ## hw embedding\n",
    "    hw1 = layers.Dense(1, activation = None)(hw)\n",
    "\n",
    "    ## loc embedding\n",
    "    loc1 = layers.Dense(3, activation = None)(loc)\n",
    "    \n",
    "    ## rr interval embedding\n",
    "    \n",
    "    rr1 = layers.Dense(10, activation = \"relu\")(rr)\n",
    "    rr1 = BatchNormalization()(rr1)\n",
    "    rr1 = layers.Dense(10, activation = \"relu\")(rr1)\n",
    "    rr1 = BatchNormalization()(rr1)\n",
    "    rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "    rr1 = BatchNormalization()(rr1)\n",
    "    rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "    rr1 = BatchNormalization()(rr1)\n",
    "    rr1 = layers.Dense(20, activation = None)(rr1)\n",
    "    rr1 = BatchNormalization()(rr1)\n",
    "\n",
    "    ## qrs interval embedding    \n",
    "    qrs1 = layers.Dense(10, activation = \"relu\")(qrs)\n",
    "    qrs1 = BatchNormalization()(qrs1)\n",
    "    qrs1 = layers.Dense(10, activation = \"relu\")(qrs1)\n",
    "    qrs1 = BatchNormalization()(qrs1)\n",
    "    qrs1 = layers.Dense(20, activation = \"relu\")(qrs1)\n",
    "    qrs1 = BatchNormalization()(qrs1)\n",
    "    qrs1 = layers.Dense(20, activation = \"relu\")(qrs1)\n",
    "    qrs1 = BatchNormalization()(qrs1)\n",
    "    qrs1 = layers.Dense(20, activation = None)(qrs1)\n",
    "    qrs1 = BatchNormalization()(qrs1)\n",
    "\n",
    "    ## mel embedding\n",
    "    if use_mel :\n",
    "        \n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        mel2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        mel2 = Dropout(dp)(mel2)\n",
    "\n",
    "    if use_cqt :\n",
    "        \n",
    "        ## cqt embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        \n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm27)\n",
    "        cqt2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            cqt2 = Dropout(dp)(cqt2)\n",
    "            \n",
    "    if use_stft :\n",
    "        ## stft embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        \n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "        \n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        stft2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        stft2 = Dropout(dp)(stft2)\n",
    "    \n",
    "    \n",
    "    if use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2, stft2])\n",
    "    if not use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([cqt2, stft2])\n",
    "    if use_mel and not use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, stft2])\n",
    "    if use_mel and use_cqt and not use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2])\n",
    "    if not use_mel and not use_cqt and use_stft :  ## stft 만\n",
    "        concat2 = stft2\n",
    "    if use_mel and not use_cqt and not use_stft :  ### mel만\n",
    "        concat2 = mel2\n",
    "    if not use_mel and use_cqt and not use_stft :  ### cqt만\n",
    "        concat2 = cqt2\n",
    "\n",
    "    if ext :\n",
    "        concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg,rr1,qrs1])\n",
    "        d1 = layers.Dense(5, activation = 'relu')(concat1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1]) # rr1,qrs1 따로 concat\n",
    "        \n",
    "    if fc :\n",
    "        concat2 = layers.Dense(10, activation = \"relu\")(concat2)\n",
    "        concat2 = Dropout(dp)(concat2)\n",
    "        \n",
    "    if ord1 :\n",
    "        res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "    else :\n",
    "        res2 = layers.Dense(3, activation = \"softmax\")(concat2)\n",
    "\n",
    "        \n",
    "#     res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "\n",
    "    model = keras.Model(inputs = [age,sex,hw,preg,loc,rr,qrs, mel1,cqt1, stft1] , outputs = res2 )\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy','AUC'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b799b088",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3507047986.py, line 43)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [145]\u001b[0;36m\u001b[0m\n\u001b[0;31m    rr1 = layers.Dense(20, activation = \"relu\")(rr1)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_LCNN_o_5_dr_qrs3_1(mel_input_shape, cqt_input_shape, stft_input_shape, use_mel = True, use_cqt = True, \n",
    "                        use_stft = True, ord1 = True, dp = .5, fc = False, ext = False):\n",
    "        # Create a towy model.\n",
    "    age = keras.Input(shape=(6,), name = 'age_cat')\n",
    "    sex = keras.Input(shape=(2,), name = 'sex_cat')\n",
    "    hw = keras.Input(shape=(2,), name = 'height_weight')\n",
    "    preg = keras.Input(shape=(1,), name = 'is_preg')\n",
    "    loc = keras.Input(shape=(5,), name = 'loc')\n",
    "    rr = keras.Input(shape=(1,), name = 'rr')\n",
    "    qrs = keras.Input(shape=(1,), name = 'qrs') \n",
    "    mel1 = keras.Input(shape=mel_input_shape, name = 'mel')\n",
    "    cqt1 = keras.Input(shape=cqt_input_shape, name = 'cqt')\n",
    "    stft1 = keras.Input(shape=stft_input_shape, name = 'stft')\n",
    "    \n",
    "    \n",
    "    ## age embeddig\n",
    "    age1 = layers.Dense(2, activation = None)(age)\n",
    "\n",
    "    ## sex embedding\n",
    "    sex1 = layers.Dense(1, activation = None)(sex)\n",
    "\n",
    "    ## hw embedding\n",
    "    hw1 = layers.Dense(1, activation = None)(hw)\n",
    "\n",
    "    ## loc embedding\n",
    "    loc1 = layers.Dense(3, activation = None)(loc)\n",
    "    \n",
    "    ## rr interval embedding\n",
    "    \n",
    "#     rr1 = layers.Dense(20, activation = \"relu\")(rr)\n",
    "#     rr1 = BatchNormalization()(rr1)\n",
    "#     rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "#     rr1 = BatchNormalization()(rr1)\n",
    "#     rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "#     rr1 = BatchNormalization()(rr1)\n",
    "#     rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "#     rr1 = BatchNormalization()(rr1)\n",
    "\n",
    "\n",
    "    \n",
    "    rr = rr.reshape(rr.shape[0],1,1)\n",
    "    rr1 = tf.keras.layers.LSTM(units=5, activation='relu',return_sequences=True)(rr) \n",
    "    rr1 = tf.keras.layers.LSTM(units=4, activation='relu')(rr1\n",
    "    rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "    rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "    rr1 = tf.keras.layers.RepeatVector(4)(rr1)\n",
    "    rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "    rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "    rr1 = Bidirectional(LSTM(4), merge_mode = 'concat')(rr1)\n",
    "    rr1 = Bidirectional(LSTM(5), merge_mode = 'concat')(rr1)\n",
    "    rr1 = TimeDistributed(Dense(6, activation=('relu')))(rr1)\n",
    "    rr1 = BatchNormalization()(rr1)\n",
    "    rr1 = TimeDistributed(Dense(7, activation=('relu')))(rr1)\n",
    "    rr1 = TimeDistributed(3)(rr1)\n",
    "    rr1 = BatchNormalization()(rr1)\n",
    "    rr1 = TimeDistributed(Dense(8, activation=('relu')))(rr1)\n",
    "    rr1 = BatchNormalization()(rr1)\n",
    "    rr1 = TimeDistributed(Dense(3))(rr1)\n",
    "    \n",
    "    ## qrs interval embedding\n",
    "    qrs1 = layers.Dense(20, activation = \"relu\")(qrs)\n",
    "    qrs1 = tf.keras.layers.LSTM(units=5, activation='relu',return_sequences=True)(qrs1) \n",
    "    qrs1 = tf.keras.layers.LSTM(units=4, activation='relu')(qrs1) \n",
    "    qrs1 = layers.Dense(20, activation = \"relu\")(qrs1)\n",
    "    qrs1 = layers.Dense(20, activation = \"relu\")(qrs1)\n",
    "    qrs1 = tf.keras.layers.RepeatVector(4)(qrs1)\n",
    "    qrs1 = layers.Dense(20, activation = \"relu\")(qrs1)\n",
    "    qrs1 = layers.Dense(20, activation = \"relu\")(qrs1)\n",
    "    qrs1 = Bidirectional(LSTM(4), merge_mode = 'concat')(qrs1)\n",
    "    qrs1 = Bidirectional(LSTM(5), merge_mode = 'concat')(qrs1)\n",
    "    qrs1 = TimeDistributed(Dense(6, activation=('relu')))(qrs1)\n",
    "    qrs1 = BatchNormalization()(qrs1)\n",
    "    qrs1 = TimeDistributed(Dense(7, activation=('relu')))(qrs1)\n",
    "    qrs1 = TimeDistributed(3)(qrs1)\n",
    "    qrs1 = BatchNormalization()(qrs1)\n",
    "    qrs1 = TimeDistributed(Dense(8, activation=('relu')))(qrs1)\n",
    "    qrs1 = BatchNormalization()(qrs1)\n",
    "    qrs1 = TimeDistributed(Dense(3))(qrs1)\n",
    "\n",
    "\n",
    "    ## mel embedding\n",
    "    if use_mel :\n",
    "        \n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        mel2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        mel2 = Dropout(dp)(mel2)\n",
    "\n",
    "    if use_cqt :\n",
    "        \n",
    "        ## cqt embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        \n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm27)\n",
    "        cqt2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            cqt2 = Dropout(dp)(cqt2)\n",
    "            \n",
    "    if use_stft :\n",
    "        ## stft embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        \n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "        \n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        stft2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        stft2 = Dropout(dp)(stft2)\n",
    "    \n",
    "    \n",
    "    if use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2, stft2])\n",
    "    if not use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([cqt2, stft2])\n",
    "    if use_mel and not use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, stft2])\n",
    "    if use_mel and use_cqt and not use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2])\n",
    "    if not use_mel and not use_cqt and use_stft :  ## stft 만\n",
    "        concat2 = stft2\n",
    "    if use_mel and not use_cqt and not use_stft :  ### mel만\n",
    "        concat2 = mel2\n",
    "    if not use_mel and use_cqt and not use_stft :  ### cqt만\n",
    "        concat2 = cqt2\n",
    "\n",
    "    if ext :\n",
    "        concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg])\n",
    "        d1 = layers.Dense(5, activation = 'relu')(concat1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1,rr1,qrs1]) # rr1,qrs1 따로 concat\n",
    "        \n",
    "    if fc :\n",
    "        concat2 = layers.Dense(10, activation = \"relu\")(concat2)\n",
    "        concat2 = Dropout(dp)(concat2)\n",
    "        \n",
    "    if ord1 :\n",
    "        res1 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "    else :\n",
    "        res1 = layers.Dense(3, activation = \"softmax\")(concat2)\n",
    "\n",
    "        \n",
    "#     res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "\n",
    "    model = keras.Model(inputs = [age,sex,hw,preg,loc,rr,qrs, mel1,cqt1, stft1] , outputs = res1 )\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy','AUC'])\n",
    "    return(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_LCNN_o_5_dr_qrs3_2(mel_input_shape, cqt_input_shape, stft_input_shape, use_mel = True, use_cqt = True, \n",
    "                        use_stft = True, ord1 = True, dp = .5, fc = False, ext = False):\n",
    "        # Create a towy model.\n",
    "    age = keras.Input(shape=(6,), name = 'age_cat')\n",
    "    sex = keras.Input(shape=(2,), name = 'sex_cat')\n",
    "    hw = keras.Input(shape=(2,), name = 'height_weight')\n",
    "    preg = keras.Input(shape=(1,), name = 'is_preg')\n",
    "    loc = keras.Input(shape=(5,), name = 'loc')\n",
    "    rr = keras.Input(shape=(3,), name = 'rr')\n",
    "    qrs = keras.Input(shape=(3,), name = 'qrs') \n",
    "    mel1 = keras.Input(shape=mel_input_shape, name = 'mel')\n",
    "    cqt1 = keras.Input(shape=cqt_input_shape, name = 'cqt')\n",
    "    stft1 = keras.Input(shape=stft_input_shape, name = 'stft')\n",
    "    \n",
    "    \n",
    "    ## age embeddig\n",
    "    age1 = layers.Dense(2, activation = None)(age)\n",
    "\n",
    "    ## sex embedding\n",
    "    sex1 = layers.Dense(1, activation = None)(sex)\n",
    "\n",
    "    ## hw embedding\n",
    "    hw1 = layers.Dense(1, activation = None)(hw)\n",
    "\n",
    "    ## loc embedding\n",
    "    loc1 = layers.Dense(3, activation = None)(loc)\n",
    "    \n",
    "#     ## rr interval embedding\n",
    "    \n",
    "#     rr1 = layers.Dense(20, activation = \"relu\")(rr)\n",
    "#     rr1 = BatchNormalization()(rr1)\n",
    "#     rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "#     rr1 = BatchNormalization()(rr1)\n",
    "#     rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "#     rr1 = BatchNormalization()(rr1)\n",
    "#     rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "#     rr1 = BatchNormalization()(rr1)\n",
    "\n",
    "#     ## qrs interval embedding    \n",
    "#     qrs1 = layers.Dense(20, activation = \"relu\")(qrs)\n",
    "#     qrs1 = BatchNormalization()(qrs1)\n",
    "#     qrs1 = layers.Dense(20, activation = \"relu\")(qrs1)\n",
    "#     qrs1 = BatchNormalization()(qrs1)\n",
    "#     qrs1 = layers.Dense(20, activation = \"relu\")(qrs1)\n",
    "#     qrs1 = BatchNormalization()(qrs1)\n",
    "#     qrs1 = layers.Dense(20, activation = \"relu\")(qrs1)\n",
    "#     qrs1 = BatchNormalization()(qrs1)\n",
    "\n",
    "    rr = rr.reshape(rr.shape[0],1,1)\n",
    "    rr1 = tf.keras.layers.LSTM(units=5, input_shape = (rr.shape[0], rr.shape[1]),activation='relu')(rr) \n",
    "    rr1 = tf.keras.layers.LSTM(units=4, activation='relu')(rr1) \n",
    "    rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "    rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "    rr1 = tf.keras.layers.RepeatVector(4)(rr1)\n",
    "    rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "    rr1 = layers.Dense(20, activation = \"relu\")(rr1)\n",
    "    rr1 = Bidirectional(LSTM(4), merge_mode = 'concat')(rr1)\n",
    "    rr1 = Bidirectional(LSTM(5), merge_mode = 'concat')(rr1)\n",
    "    rr1 = TimeDistributed(Dense(6, activation=('relu')))(rr1)\n",
    "    rr1 = BatchNormalization()(rr1)\n",
    "    rr1 = TimeDistributed(Dense(7, activation=('relu')))(rr1)\n",
    "    rr1 = TimeDistributed(3)(rr1)\n",
    "    rr1 = BatchNormalization()(rr1)\n",
    "    rr1 = TimeDistributed(Dense(8, activation=('relu')))(rr1)\n",
    "    rr1 = BatchNormalization()(rr1)\n",
    "    rr1 = TimeDistributed(Dense(3))(rr1)\n",
    "    \n",
    "    ## qrs interval embedding\n",
    "    qrs1 = layers.Dense(20, activation = \"relu\")(qrs)\n",
    "    qrs1 = tf.keras.layers.LSTM(units=5, activation='relu')(qrs1) \n",
    "    qrs1 = tf.keras.layers.LSTM(units=4, activation='relu')(qrs1) \n",
    "    qrs1 = layers.Dense(20, activation = \"relu\")(qrs1)\n",
    "    qrs1 = layers.Dense(20, activation = \"relu\")(qrs1)\n",
    "    qrs1 = tf.keras.layers.RepeatVector(4)(qrs1)\n",
    "    qrs1 = layers.Dense(20, activation = \"relu\")(qrs1)\n",
    "    qrs1 = layers.Dense(20, activation = \"relu\")(qrs1)\n",
    "    qrs1 = Bidirectional(LSTM(4), merge_mode = 'concat')(qrs1)\n",
    "    qrs1 = Bidirectional(LSTM(5), merge_mode = 'concat')(qrs1)\n",
    "    qrs1 = TimeDistributed(Dense(6, activation=('relu')))(qrs1)\n",
    "    qrs1 = BatchNormalization()(qrs1)\n",
    "    qrs1 = TimeDistributed(Dense(7, activation=('relu')))(qrs1)\n",
    "    qrs1 = TimeDistributed(3)(qrs1)\n",
    "    qrs1 = BatchNormalization()(qrs1)\n",
    "    qrs1 = TimeDistributed(Dense(8, activation=('relu')))(qrs1)\n",
    "    qrs1 = BatchNormalization()(qrs1)\n",
    "    qrs1 = TimeDistributed(Dense(3))(qrs1)\n",
    "    \n",
    "    \n",
    "    ## mel embedding\n",
    "    if use_mel :\n",
    "        \n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        mel2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        mel2 = Dropout(dp)(mel2)\n",
    "\n",
    "    if use_cqt :\n",
    "        \n",
    "        ## cqt embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        \n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm27)\n",
    "        cqt2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            cqt2 = Dropout(dp)(cqt2)\n",
    "            \n",
    "    if use_stft :\n",
    "        ## stft embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        \n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "        \n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        stft2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        stft2 = Dropout(dp)(stft2)\n",
    "    \n",
    "    \n",
    "    if use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2, stft2])\n",
    "    if not use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([cqt2, stft2])\n",
    "    if use_mel and not use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, stft2])\n",
    "    if use_mel and use_cqt and not use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2])\n",
    "    if not use_mel and not use_cqt and use_stft :  ## stft 만\n",
    "        concat2 = stft2\n",
    "    if use_mel and not use_cqt and not use_stft :  ### mel만\n",
    "        concat2 = mel2\n",
    "    if not use_mel and use_cqt and not use_stft :  ### cqt만\n",
    "        concat2 = cqt2\n",
    "\n",
    "    if ext :\n",
    "        concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg,rr1,qrs1])\n",
    "        d1 = layers.Dense(5, activation = 'relu')(concat1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1]) # rr1,qrs1 따로 concat\n",
    "        \n",
    "    if fc :\n",
    "        concat2 = layers.Dense(10, activation = \"relu\")(concat2)\n",
    "        concat2 = Dropout(dp)(concat2)\n",
    "        \n",
    "    if ord1 :\n",
    "        res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "    else :\n",
    "        res2 = layers.Dense(3, activation = \"softmax\")(concat2)\n",
    "\n",
    "        \n",
    "#     res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "\n",
    "    model = keras.Model(inputs = [age,sex,hw,preg,loc,rr,qrs, mel1,cqt1, stft1] , outputs = res2 )\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy','AUC'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4971f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01d6039a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 11:25:21.545671: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-31 11:25:22.200071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46717 MB memory:  -> device: 4, name: Quadro RTX 8000, pci bus id: 0000:41:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "use_mel = True\n",
    "use_cqt = True\n",
    "use_stft = True\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model1 = get_LCNN_o_5_dr_qrs2_1_1(mel_input_shape, cqt_input_shape, stft_input_shape, use_mel = use_mel, use_cqt = use_cqt, use_stft = use_stft)\n",
    "model2 = get_LCNN_o_5_dr_qrs2_1_2(mel_input_shape, cqt_input_shape, stft_input_shape, use_mel = use_mel, use_cqt = use_cqt, use_stft = use_stft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6a5e9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "mel (InputLayer)                [(None, 100, 626, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cqt (InputLayer)                [(None, 80, 157, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "stft (InputLayer)               [(None, 129, 626, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 100, 626, 32) 832         mel[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 100, 626, 32) 832         mel[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 80, 157, 32)  832         cqt[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 80, 157, 32)  832         cqt[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 129, 626, 32) 832         stft[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 129, 626, 32) 832         stft[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "maximum_27 (Maximum)            (None, 100, 626, 32) 0           conv2d_54[0][0]                  \n",
      "                                                                 conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_36 (Maximum)            (None, 80, 157, 32)  0           conv2d_72[0][0]                  \n",
      "                                                                 conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_45 (Maximum)            (None, 129, 626, 32) 0           conv2d_90[0][0]                  \n",
      "                                                                 conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 50, 313, 32)  0           maximum_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D) (None, 40, 79, 32)   0           maximum_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 65, 313, 32)  0           maximum_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 50, 313, 32)  1056        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 50, 313, 32)  1056        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 40, 79, 32)   1056        max_pooling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 40, 79, 32)   1056        max_pooling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 65, 313, 32)  1056        max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 65, 313, 32)  1056        max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "maximum_28 (Maximum)            (None, 50, 313, 32)  0           conv2d_56[0][0]                  \n",
      "                                                                 conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_37 (Maximum)            (None, 40, 79, 32)   0           conv2d_74[0][0]                  \n",
      "                                                                 conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_46 (Maximum)            (None, 65, 313, 32)  0           conv2d_92[0][0]                  \n",
      "                                                                 conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 50, 313, 32)  96          maximum_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 40, 79, 32)   96          maximum_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 65, 313, 32)  96          maximum_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 50, 313, 48)  13872       batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 50, 313, 48)  13872       batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 40, 79, 48)   13872       batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 40, 79, 48)   13872       batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 65, 313, 48)  13872       batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 65, 313, 48)  13872       batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "maximum_29 (Maximum)            (None, 50, 313, 48)  0           conv2d_58[0][0]                  \n",
      "                                                                 conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_38 (Maximum)            (None, 40, 79, 48)   0           conv2d_76[0][0]                  \n",
      "                                                                 conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_47 (Maximum)            (None, 65, 313, 48)  0           conv2d_94[0][0]                  \n",
      "                                                                 conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 25, 157, 48)  0           maximum_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling2D) (None, 20, 40, 48)   0           maximum_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling2D) (None, 33, 157, 48)  0           maximum_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 25, 157, 48)  144         max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 20, 40, 48)   144         max_pooling2d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 33, 157, 48)  144         max_pooling2d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 25, 157, 48)  2352        batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 25, 157, 48)  2352        batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 20, 40, 48)   2352        batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 20, 40, 48)   2352        batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 33, 157, 48)  2352        batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 33, 157, 48)  2352        batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "maximum_30 (Maximum)            (None, 25, 157, 48)  0           conv2d_60[0][0]                  \n",
      "                                                                 conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_39 (Maximum)            (None, 20, 40, 48)   0           conv2d_78[0][0]                  \n",
      "                                                                 conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_48 (Maximum)            (None, 33, 157, 48)  0           conv2d_96[0][0]                  \n",
      "                                                                 conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 25, 157, 48)  144         maximum_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 20, 40, 48)   144         maximum_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 33, 157, 48)  144         maximum_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 25, 157, 64)  27712       batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 25, 157, 64)  27712       batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 20, 40, 64)   27712       batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 20, 40, 64)   27712       batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 33, 157, 64)  27712       batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 33, 157, 64)  27712       batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "maximum_31 (Maximum)            (None, 25, 157, 64)  0           conv2d_62[0][0]                  \n",
      "                                                                 conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_40 (Maximum)            (None, 20, 40, 64)   0           conv2d_80[0][0]                  \n",
      "                                                                 conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_49 (Maximum)            (None, 33, 157, 64)  0           conv2d_98[0][0]                  \n",
      "                                                                 conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 13, 79, 64)   0           maximum_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling2D) (None, 10, 20, 64)   0           maximum_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling2D) (None, 17, 79, 64)   0           maximum_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 13, 79, 64)   4160        max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 13, 79, 64)   4160        max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 10, 20, 64)   4160        max_pooling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 10, 20, 64)   4160        max_pooling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 17, 79, 64)   4160        max_pooling2d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 17, 79, 64)   4160        max_pooling2d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "maximum_32 (Maximum)            (None, 13, 79, 64)   0           conv2d_64[0][0]                  \n",
      "                                                                 conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_41 (Maximum)            (None, 10, 20, 64)   0           conv2d_82[0][0]                  \n",
      "                                                                 conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_50 (Maximum)            (None, 17, 79, 64)   0           conv2d_100[0][0]                 \n",
      "                                                                 conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 13, 79, 64)   192         maximum_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 10, 20, 64)   192         maximum_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 17, 79, 64)   192         maximum_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 13, 79, 32)   18464       batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 13, 79, 32)   18464       batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 10, 20, 32)   18464       batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 10, 20, 32)   18464       batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 17, 79, 32)   18464       batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 17, 79, 32)   18464       batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "maximum_33 (Maximum)            (None, 13, 79, 32)   0           conv2d_66[0][0]                  \n",
      "                                                                 conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_42 (Maximum)            (None, 10, 20, 32)   0           conv2d_84[0][0]                  \n",
      "                                                                 conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_51 (Maximum)            (None, 17, 79, 32)   0           conv2d_102[0][0]                 \n",
      "                                                                 conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 13, 79, 32)   96          maximum_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 10, 20, 32)   96          maximum_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 17, 79, 32)   96          maximum_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 13, 79, 32)   1056        batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 13, 79, 32)   1056        batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 10, 20, 32)   1056        batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 10, 20, 32)   1056        batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 17, 79, 32)   1056        batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 17, 79, 32)   1056        batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "maximum_34 (Maximum)            (None, 13, 79, 32)   0           conv2d_68[0][0]                  \n",
      "                                                                 conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_43 (Maximum)            (None, 10, 20, 32)   0           conv2d_86[0][0]                  \n",
      "                                                                 conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_52 (Maximum)            (None, 17, 79, 32)   0           conv2d_104[0][0]                 \n",
      "                                                                 conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 13, 79, 32)   96          maximum_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 10, 20, 32)   96          maximum_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 17, 79, 32)   96          maximum_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 13, 79, 32)   1056        batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 13, 79, 32)   1056        batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 10, 20, 32)   1056        batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 10, 20, 32)   1056        batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 17, 79, 32)   1056        batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 17, 79, 32)   1056        batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "maximum_35 (Maximum)            (None, 13, 79, 32)   0           conv2d_70[0][0]                  \n",
      "                                                                 conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_44 (Maximum)            (None, 10, 20, 32)   0           conv2d_88[0][0]                  \n",
      "                                                                 conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "maximum_53 (Maximum)            (None, 17, 79, 32)   0           conv2d_106[0][0]                 \n",
      "                                                                 conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 6, 39, 32)    0           maximum_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, 5, 10, 32)    0           maximum_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling2D) (None, 8, 39, 32)    0           maximum_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 32)           0           max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 32)           0           max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 32)           0           max_pooling2d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32)           0           global_average_pooling2d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 96)           0           dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "age_cat (InputLayer)            [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sex_cat (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "height_weight (InputLayer)      [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "is_preg (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "loc (InputLayer)                [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "rr (InputLayer)                 [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "qrs (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 2)            194         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 425,858\n",
      "Trainable params: 424,322\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d637cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_feature['ord1'] = True \n",
    "params_feature['mm_mean'] = False\n",
    "params_feature['trim'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d2e2a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 11:25:28.897421: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 11:25:33.266931: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 23s 364ms/step - loss: 1.2458 - accuracy: 0.6211 - auc: 0.6623 - val_loss: 7.0394 - val_accuracy: 0.2108 - val_auc: 0.2085\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 12s 310ms/step - loss: 0.9606 - accuracy: 0.7035 - auc: 0.7775 - val_loss: 1.2883 - val_accuracy: 0.2108 - val_auc: 0.2626\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 12s 310ms/step - loss: 0.8953 - accuracy: 0.7363 - auc: 0.8003 - val_loss: 0.8802 - val_accuracy: 0.2805 - val_auc: 0.2561\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 12s 313ms/step - loss: 0.8572 - accuracy: 0.7723 - auc: 0.8220 - val_loss: 1.8427 - val_accuracy: 0.2171 - val_auc: 0.2544\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 12s 307ms/step - loss: 0.8272 - accuracy: 0.8109 - auc: 0.8445 - val_loss: 1.5472 - val_accuracy: 0.2520 - val_auc: 0.2707\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 12s 311ms/step - loss: 0.7744 - accuracy: 0.8324 - auc: 0.8590 - val_loss: 0.4300 - val_accuracy: 0.8225 - val_auc: 0.8912\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 12s 311ms/step - loss: 0.7500 - accuracy: 0.8551 - auc: 0.8668 - val_loss: 1.0294 - val_accuracy: 0.2932 - val_auc: 0.3201\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 12s 307ms/step - loss: 0.7583 - accuracy: 0.8520 - auc: 0.8608 - val_loss: 0.3803 - val_accuracy: 0.8558 - val_auc: 0.9184\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 12s 308ms/step - loss: 0.7201 - accuracy: 0.8680 - auc: 0.8721 - val_loss: 0.5649 - val_accuracy: 0.8479 - val_auc: 0.8700\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 12s 308ms/step - loss: 0.7134 - accuracy: 0.8664 - auc: 0.8743 - val_loss: 0.3309 - val_accuracy: 0.8875 - val_auc: 0.9471\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.6899 - accuracy: 0.8809 - auc: 0.8794 - val_loss: 0.6562 - val_accuracy: 0.8399 - val_auc: 0.9003\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 12s 305ms/step - loss: 0.6873 - accuracy: 0.8738 - auc: 0.8872 - val_loss: 0.4453 - val_accuracy: 0.7940 - val_auc: 0.8794\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 12s 306ms/step - loss: 0.6838 - accuracy: 0.8785 - auc: 0.8745 - val_loss: 0.3307 - val_accuracy: 0.8764 - val_auc: 0.9382\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.7032 - accuracy: 0.8625 - auc: 0.8724 - val_loss: 0.3282 - val_accuracy: 0.8637 - val_auc: 0.9382\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 0.6756 - accuracy: 0.8754 - auc: 0.8783 - val_loss: 0.3462 - val_accuracy: 0.8669 - val_auc: 0.9310\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 12s 297ms/step - loss: 0.6537 - accuracy: 0.8949 - auc: 0.8858 - val_loss: 0.3786 - val_accuracy: 0.8716 - val_auc: 0.9289\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6587 - accuracy: 0.8754 - auc: 0.8823 - val_loss: 0.3375 - val_accuracy: 0.8811 - val_auc: 0.9321\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.6366 - accuracy: 0.8906 - auc: 0.8844 - val_loss: 0.3424 - val_accuracy: 0.8590 - val_auc: 0.9302\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.6488 - accuracy: 0.8816 - auc: 0.8788 - val_loss: 0.3011 - val_accuracy: 0.8906 - val_auc: 0.9477\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.6168 - accuracy: 0.8938 - auc: 0.8829 - val_loss: 0.3120 - val_accuracy: 0.8843 - val_auc: 0.9440\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.6311 - accuracy: 0.8949 - auc: 0.8853 - val_loss: 0.2955 - val_accuracy: 0.8938 - val_auc: 0.9525\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.6304 - accuracy: 0.8957 - auc: 0.8822 - val_loss: 0.3255 - val_accuracy: 0.8891 - val_auc: 0.9462\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 12s 305ms/step - loss: 0.6309 - accuracy: 0.8953 - auc: 0.8902 - val_loss: 1.1673 - val_accuracy: 0.8146 - val_auc: 0.8642\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 12s 305ms/step - loss: 0.6340 - accuracy: 0.8844 - auc: 0.8855 - val_loss: 0.5230 - val_accuracy: 0.7179 - val_auc: 0.8096\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.6268 - accuracy: 0.8859 - auc: 0.8885 - val_loss: 0.4657 - val_accuracy: 0.7813 - val_auc: 0.8610\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.6531 - accuracy: 0.8816 - auc: 0.8882 - val_loss: 0.3123 - val_accuracy: 0.9033 - val_auc: 0.9439\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.6126 - accuracy: 0.8984 - auc: 0.8897 - val_loss: 0.3318 - val_accuracy: 0.8526 - val_auc: 0.9353\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.6348 - accuracy: 0.8945 - auc: 0.8827 - val_loss: 0.2949 - val_accuracy: 0.8986 - val_auc: 0.9500\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6470 - accuracy: 0.8781 - auc: 0.8878 - val_loss: 0.4173 - val_accuracy: 0.8225 - val_auc: 0.8951\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6051 - accuracy: 0.8988 - auc: 0.8880 - val_loss: 0.6138 - val_accuracy: 0.8463 - val_auc: 0.9173\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.6298 - accuracy: 0.8922 - auc: 0.8886 - val_loss: 0.3934 - val_accuracy: 0.8368 - val_auc: 0.9067\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.6080 - accuracy: 0.8977 - auc: 0.8829 - val_loss: 0.2963 - val_accuracy: 0.9033 - val_auc: 0.9486\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6027 - accuracy: 0.8957 - auc: 0.8895 - val_loss: 4.4192 - val_accuracy: 0.2282 - val_auc: 0.2665\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.6335 - accuracy: 0.8871 - auc: 0.8893 - val_loss: 0.3316 - val_accuracy: 0.8827 - val_auc: 0.9434\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 12s 307ms/step - loss: 0.6104 - accuracy: 0.9008 - auc: 0.8926 - val_loss: 0.3522 - val_accuracy: 0.8843 - val_auc: 0.9381\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 12s 305ms/step - loss: 0.6045 - accuracy: 0.8938 - auc: 0.8906 - val_loss: 0.4615 - val_accuracy: 0.7876 - val_auc: 0.8648\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 12s 306ms/step - loss: 0.6020 - accuracy: 0.8879 - auc: 0.8813 - val_loss: 0.3119 - val_accuracy: 0.8796 - val_auc: 0.9479\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.6022 - accuracy: 0.9047 - auc: 0.8874 - val_loss: 0.3966 - val_accuracy: 0.8843 - val_auc: 0.9276\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 12s 307ms/step - loss: 0.5905 - accuracy: 0.9020 - auc: 0.8880 - val_loss: 0.5118 - val_accuracy: 0.8558 - val_auc: 0.9173\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 12s 305ms/step - loss: 0.5776 - accuracy: 0.9059 - auc: 0.8941 - val_loss: 0.3415 - val_accuracy: 0.8938 - val_auc: 0.9343\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6038 - accuracy: 0.8934 - auc: 0.8910 - val_loss: 0.3142 - val_accuracy: 0.9017 - val_auc: 0.9422\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.5932 - accuracy: 0.9074 - auc: 0.8940 - val_loss: 0.4167 - val_accuracy: 0.8162 - val_auc: 0.8962\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 12s 305ms/step - loss: 0.5955 - accuracy: 0.9031 - auc: 0.8927 - val_loss: 1.8880 - val_accuracy: 0.8146 - val_auc: 0.8354\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 12s 297ms/step - loss: 0.5558 - accuracy: 0.9133 - auc: 0.8937 - val_loss: 0.2928 - val_accuracy: 0.8891 - val_auc: 0.9534\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 12s 291ms/step - loss: 0.6072 - accuracy: 0.8855 - auc: 0.8836 - val_loss: 0.4685 - val_accuracy: 0.7734 - val_auc: 0.8626\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 12s 294ms/step - loss: 0.5570 - accuracy: 0.9160 - auc: 0.8997 - val_loss: 0.4581 - val_accuracy: 0.7829 - val_auc: 0.8648\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.5640 - accuracy: 0.9176 - auc: 0.8967 - val_loss: 0.4381 - val_accuracy: 0.8177 - val_auc: 0.9027\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.5435 - accuracy: 0.9227 - auc: 0.8881 - val_loss: 0.3304 - val_accuracy: 0.8938 - val_auc: 0.9385\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 12s 305ms/step - loss: 0.5607 - accuracy: 0.9109 - auc: 0.8997 - val_loss: 0.3165 - val_accuracy: 0.8780 - val_auc: 0.9422\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 12s 308ms/step - loss: 0.5443 - accuracy: 0.9145 - auc: 0.8992 - val_loss: 0.4344 - val_accuracy: 0.8669 - val_auc: 0.9354\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 12s 306ms/step - loss: 0.5302 - accuracy: 0.9176 - auc: 0.8992 - val_loss: 0.2961 - val_accuracy: 0.8970 - val_auc: 0.9520\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 12s 306ms/step - loss: 0.5440 - accuracy: 0.9305 - auc: 0.9041 - val_loss: 0.5124 - val_accuracy: 0.7369 - val_auc: 0.8404\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.5278 - accuracy: 0.9285 - auc: 0.9010 - val_loss: 0.6747 - val_accuracy: 0.7290 - val_auc: 0.8068\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.5480 - accuracy: 0.9180 - auc: 0.8981 - val_loss: 0.3210 - val_accuracy: 0.8906 - val_auc: 0.9398\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.5318 - accuracy: 0.9238 - auc: 0.9049 - val_loss: 0.3906 - val_accuracy: 0.8938 - val_auc: 0.9401\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.5451 - accuracy: 0.9246 - auc: 0.8958 - val_loss: 0.4333 - val_accuracy: 0.7940 - val_auc: 0.8877\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.5164 - accuracy: 0.9273 - auc: 0.9039 - val_loss: 0.3388 - val_accuracy: 0.8891 - val_auc: 0.9454\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.5116 - accuracy: 0.9375 - auc: 0.9007 - val_loss: 0.5267 - val_accuracy: 0.8716 - val_auc: 0.9271\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.5115 - accuracy: 0.9301 - auc: 0.9035 - val_loss: 0.3597 - val_accuracy: 0.8431 - val_auc: 0.9274\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.5215 - accuracy: 0.9316 - auc: 0.9013 - val_loss: 0.3051 - val_accuracy: 0.8843 - val_auc: 0.9511\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.5085 - accuracy: 0.9355 - auc: 0.9037 - val_loss: 0.3129 - val_accuracy: 0.8891 - val_auc: 0.9483\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.5098 - accuracy: 0.9277 - auc: 0.9051 - val_loss: 0.3066 - val_accuracy: 0.8906 - val_auc: 0.9506\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.4937 - accuracy: 0.9359 - auc: 0.9049 - val_loss: 0.3245 - val_accuracy: 0.8875 - val_auc: 0.9434\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 12s 305ms/step - loss: 0.5182 - accuracy: 0.9379 - auc: 0.9063 - val_loss: 0.3979 - val_accuracy: 0.8938 - val_auc: 0.9376\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 12s 307ms/step - loss: 0.5140 - accuracy: 0.9383 - auc: 0.9066 - val_loss: 0.3526 - val_accuracy: 0.8494 - val_auc: 0.9280\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.4852 - accuracy: 0.9457 - auc: 0.9042 - val_loss: 0.3402 - val_accuracy: 0.8938 - val_auc: 0.9396\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.4865 - accuracy: 0.9406 - auc: 0.9063 - val_loss: 0.3427 - val_accuracy: 0.8938 - val_auc: 0.9454\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.4736 - accuracy: 0.9383 - auc: 0.9110 - val_loss: 0.3212 - val_accuracy: 0.8859 - val_auc: 0.9447\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.5067 - accuracy: 0.9379 - auc: 0.9101 - val_loss: 0.3631 - val_accuracy: 0.8938 - val_auc: 0.9437\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.4830 - accuracy: 0.9457 - auc: 0.9058 - val_loss: 0.3529 - val_accuracy: 0.8906 - val_auc: 0.9448\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.4823 - accuracy: 0.9441 - auc: 0.9074 - val_loss: 0.3435 - val_accuracy: 0.8875 - val_auc: 0.9496\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.5043 - accuracy: 0.9477 - auc: 0.9003 - val_loss: 0.3251 - val_accuracy: 0.8906 - val_auc: 0.9451\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.4873 - accuracy: 0.9520 - auc: 0.9105 - val_loss: 0.3508 - val_accuracy: 0.8875 - val_auc: 0.9465\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.4977 - accuracy: 0.9441 - auc: 0.9119 - val_loss: 0.3265 - val_accuracy: 0.8780 - val_auc: 0.9450\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.4927 - accuracy: 0.9457 - auc: 0.9081 - val_loss: 0.3277 - val_accuracy: 0.8764 - val_auc: 0.9442\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.4722 - accuracy: 0.9418 - auc: 0.9088 - val_loss: 0.3307 - val_accuracy: 0.8764 - val_auc: 0.9453\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 12s 307ms/step - loss: 0.4798 - accuracy: 0.9496 - auc: 0.9067 - val_loss: 0.3257 - val_accuracy: 0.8811 - val_auc: 0.9460\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.4848 - accuracy: 0.9387 - auc: 0.9058 - val_loss: 0.3366 - val_accuracy: 0.8891 - val_auc: 0.9479\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.4933 - accuracy: 0.9402 - auc: 0.9116 - val_loss: 0.3320 - val_accuracy: 0.8906 - val_auc: 0.9479\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.4797 - accuracy: 0.9445 - auc: 0.9078 - val_loss: 0.3241 - val_accuracy: 0.8843 - val_auc: 0.9452\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.4987 - accuracy: 0.9414 - auc: 0.9079 - val_loss: 0.3340 - val_accuracy: 0.8906 - val_auc: 0.9453\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 12s 308ms/step - loss: 0.4866 - accuracy: 0.9395 - auc: 0.9141 - val_loss: 0.3363 - val_accuracy: 0.8906 - val_auc: 0.9457\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 12s 309ms/step - loss: 0.4807 - accuracy: 0.9520 - auc: 0.9037 - val_loss: 0.3325 - val_accuracy: 0.8922 - val_auc: 0.9460\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 12s 306ms/step - loss: 0.4818 - accuracy: 0.9410 - auc: 0.9108 - val_loss: 0.3385 - val_accuracy: 0.8922 - val_auc: 0.9460\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.4755 - accuracy: 0.9484 - auc: 0.9093 - val_loss: 0.3484 - val_accuracy: 0.8875 - val_auc: 0.9441\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.4862 - accuracy: 0.9449 - auc: 0.9092 - val_loss: 0.3390 - val_accuracy: 0.8875 - val_auc: 0.9450\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.4641 - accuracy: 0.9488 - auc: 0.9040 - val_loss: 0.3454 - val_accuracy: 0.8843 - val_auc: 0.9458\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.4879 - accuracy: 0.9367 - auc: 0.9182 - val_loss: 0.3280 - val_accuracy: 0.8954 - val_auc: 0.9479\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.4843 - accuracy: 0.9488 - auc: 0.9120 - val_loss: 0.3299 - val_accuracy: 0.8875 - val_auc: 0.9477\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.4898 - accuracy: 0.9480 - auc: 0.9083 - val_loss: 0.3285 - val_accuracy: 0.8906 - val_auc: 0.9479\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.4828 - accuracy: 0.9426 - auc: 0.9135 - val_loss: 0.3351 - val_accuracy: 0.8922 - val_auc: 0.9484\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 12s 307ms/step - loss: 0.4922 - accuracy: 0.9453 - auc: 0.9124 - val_loss: 0.3354 - val_accuracy: 0.8891 - val_auc: 0.9481\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 12s 306ms/step - loss: 0.4625 - accuracy: 0.9543 - auc: 0.9060 - val_loss: 0.3373 - val_accuracy: 0.8891 - val_auc: 0.9479\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.4990 - accuracy: 0.9473 - auc: 0.9097 - val_loss: 0.3340 - val_accuracy: 0.8891 - val_auc: 0.9467\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 12s 306ms/step - loss: 0.4852 - accuracy: 0.9426 - auc: 0.9171 - val_loss: 0.3429 - val_accuracy: 0.8875 - val_auc: 0.9451\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.4565 - accuracy: 0.9496 - auc: 0.9025 - val_loss: 0.3324 - val_accuracy: 0.8922 - val_auc: 0.9476\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.4852 - accuracy: 0.9496 - auc: 0.9083 - val_loss: 0.3330 - val_accuracy: 0.8954 - val_auc: 0.9481\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.4712 - accuracy: 0.9453 - auc: 0.9079 - val_loss: 0.3470 - val_accuracy: 0.8859 - val_auc: 0.9467\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.4754 - accuracy: 0.9500 - auc: 0.9044 - val_loss: 0.3360 - val_accuracy: 0.8938 - val_auc: 0.9476\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.4799 - accuracy: 0.9484 - auc: 0.9110 - val_loss: 0.3320 - val_accuracy: 0.8906 - val_auc: 0.9472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa2807aebb0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    n_epoch = 100\n",
    "    lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "    batch_size = 64\n",
    "    params = {'batch_size': batch_size,\n",
    "              #          'input_shape': (100, 313, 1),\n",
    "              'shuffle': True,\n",
    "              'beta_param': 0.7,\n",
    "              'mixup': True,\n",
    "              #          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "              #          'highpass': [.5, [78,79,80,81,82,83,84,85]]\n",
    "              'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "              #           'dropblock' : [30, 100]\n",
    "              #'device' : device\n",
    "    }\n",
    "\n",
    "    params_no_shuffle = {'batch_size': batch_size,\n",
    "                         #          'input_shape': (100, 313, 1),\n",
    "                         'shuffle': False,\n",
    "                         'beta_param': 0.7,\n",
    "                         'mixup': False\n",
    "                         #'device': device\n",
    "    }\n",
    "\n",
    "    TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], features_trn['preg'], features_trn['loc'],\n",
    "                              features_trn['rr1'],features_trn['qrs1'],\n",
    "                              features_trn['mel1'],features_trn['cqt1'],features_trn['stft1']],\n",
    "                             mm_lbs_trn,  ## our Y\n",
    "                             **params)()\n",
    "\n",
    "    class_weight = {0: 3, 1: 1.}\n",
    "\n",
    "    model1.fit(TrainDGen_1,\n",
    "          validation_data = ([features_test['age'],features_test['sex'], features_test['hw'],\n",
    "                              features_test['preg'], features_test['loc'], \n",
    "                              features_test['rr1'], features_test['qrs1'],\n",
    "                              features_test['mel1'],\n",
    "                              features_test['cqt1'], features_test['stft1']],\n",
    "                             mm_lbs_test),\n",
    "                             callbacks=[lr],\n",
    "                              steps_per_epoch=np.ceil(len(mm_lbs_trn)/64),\n",
    "                           class_weight=class_weight,\n",
    "                             epochs = n_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "725df76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    params_feature['mel_shape'] = mel_input_shape\n",
    "    params_feature['cqt_shape'] = cqt_input_shape\n",
    "    params_feature['stft_shape'] = stft_input_shape\n",
    "\n",
    "    params_feature['use_mel'] = use_mel\n",
    "    params_feature['use_cqt'] = use_cqt\n",
    "    params_feature['use_stft'] = use_stft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33a0ae8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 16s 328ms/step - loss: 0.8513 - accuracy: 0.5199 - auc: 0.5247 - val_loss: 5.3744 - val_accuracy: 0.5008 - val_auc: 0.5024\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.7361 - accuracy: 0.5227 - auc: 0.5384 - val_loss: 2.8538 - val_accuracy: 0.5040 - val_auc: 0.5646\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.7161 - accuracy: 0.5289 - auc: 0.5532 - val_loss: 0.8623 - val_accuracy: 0.5784 - val_auc: 0.6247\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 12s 293ms/step - loss: 0.7086 - accuracy: 0.5309 - auc: 0.5445 - val_loss: 1.0038 - val_accuracy: 0.5135 - val_auc: 0.5938\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 12s 294ms/step - loss: 0.6986 - accuracy: 0.5273 - auc: 0.5711 - val_loss: 1.0302 - val_accuracy: 0.5055 - val_auc: 0.5928\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 12s 298ms/step - loss: 0.6930 - accuracy: 0.5395 - auc: 0.5503 - val_loss: 0.8318 - val_accuracy: 0.5151 - val_auc: 0.6014\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.6901 - accuracy: 0.5449 - auc: 0.5763 - val_loss: 0.6653 - val_accuracy: 0.5800 - val_auc: 0.6406\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.6850 - accuracy: 0.5660 - auc: 0.5903 - val_loss: 0.6267 - val_accuracy: 0.6545 - val_auc: 0.7107\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6859 - accuracy: 0.5527 - auc: 0.5816 - val_loss: 0.6508 - val_accuracy: 0.6117 - val_auc: 0.6658\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.6829 - accuracy: 0.5617 - auc: 0.5925 - val_loss: 0.6843 - val_accuracy: 0.5610 - val_auc: 0.6271\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6785 - accuracy: 0.5836 - auc: 0.5992 - val_loss: 0.6319 - val_accuracy: 0.6371 - val_auc: 0.6965\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.6756 - accuracy: 0.5734 - auc: 0.5977 - val_loss: 0.6375 - val_accuracy: 0.6751 - val_auc: 0.7071\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6731 - accuracy: 0.5805 - auc: 0.6132 - val_loss: 0.6898 - val_accuracy: 0.4976 - val_auc: 0.6067\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.6723 - accuracy: 0.5801 - auc: 0.6047 - val_loss: 0.6572 - val_accuracy: 0.6101 - val_auc: 0.6708\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 0.6680 - accuracy: 0.5980 - auc: 0.6219 - val_loss: 0.7027 - val_accuracy: 0.4992 - val_auc: 0.6046\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6716 - accuracy: 0.5789 - auc: 0.6111 - val_loss: 0.6179 - val_accuracy: 0.6561 - val_auc: 0.7207\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.6658 - accuracy: 0.6020 - auc: 0.6294 - val_loss: 0.6295 - val_accuracy: 0.6513 - val_auc: 0.7094\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 12s 298ms/step - loss: 0.6619 - accuracy: 0.6137 - auc: 0.6263 - val_loss: 0.6869 - val_accuracy: 0.6117 - val_auc: 0.6763\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.6583 - accuracy: 0.6164 - auc: 0.6328 - val_loss: 0.6168 - val_accuracy: 0.6704 - val_auc: 0.7175\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.6603 - accuracy: 0.6055 - auc: 0.6450 - val_loss: 0.6036 - val_accuracy: 0.6815 - val_auc: 0.7364\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.6636 - accuracy: 0.5992 - auc: 0.6278 - val_loss: 0.6140 - val_accuracy: 0.6513 - val_auc: 0.7127\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.6620 - accuracy: 0.6187 - auc: 0.6291 - val_loss: 0.6513 - val_accuracy: 0.6212 - val_auc: 0.6563\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.6567 - accuracy: 0.6238 - auc: 0.6395 - val_loss: 0.6096 - val_accuracy: 0.6688 - val_auc: 0.7168\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.6534 - accuracy: 0.6191 - auc: 0.6464 - val_loss: 1.3800 - val_accuracy: 0.5135 - val_auc: 0.5841\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.6597 - accuracy: 0.6098 - auc: 0.6315 - val_loss: 0.8288 - val_accuracy: 0.6323 - val_auc: 0.6925\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.6546 - accuracy: 0.6164 - auc: 0.6253 - val_loss: 0.8448 - val_accuracy: 0.6228 - val_auc: 0.6767\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.6551 - accuracy: 0.6199 - auc: 0.6363 - val_loss: 0.6796 - val_accuracy: 0.6260 - val_auc: 0.6657\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.6500 - accuracy: 0.6137 - auc: 0.6504 - val_loss: 0.6184 - val_accuracy: 0.6656 - val_auc: 0.7133\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6484 - accuracy: 0.6316 - auc: 0.6460 - val_loss: 0.5982 - val_accuracy: 0.6609 - val_auc: 0.7265\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.6449 - accuracy: 0.6313 - auc: 0.6610 - val_loss: 1.1817 - val_accuracy: 0.5325 - val_auc: 0.6030\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.6430 - accuracy: 0.6352 - auc: 0.6648 - val_loss: 0.9073 - val_accuracy: 0.6006 - val_auc: 0.6383\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.6427 - accuracy: 0.6453 - auc: 0.6597 - val_loss: 0.6093 - val_accuracy: 0.6609 - val_auc: 0.7204\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.6438 - accuracy: 0.6398 - auc: 0.6529 - val_loss: 0.6432 - val_accuracy: 0.6403 - val_auc: 0.6845\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 12s 295ms/step - loss: 0.6403 - accuracy: 0.6438 - auc: 0.6810 - val_loss: 0.6266 - val_accuracy: 0.6624 - val_auc: 0.7135\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 0.6372 - accuracy: 0.6523 - auc: 0.6744 - val_loss: 1.2547 - val_accuracy: 0.5674 - val_auc: 0.6259\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.6413 - accuracy: 0.6410 - auc: 0.6566 - val_loss: 0.6524 - val_accuracy: 0.6656 - val_auc: 0.7002\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6374 - accuracy: 0.6387 - auc: 0.6652 - val_loss: 0.6229 - val_accuracy: 0.6672 - val_auc: 0.7116\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6394 - accuracy: 0.6516 - auc: 0.6624 - val_loss: 0.6864 - val_accuracy: 0.6149 - val_auc: 0.6467\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6325 - accuracy: 0.6410 - auc: 0.6738 - val_loss: 0.9849 - val_accuracy: 0.5705 - val_auc: 0.5982\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.6262 - accuracy: 0.6457 - auc: 0.6869 - val_loss: 0.6297 - val_accuracy: 0.6181 - val_auc: 0.6809\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6314 - accuracy: 0.6668 - auc: 0.6716 - val_loss: 1.1260 - val_accuracy: 0.5420 - val_auc: 0.5568\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6292 - accuracy: 0.6578 - auc: 0.6821 - val_loss: 0.7889 - val_accuracy: 0.5515 - val_auc: 0.6248\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6342 - accuracy: 0.6652 - auc: 0.6782 - val_loss: 0.7856 - val_accuracy: 0.6212 - val_auc: 0.6577\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.6199 - accuracy: 0.6699 - auc: 0.6909 - val_loss: 0.6630 - val_accuracy: 0.6498 - val_auc: 0.6846\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.6280 - accuracy: 0.6641 - auc: 0.6744 - val_loss: 0.7574 - val_accuracy: 0.6260 - val_auc: 0.6655\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6276 - accuracy: 0.6617 - auc: 0.6813 - val_loss: 0.8424 - val_accuracy: 0.5610 - val_auc: 0.5875\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.6173 - accuracy: 0.6699 - auc: 0.7007 - val_loss: 0.6522 - val_accuracy: 0.6165 - val_auc: 0.6635\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6239 - accuracy: 0.6637 - auc: 0.6887 - val_loss: 0.8601 - val_accuracy: 0.6101 - val_auc: 0.6343\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.6133 - accuracy: 0.6840 - auc: 0.6907 - val_loss: 0.6203 - val_accuracy: 0.6466 - val_auc: 0.7145\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6037 - accuracy: 0.6980 - auc: 0.7079 - val_loss: 0.6458 - val_accuracy: 0.6498 - val_auc: 0.6912\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6114 - accuracy: 0.6832 - auc: 0.6918 - val_loss: 0.8467 - val_accuracy: 0.6244 - val_auc: 0.6347\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.6037 - accuracy: 0.6992 - auc: 0.7085 - val_loss: 1.1982 - val_accuracy: 0.5468 - val_auc: 0.5852\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.6005 - accuracy: 0.6973 - auc: 0.7174 - val_loss: 0.8201 - val_accuracy: 0.5341 - val_auc: 0.6191\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.6002 - accuracy: 0.7043 - auc: 0.7151 - val_loss: 0.6291 - val_accuracy: 0.6656 - val_auc: 0.7164\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.5907 - accuracy: 0.7082 - auc: 0.7134 - val_loss: 0.6557 - val_accuracy: 0.6545 - val_auc: 0.6999\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.5904 - accuracy: 0.7043 - auc: 0.7191 - val_loss: 0.9217 - val_accuracy: 0.5008 - val_auc: 0.5993\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.5714 - accuracy: 0.7441 - auc: 0.7334 - val_loss: 0.6555 - val_accuracy: 0.6529 - val_auc: 0.6941\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.5754 - accuracy: 0.7156 - auc: 0.7250 - val_loss: 1.0017 - val_accuracy: 0.5103 - val_auc: 0.5836\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.5595 - accuracy: 0.7305 - auc: 0.7380 - val_loss: 0.6404 - val_accuracy: 0.6529 - val_auc: 0.7015\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.5761 - accuracy: 0.7180 - auc: 0.7302 - val_loss: 0.8568 - val_accuracy: 0.6181 - val_auc: 0.6513\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.5529 - accuracy: 0.7480 - auc: 0.7442 - val_loss: 0.7489 - val_accuracy: 0.5658 - val_auc: 0.6240\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.5402 - accuracy: 0.7621 - auc: 0.7588 - val_loss: 0.7740 - val_accuracy: 0.6292 - val_auc: 0.6641\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.5482 - accuracy: 0.7590 - auc: 0.7597 - val_loss: 0.6927 - val_accuracy: 0.6403 - val_auc: 0.7029\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 12s 298ms/step - loss: 0.5462 - accuracy: 0.7648 - auc: 0.7458 - val_loss: 0.7982 - val_accuracy: 0.5800 - val_auc: 0.6374\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 0.5342 - accuracy: 0.7621 - auc: 0.7565 - val_loss: 0.9357 - val_accuracy: 0.6181 - val_auc: 0.6322\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 12s 295ms/step - loss: 0.5302 - accuracy: 0.7684 - auc: 0.7648 - val_loss: 0.7463 - val_accuracy: 0.6260 - val_auc: 0.6688\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.5218 - accuracy: 0.7812 - auc: 0.7743 - val_loss: 0.9792 - val_accuracy: 0.5563 - val_auc: 0.6141\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.5241 - accuracy: 0.7789 - auc: 0.7640 - val_loss: 0.9936 - val_accuracy: 0.6228 - val_auc: 0.6327\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.5205 - accuracy: 0.7918 - auc: 0.7696 - val_loss: 0.7841 - val_accuracy: 0.6212 - val_auc: 0.6637\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.5171 - accuracy: 0.7965 - auc: 0.7798 - val_loss: 0.7175 - val_accuracy: 0.6450 - val_auc: 0.6908\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.5166 - accuracy: 0.7879 - auc: 0.7639 - val_loss: 0.8406 - val_accuracy: 0.6307 - val_auc: 0.6811\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.5129 - accuracy: 0.7879 - auc: 0.7744 - val_loss: 0.8027 - val_accuracy: 0.6212 - val_auc: 0.6678\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.5212 - accuracy: 0.7848 - auc: 0.7747 - val_loss: 0.7927 - val_accuracy: 0.6101 - val_auc: 0.6589\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.5132 - accuracy: 0.7949 - auc: 0.7727 - val_loss: 0.8655 - val_accuracy: 0.6498 - val_auc: 0.6864\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.5084 - accuracy: 0.8027 - auc: 0.7785 - val_loss: 0.7596 - val_accuracy: 0.6323 - val_auc: 0.6848\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.5068 - accuracy: 0.7988 - auc: 0.7870 - val_loss: 0.7748 - val_accuracy: 0.6260 - val_auc: 0.6737\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.5000 - accuracy: 0.8094 - auc: 0.7862 - val_loss: 0.7753 - val_accuracy: 0.6450 - val_auc: 0.6942\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 12s 298ms/step - loss: 0.5020 - accuracy: 0.7969 - auc: 0.7831 - val_loss: 0.8946 - val_accuracy: 0.6292 - val_auc: 0.6630\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.5085 - accuracy: 0.8098 - auc: 0.7759 - val_loss: 0.8444 - val_accuracy: 0.6339 - val_auc: 0.6825\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 12s 303ms/step - loss: 0.5007 - accuracy: 0.8070 - auc: 0.7882 - val_loss: 0.8931 - val_accuracy: 0.6181 - val_auc: 0.6607\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.4964 - accuracy: 0.8109 - auc: 0.7804 - val_loss: 0.8105 - val_accuracy: 0.6165 - val_auc: 0.6667\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.4980 - accuracy: 0.8043 - auc: 0.7835 - val_loss: 0.8008 - val_accuracy: 0.6165 - val_auc: 0.6662\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.4999 - accuracy: 0.8043 - auc: 0.7808 - val_loss: 0.7954 - val_accuracy: 0.6197 - val_auc: 0.6679\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.5043 - accuracy: 0.7941 - auc: 0.7867 - val_loss: 0.8006 - val_accuracy: 0.6260 - val_auc: 0.6840\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.4955 - accuracy: 0.8164 - auc: 0.7954 - val_loss: 0.8053 - val_accuracy: 0.6339 - val_auc: 0.6834\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.5001 - accuracy: 0.8145 - auc: 0.7861 - val_loss: 0.8752 - val_accuracy: 0.6197 - val_auc: 0.6753\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.5001 - accuracy: 0.7977 - auc: 0.7905 - val_loss: 0.7795 - val_accuracy: 0.6307 - val_auc: 0.6880\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.4885 - accuracy: 0.8145 - auc: 0.7921 - val_loss: 0.7945 - val_accuracy: 0.6434 - val_auc: 0.6919\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.4896 - accuracy: 0.8203 - auc: 0.7901 - val_loss: 0.8055 - val_accuracy: 0.6292 - val_auc: 0.6794\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.4997 - accuracy: 0.8109 - auc: 0.7881 - val_loss: 0.8204 - val_accuracy: 0.6165 - val_auc: 0.6687\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.4983 - accuracy: 0.8027 - auc: 0.7916 - val_loss: 0.8361 - val_accuracy: 0.6244 - val_auc: 0.6712\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.4946 - accuracy: 0.8090 - auc: 0.7889 - val_loss: 0.8357 - val_accuracy: 0.6212 - val_auc: 0.6747\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 12s 300ms/step - loss: 0.4883 - accuracy: 0.8160 - auc: 0.7993 - val_loss: 0.8143 - val_accuracy: 0.6307 - val_auc: 0.6747\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 12s 295ms/step - loss: 0.4905 - accuracy: 0.8133 - auc: 0.7889 - val_loss: 0.8399 - val_accuracy: 0.6197 - val_auc: 0.6698\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 12s 294ms/step - loss: 0.4897 - accuracy: 0.8141 - auc: 0.7898 - val_loss: 0.8098 - val_accuracy: 0.6307 - val_auc: 0.6836\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 12s 297ms/step - loss: 0.4918 - accuracy: 0.8152 - auc: 0.7904 - val_loss: 0.8196 - val_accuracy: 0.6165 - val_auc: 0.6708\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 12s 301ms/step - loss: 0.4912 - accuracy: 0.8133 - auc: 0.7922 - val_loss: 0.8176 - val_accuracy: 0.6244 - val_auc: 0.6795\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 0.4916 - accuracy: 0.8023 - auc: 0.7940 - val_loss: 0.8166 - val_accuracy: 0.6165 - val_auc: 0.6707\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 0.4974 - accuracy: 0.8059 - auc: 0.7828 - val_loss: 0.8653 - val_accuracy: 0.6276 - val_auc: 0.6714\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 12s 300ms/step - loss: 0.4950 - accuracy: 0.8082 - auc: 0.7942 - val_loss: 0.8165 - val_accuracy: 0.6197 - val_auc: 0.6730\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'params_feature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m class_weight \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;241m1.\u001b[39m}\n\u001b[1;32m     31\u001b[0m model2\u001b[38;5;241m.\u001b[39mfit(TrainDGen_1,\n\u001b[1;32m     32\u001b[0m       validation_data \u001b[38;5;241m=\u001b[39m ([features_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m],features_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m], features_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhw\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     33\u001b[0m                           features_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreg\u001b[39m\u001b[38;5;124m'\u001b[39m], features_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m                           steps_per_epoch\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(out_lbs_trn)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m64\u001b[39m),\n\u001b[1;32m     40\u001b[0m                          epochs \u001b[38;5;241m=\u001b[39m n_epoch)\n\u001b[0;32m---> 43\u001b[0m \u001b[43mparams_feature\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmel_shape\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m mel_input_shape\n\u001b[1;32m     44\u001b[0m params_feature[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcqt_shape\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cqt_input_shape\n\u001b[1;32m     45\u001b[0m params_feature[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstft_shape\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m stft_input_shape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'params_feature' is not defined"
     ]
    }
   ],
   "source": [
    "    n_epoch = 100\n",
    "    lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "    batch_size = 64\n",
    "    params = {'batch_size': batch_size,\n",
    "              #          'input_shape': (100, 313, 1),\n",
    "              'shuffle': True,\n",
    "              'beta_param': 0.7,\n",
    "              'mixup': True,\n",
    "              #          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "            'highpass': [.5, [78,79,80,81,82,83,84,85]],\n",
    "              'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "            #           'dropblock' : [30, 100]\n",
    "              #'device' : device\n",
    "    }\n",
    "    params_no_shuffle = {'batch_size': batch_size,\n",
    "                         #          'input_shape': (100, 313, 1),\n",
    "                         'shuffle': False,\n",
    "                         'beta_param': 0.7,\n",
    "                         'mixup': False\n",
    "                         #'device': device\n",
    "    }\n",
    "\n",
    "    TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], features_trn['preg'], features_trn['loc'],\n",
    "                              features_trn['rr1'],features_trn['qrs1'],\n",
    "                              features_trn['mel1'],features_trn['cqt1'],features_trn['stft1']],\n",
    "                             out_lbs_trn,  ## our Y\n",
    "                             **params)()\n",
    "\n",
    "    class_weight = {0: 3, 1: 1.}\n",
    "\n",
    "    model2.fit(TrainDGen_1,\n",
    "          validation_data = ([features_test['age'],features_test['sex'], features_test['hw'],\n",
    "                              features_test['preg'], features_test['loc'], \n",
    "                              features_test['rr1'], features_test['qrs1'],\n",
    "                              features_test['mel1'],\n",
    "                              features_test['cqt1'], features_test['stft1']],\n",
    "                             out_lbs_test),\n",
    "                             callbacks=[lr],\n",
    "                              steps_per_epoch=np.ceil(len(out_lbs_trn)/64),\n",
    "                             epochs = n_epoch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa43fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "def save_challenge_model(model_folder, model1, model2, m_name1, m_name2, param_feature) :\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    filename1 = os.path.join(model_folder, m_name1 + '_model1.hdf5')\n",
    "    filename2 = os.path.join(model_folder, m_name2 + '_model2.hdf5')\n",
    "    model1.save(filename1)\n",
    "    model2.save(filename2)\n",
    "    param_feature['model1'] = m_name1\n",
    "    param_feature['model2'] = m_name2\n",
    "    param_feature['model_fnm1'] = filename1\n",
    "    param_feature['model_fnm2'] = filename2\n",
    "    with open(info_fnm, 'wb') as f:\n",
    "        pk.dump(param_feature, f, pk.HIGHEST_PROTOCOL)\n",
    "    return 1\n",
    "\n",
    "def load_challenge_model(model_folder, verbose):\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    with open(info_fnm, 'rb') as f:\n",
    "        info_m = pk.load(f)\n",
    "#    if info_m['model'] == 'toy' :\n",
    "#        model = get_toy(info_m['mel_shape'])\n",
    "#    filename = os.path.join(model_folder, info_m['model'] + '_model.hdf5')\n",
    "#    model.load_weights(filename)\n",
    "    return info_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f26f41e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model.\n",
    "def run_model(model_folder, data_folder, output_folder, allow_failures, verbose):\n",
    "    # Load models.\n",
    "    if verbose >= 1:\n",
    "        print('Loading Challenge model...')\n",
    "\n",
    "    model = load_challenge_model(model_folder, verbose) ### Teams: Implement this function!!!\n",
    "\n",
    "    # Find the patient data files.\n",
    "    patient_files = find_patient_files(data_folder)\n",
    "    num_patient_files = len(patient_files)\n",
    "\n",
    "    if num_patient_files==0:\n",
    "        raise Exception('No data was provided.')\n",
    "\n",
    "    # Create a folder for the Challenge outputs if it does not already exist.\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Run the team's model on the Challenge data.\n",
    "    if verbose >= 1:\n",
    "        print('Running model on Challenge data...')\n",
    "\n",
    "#    @tf.function\n",
    "    # Iterate over the patient files.\n",
    "    for i in range(num_patient_files):\n",
    "        if verbose >= 2:\n",
    "            print('    {}/{}...'.format(i+1, num_patient_files))\n",
    "\n",
    "        patient_data = load_patient_data(patient_files[i])\n",
    "        recordings = load_recordings(data_folder, patient_data)\n",
    "\n",
    "        # Allow or disallow the model to fail on parts of the data; helpful for debugging.\n",
    "        try:\n",
    "            classes, labels, probabilities = run_challenge_model(model, patient_data, recordings, verbose) ### Teams: Implement this function!!!\n",
    "        except:\n",
    "            if allow_failures:\n",
    "                if verbose >= 2:\n",
    "                    print('... failed.')\n",
    "                classes, labels, probabilities = list(), list(), list()\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # Save Challenge outputs.\n",
    "        head, tail = os.path.split(patient_files[i])\n",
    "        root, extension = os.path.splitext(tail)\n",
    "        output_file = os.path.join(output_folder, root + '.csv')\n",
    "        patient_id = get_patient_id(patient_data)\n",
    "        save_challenge_outputs(output_file, patient_id, classes, labels, probabilities)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50591d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_challenge_model(model, data, recordings, verbose):\n",
    "\n",
    "    murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "    outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "    if model['model1'] == 'toy1' :\n",
    "        model1 = get_toy5_1(model['mel_shape'],model['cqt_shape'],model['stft_shape'] )\n",
    "    if model['model2'] == 'toy2' :\n",
    "        model2 = get_toy5_2(model['mel_shape'],model['cqt_shape'],model['stft_shape'])\n",
    "    if model['model1'] == 'lcnn1' :\n",
    "        model1 = get_LCNN_o_1(model['mel_shape'],model['cqt_shape'],model['stft_shape'], use_mel = model['use_mel'], use_cqt = model['use_cqt'], use_stft = model['use_stft'], ord1 = model['ord1'])\n",
    "    if model['model2'] == 'lcnn2' :\n",
    "        model2 = get_LCNN_2(model['mel_shape'],model['cqt_shape'],model['stft_shape'], use_mel = model['use_mel'], use_cqt = model['use_cqt'], use_stft = model['use_stft'])\n",
    "    if model['model1'] == 'resmax1' :\n",
    "        model1 = get_ResMax_o_1(model['mel_shape'],model['cqt_shape'],model['stft_shape'], use_mel = model['use_mel'], use_cqt = model['use_cqt'], use_stft = model['use_stft'], ord1 = model['ord1'])\n",
    "    if model['model2'] == 'resmax2' :\n",
    "        model2 = get_ResMax_2(model['mel_shape'],model['cqt_shape'],model['stft_shape'], use_mel = model['use_mel'], use_cqt = model['use_cqt'], use_stft = model['use_stft'])\n",
    "    if model['model1'] == 'lcnn1_dr' :\n",
    "        model1 = get_LCNN_o_1_dr(model['mel_shape'],model['cqt_shape'],model['stft_shape'], use_mel = model['use_mel'], use_cqt = model['use_cqt'], use_stft = model['use_stft'], ord1 = model['ord1'], dp = model['dp'], fc = model['fc'], ext = model[\\\n",
    "'ext'])\n",
    "    if model['model2'] == 'lcnn2_dr' :\n",
    "        model2 = get_LCNN_2_dr(model['mel_shape'],model['cqt_shape'],model['stft_shape'], use_mel = model['use_mel'], use_cqt = model['use_cqt'], use_stft = model['use_stft'], dp = model['dp'], fc = model['fc'], ext = model['ext'])\n",
    "    if model['model1'] == 'lcnn_rr_qrs1' :\n",
    "        model1 = get_LCNN_o_5_dr_qrs1(model['mel_shape'],model['cqt_shape'],model['stft_shape'], use_mel = model['use_mel'], use_cqt = model['use_cqt'], use_stft = model['use_stft'], ord1 = model['ord1'])\n",
    "    if model['model2'] == 'lcnn_rr_qrs2' :\n",
    "        model2 = get_LCNN_o_5_dr_qrs1_1(model['mel_shape'],model['cqt_shape'],model['stft_shape'], use_mel = model['use_mel'], use_cqt = model['use_cqt'], use_stft = model['use_stft'], ord1 = model['ord1'])\n",
    "    if model['model1'] == 'lcnn_rr_qrs2_1' :\n",
    "        model1 = get_LCNN_o_5_dr_qrs2(model['mel_shape'],model['cqt_shape'],model['stft_shape'], use_mel = model['use_mel'], use_cqt = model['use_cqt'], use_stft = model['use_stft'], ord1 = model['ord1'])\n",
    "    if model['model2'] == 'lcnn_rr_qrs2_2' :\n",
    "        model2 = get_LCNN_o_5_dr_qrs2_1(model['mel_shape'],model['cqt_shape'],model['stft_shape'], use_mel = model['use_mel'], use_cqt = model['use_cqt'], use_stft = model['use_stft'], ord1 = model['ord1'])\n",
    "    if model['model1'] == 'lcnn_rr_qrs2_1_1' :\n",
    "        model1 = get_LCNN_o_5_dr_qrs2_1_1(model['mel_shape'],model['cqt_shape'],model['stft_shape'], use_mel = model['use_mel'], use_cqt = model['use_cqt'], use_stft = model['use_stft'], ord1 = model['ord1'])\n",
    "    if model['model2'] == 'lcnn_rr_qrs2_1_2' :\n",
    "        model2 = get_LCNN_o_5_dr_qrs2_1_2(model['mel_shape'],model['cqt_shape'],model['stft_shape'], use_mel = model['use_mel'], use_cqt = model['use_cqt'], use_stft = model['use_stft'], ord1 = model['ord1'])\n",
    "    \n",
    "    \n",
    "    if model['model1'] == 'lcnn_rr_qrs3_1' :\n",
    "        model1 = get_LCNN_o_5_dr_qrs3_1(model['mel_shape'],model['cqt_shape'],model['stft_shape'], use_mel = model['use_mel'], use_cqt = model['use_cqt'], use_stft = model['use_stft'], ord1 = model['ord1'])\n",
    "    if model['model2'] == 'lcnn_rr_qrs3_2' :\n",
    "        model2 = get_LCNN_o_5_dr_qrs3_2(model['mel_shape'],model['cqt_shape'],model['stft_shape'], use_mel = model['use_mel'], use_cqt = model['use_cqt'], use_stft = model['use_stft'], ord1 = model['ord1'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    model1.load_weights(model['model_fnm1'])\n",
    "    model2.load_weights(model['model_fnm2'])\n",
    "\n",
    "#    classes = model['classes']\n",
    "    # Load features.\n",
    "    features = get_feature_one(data, verbose = 0)\n",
    "\n",
    "    samp_sec = model['samp_sec']\n",
    "    pre_emphasis = model['pre_emphasis']\n",
    "    hop_length = model['hop_length']\n",
    "    win_length = model['win_length']\n",
    "    n_mels = model['n_mels']\n",
    "    filter_scale = model['filter_scale']\n",
    "    n_bins = model['n_bins']\n",
    "    fmin = model['fmin']\n",
    "    use_mel = model['use_mel']\n",
    "    use_cqt = model['use_cqt']\n",
    "    use_stft = model['use_stft']\n",
    "\n",
    "    trim = model['trim']\n",
    "    \n",
    "    features['rr1'] = [] \n",
    "    for i in range(len(recordings)):\n",
    "        try:\n",
    "            ____, info = nk.ecg_process(recordings[i], sampling_rate=4000)\n",
    "#             rr = np.diff(info['ECG_R_Peaks'])/4000\n",
    "            rr = np.mean(np.diff(info['ECG_R_Peaks'])/4000)\n",
    "            current_rr = rr\n",
    "        except:\n",
    "            current_rr=np.zeros(1)\n",
    "        features['rr1'].append(current_rr)  \n",
    "    features['rr1'] = np.array(features['rr1'])\n",
    "    \n",
    "    \n",
    "    features['qrs1'] = []\n",
    "    for i in range(len(recordings)):\n",
    "        try:\n",
    "            R_peaks, S_point, Q_point=EKG_QRS_detect1(recording1, 4000, True, False)\n",
    "#             qrs = S_point-Q_point/4000\n",
    "            qrs = np.mean(S_point-Q_point)\n",
    "            current_qrs = qrs\n",
    "        except:\n",
    "            current_qrs=np.zeros(1) \n",
    "        features['qrs1'].append(current_rr)  \n",
    "    features['qrs1'] = np.array(features['qrs1'])\n",
    "    \n",
    "\n",
    "    features['mel1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_mel :\n",
    "            mel1 = feature_extract_melspec(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length,\n",
    "                                           win_length = win_length, n_mels = n_mels, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1) )\n",
    "        features['mel1'].append(mel1)\n",
    "    M, N = features['mel1'][0].shape\n",
    "\n",
    "    if use_mel :\n",
    "        for i in range(len(features['mel1'])) :\n",
    "            features['mel1'][i] = features['mel1'][i].reshape(M,N,1)\n",
    "    features['mel1'] = np.array(features['mel1'])\n",
    "\n",
    "    features['cqt1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_cqt :\n",
    "            mel1 = feature_extract_cqt(recordings[i], samp_sec=samp_sec, pre_emphasis = pre_emphasis, filter_scale = filter_scale,\n",
    "                                        n_bins = n_bins, fmin = fmin, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1))\n",
    "        features['cqt1'].append(mel1)\n",
    "    M, N = features['cqt1'][0].shape\n",
    "    if use_cqt :\n",
    "        for i in range(len(features['cqt1'])) :\n",
    "            features['cqt1'][i] = features['cqt1'][i].reshape(M,N,1)\n",
    "    features['cqt1'] = np.array(features['cqt1'])\n",
    "\n",
    "    features['stft1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_stft :\n",
    "            mel1 = feature_extract_stft(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length,\n",
    "                                        win_length = win_length, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1) )\n",
    "        features['stft1'].append(mel1)\n",
    "    M, N = features['stft1'][0].shape\n",
    "    if use_stft :\n",
    "        for i in range(len(features['stft1'])) :\n",
    "            features['stft1'][i] = features['stft1'][i].reshape(M,N,1)\n",
    "    features['stft1'] = np.array(features['stft1'])\n",
    "\n",
    "    #    print(features)\n",
    "    # Impute missing data.\n",
    "    res1 = model1.predict([features['age'], features['sex'], features['hw'], features['preg'], features['loc'], \n",
    "                           features['rr1'], features['qrs1'], features['mel1'], features['cqt1'], features['stft1']])\n",
    "    res2 = model2.predict([features['age'], features['sex'], features['hw'], features['preg'], features['loc'], \n",
    "                           features['rr1'], features['qrs1'], features['mel1'], features['cqt1'], features['stft1']])\n",
    "\n",
    "    # Get classifier probabilities.\n",
    "    if model['ord1'] :\n",
    "        idx1 = res1.argmax(axis=0)[0]\n",
    "        murmur_p = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "        murmur_probabilities = np.zeros((3,))\n",
    "        murmur_probabilities[0] = murmur_p[0]\n",
    "        murmur_probabilities[1] = 0\n",
    "        murmur_probabilities[2] = murmur_p[1]\n",
    "        outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "    else :\n",
    "        if model['mm_mean'] :\n",
    "            murmur_probabilities = res1.mean(axis = 0)\n",
    "        else :\n",
    "            idx1 = res1.argmax(axis=0)[0]\n",
    "            murmur_probabilities = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "        outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "\n",
    "        \n",
    "    ## 이부분도 생각 필요.. rule 을 cost를 maximize 하는 기준으로 threshold 탐색 필요할지도..\n",
    "    # Choose label with highest probability.\n",
    "    murmur_labels = np.zeros(len(murmur_classes), dtype=np.int_)\n",
    "    idx = np.argmax(murmur_probabilities)\n",
    "    murmur_labels[idx] = 1\n",
    "    outcome_labels = np.zeros(len(outcome_classes), dtype=np.int_)\n",
    "    idx = np.argmax(outcome_probabilities)\n",
    "    outcome_labels[idx] = 1\n",
    "\n",
    "    # Concatenate classes, labels, and probabilities.\n",
    "    classes = murmur_classes + outcome_classes\n",
    "    labels = np.concatenate((murmur_labels, outcome_labels))\n",
    "    probabilities = np.concatenate((murmur_probabilities, outcome_probabilities))\n",
    "\n",
    "    return classes, labels, probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d1c4393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_challenge_model(model_folder, model1, model2, m_name1 = 'lcnn_rr_qrs2_1_1', m_name2 = 'lcnn_rr_qrs2_1_2', param_feature = params_feature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73f66a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lcnn__rr_qrs3'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "513b7bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Data2/hmd/data_split/murmur/test'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = test_folder\n",
    "data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff610cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Data2/hmd/hmd_sy/notebooks/out_lcnn__rr_qrs3'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed1bd147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Challenge model...\n",
      "Running model on Challenge data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9ea8041b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9ec419d700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "run_model(model_folder, test_folder, output_folder, allow_failures = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23bcbb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.535,0.812,0.774,17566.761\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.680,0.681,0.655,12840.238\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.725,0.000,0.881\n",
      "Accuracy,0.868,0.000,0.878\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.674,0.687\n",
      "Accuracy,0.643,0.720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "murmur_scores, outcome_scores = evaluate_model(test_folder, output_folder)\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "    + '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "\n",
    "if len(sys.argv) == 3:\n",
    "    print(output_string)\n",
    "elif len(sys.argv) == 4:\n",
    "    with open(sys.argv[3], 'w') as f:\n",
    "        f.write(output_string)\n",
    "#Murmur scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4090c4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3211895880647079\n",
      "0.6788104076640308\n",
      "0.5132315358700456\n",
      "0.4867684615713697\n"
     ]
    }
   ],
   "source": [
    "label_folder = test_folder\n",
    "murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "# Load and parse label and model output files.\n",
    "label_files, output_files = find_challenge_files(label_folder, output_folder)\n",
    "murmur_labels = load_murmurs(label_files, murmur_classes)\n",
    "murmur_binary_outputs, murmur_scalar_outputs = load_classifier_outputs(output_files, murmur_classes)\n",
    "outcome_labels = load_outcomes(label_files, outcome_classes)\n",
    "outcome_binary_outputs, outcome_scalar_outputs = load_classifier_outputs(output_files, outcome_classes)\n",
    "\n",
    "\n",
    "print(np.mean(murmur_scalar_outputs[:,0]))\n",
    "print(np.mean(murmur_scalar_outputs[:,2]))\n",
    "print(np.mean(outcome_scalar_outputs[:,0]))\n",
    "print(np.mean(outcome_scalar_outputs[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c6dc6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold:  0.01\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.184,0.277,0.553,13813.107\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.351,0.518,0.842,14922.040\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.360,0.000,0.191\n",
      "Accuracy,1.000,0.000,0.108\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.681,0.021\n",
      "Accuracy,1.000,0.011\n",
      "\n",
      "-------------\n",
      "threshold:  0.05\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.324,0.471,0.642,13128.996\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.363,0.524,0.844,14706.772\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.433,0.000,0.538\n",
      "Accuracy,0.974,0.000,0.381\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.683,0.042\n",
      "Accuracy,1.000,0.022\n",
      "\n",
      "-------------\n",
      "threshold:  0.1\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.386,0.576,0.685,12942.044\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.371,0.524,0.837,14496.256\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.490,0.000,0.670\n",
      "Accuracy,0.947,0.000,0.532\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.681,0.062\n",
      "Accuracy,0.990,0.032\n",
      "\n",
      "-------------\n",
      "threshold:  0.15\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.415,0.623,0.709,13264.701\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.401,0.534,0.834,13907.105\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.526,0.000,0.719\n",
      "Accuracy,0.947,0.000,0.597\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.683,0.119\n",
      "Accuracy,0.980,0.065\n",
      "\n",
      "-------------\n",
      "threshold:  0.2\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.453,0.686,0.741,13837.593\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.457,0.550,0.811,12815.309\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.581,0.000,0.779\n",
      "Accuracy,0.947,0.000,0.683\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.681,0.232\n",
      "Accuracy,0.939,0.140\n",
      "\n",
      "-------------\n",
      "threshold:  0.25\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.464,0.707,0.741,14593.094\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.474,0.545,0.775,12548.726\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.593,0.000,0.800\n",
      "Accuracy,0.921,0.000,0.719\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.667,0.281\n",
      "Accuracy,0.888,0.183\n",
      "\n",
      "-------------\n",
      "threshold:  0.3\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.481,0.733,0.755,15377.795\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.506,0.555,0.751,12173.980\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.619,0.000,0.824\n",
      "Accuracy,0.921,0.000,0.755\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.661,0.351\n",
      "Accuracy,0.847,0.247\n",
      "\n",
      "-------------\n",
      "threshold:  0.35\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.505,0.770,0.774,15742.156\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.559,0.586,0.741,11647.149\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.660,0.000,0.855\n",
      "Accuracy,0.921,0.000,0.806\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.669,0.448\n",
      "Accuracy,0.816,0.344\n",
      "\n",
      "-------------\n",
      "threshold:  0.4\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.516,0.785,0.782,15934.166\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.631,0.639,0.724,11364.077\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.680,0.000,0.868\n",
      "Accuracy,0.921,0.000,0.827\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.685,0.577\n",
      "Accuracy,0.765,0.505\n",
      "\n",
      "-------------\n",
      "threshold:  0.45\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.525,0.796,0.776,16951.824\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.680,0.681,0.703,11699.544\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.708,0.000,0.868\n",
      "Accuracy,0.895,0.000,0.849\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.697,0.663\n",
      "Accuracy,0.714,0.645\n",
      "\n",
      "-------------\n",
      "threshold:  0.5\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.535,0.812,0.774,17566.761\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.680,0.681,0.655,12840.238\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.725,0.000,0.881\n",
      "Accuracy,0.868,0.000,0.878\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.674,0.687\n",
      "Accuracy,0.643,0.720\n",
      "\n",
      "-------------\n",
      "threshold:  0.55\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.538,0.817,0.765,17561.153\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.662,0.665,0.588,14530.998\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.727,0.000,0.886\n",
      "Accuracy,0.842,0.000,0.892\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.628,0.695\n",
      "Accuracy,0.551,0.785\n",
      "\n",
      "-------------\n",
      "threshold:  0.6\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.534,0.817,0.744,17765.302\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.674,0.681,0.573,15096.470\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.714,0.000,0.887\n",
      "Accuracy,0.789,0.000,0.906\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.626,0.721\n",
      "Accuracy,0.520,0.849\n",
      "\n",
      "-------------\n",
      "threshold:  0.65\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.542,0.827,0.739,18180.224\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.661,0.675,0.530,16312.456\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.734,0.000,0.893\n",
      "Accuracy,0.763,0.000,0.928\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.592,0.730\n",
      "Accuracy,0.459,0.903\n",
      "\n",
      "-------------\n",
      "threshold:  0.7\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.547,0.832,0.720,19224.877\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.641,0.665,0.485,17554.007\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.750,0.000,0.892\n",
      "Accuracy,0.711,0.000,0.950\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.549,0.733\n",
      "Accuracy,0.398,0.946\n",
      "\n",
      "-------------\n",
      "threshold:  0.75\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.557,0.843,0.725,19643.342\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.632,0.665,0.458,18387.658\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.771,0.000,0.899\n",
      "Accuracy,0.711,0.000,0.964\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.522,0.742\n",
      "Accuracy,0.357,0.989\n",
      "\n",
      "-------------\n",
      "threshold:  0.8\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.605,0.550,0.838,0.712,19852.593\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.728,0.724,0.604,0.644,0.424,19224.494\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.934,0.500,0.830\n",
      "AUPRC,0.867,0.073,0.875\n",
      "F-measure,0.754,0.000,0.896\n",
      "Accuracy,0.684,0.000,0.964\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.728,0.728\n",
      "AUPRC,0.783,0.664\n",
      "F-measure,0.477,0.730\n",
      "Accuracy,0.316,0.989\n",
      "\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for th1 in [0.01, 0.05, 0.1, 0.15,0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8] :\n",
    "    murmur_binary_outputs[:,0] = murmur_scalar_outputs[:,0] > th1\n",
    "    murmur_binary_outputs[:,2] = murmur_scalar_outputs[:,2] > 1 - th1\n",
    "    outcome_binary_outputs[:,0] = outcome_scalar_outputs[:,0] > th1\n",
    "    outcome_binary_outputs[:,1] = outcome_scalar_outputs[:,1] > 1 - th1\n",
    "    # For each patient, set the 'Present' or 'Abnormal' class to positive if no class is positive or if multiple classes are positive.\n",
    "    murmur_labels = enforce_positives(murmur_labels, murmur_classes, 'Present')\n",
    "    murmur_binary_outputs = enforce_positives(murmur_binary_outputs, murmur_classes, 'Present')\n",
    "    outcome_labels = enforce_positives(outcome_labels, outcome_classes, 'Abnormal')\n",
    "    outcome_binary_outputs = enforce_positives(outcome_binary_outputs, outcome_classes, 'Abnormal')\n",
    "    # Evaluate the murmur model by comparing the labels and model outputs.\n",
    "    murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes = compute_auc(murmur_labels, murmur_scalar_outputs)\n",
    "    murmur_f_measure, murmur_f_measure_classes = compute_f_measure(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_accuracy, murmur_accuracy_classes = compute_accuracy(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_weighted_accuracy = compute_weighted_accuracy(murmur_labels, murmur_binary_outputs, murmur_classes) # This is the murmur scoring metric.\n",
    "    murmur_cost = compute_cost(outcome_labels, murmur_binary_outputs, outcome_classes, murmur_classes) # Use *outcomes* to score *murmurs* for the Challenge cost metric, but this is not the actual murmur scoring metric.\n",
    "    murmur_scores = (murmur_classes, murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes, \\\n",
    "                 murmur_f_measure, murmur_f_measure_classes, murmur_accuracy, murmur_accuracy_classes, murmur_weighted_accuracy, murmur_cost)\n",
    "\n",
    "    # Evaluate the outcome model by comparing the labels and model outputs.\n",
    "    outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes = compute_auc(outcome_labels, outcome_scalar_outputs)\n",
    "    outcome_f_measure, outcome_f_measure_classes = compute_f_measure(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_accuracy, outcome_accuracy_classes = compute_accuracy(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_weighted_accuracy = compute_weighted_accuracy(outcome_labels, outcome_binary_outputs, outcome_classes)\n",
    "    outcome_cost = compute_cost(outcome_labels, outcome_binary_outputs, outcome_classes, outcome_classes) # This is the clinical outcomes scoring metric.\n",
    "    outcome_scores = (outcome_classes, outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes, \\\n",
    "                  outcome_f_measure, outcome_f_measure_classes, outcome_accuracy, outcome_accuracy_classes, outcome_weighted_accuracy, outcome_cost)\n",
    "\n",
    "\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "    murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "    outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "                + '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "    print(\"threshold: \", th1)\n",
    "    print(output_string)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bd0be5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

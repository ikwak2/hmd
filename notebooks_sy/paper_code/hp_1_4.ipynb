{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd5b7218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 24 13:17:49 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     On   | 00000000:1A:00.0 Off |                    0 |\n",
      "| 33%   47C    P2    65W / 260W |  39631MiB / 46080MiB |     21%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    On   | 00000000:1D:00.0 Off |                    0 |\n",
      "| 30%   28C    P8    14W / 300W |      3MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    On   | 00000000:1E:00.0 Off |                    0 |\n",
      "| 30%   28C    P8    27W / 300W |      3MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro RTX 8000     On   | 00000000:3D:00.0 Off |                    0 |\n",
      "| 33%   25C    P8     5W / 260W |      3MiB / 46080MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Quadro RTX 8000     On   | 00000000:3E:00.0 Off |                    0 |\n",
      "| 33%   27C    P8    13W / 260W |      3MiB / 46080MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Quadro RTX 8000     On   | 00000000:41:00.0 Off |                    0 |\n",
      "| 33%   25C    P8     4W / 260W |      3MiB / 46080MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0357717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'samp_sec': 50, 'pre_emphasis': 0, 'hop_length': 256, 'win_length': 512, 'n_mels': 140, 'filter_scale': 1, 'n_bins': 80, 'fmin': 10, 'trim': 0, 'use_rr': True, 'use_b_detect': True, 'use_raw': False, 'use_mel': True, 'use_cqt': False, 'use_stft': False, 'ord1': True, 'mm_mean': False, 'dp': 0, 'fc': False, 'ext': True, 'oo_weight': 3, 'mm_weight': 3, 'chaug': 10, 'cout': 0.8, 'wunknown': 1, 'mixup': True, 'n1': 0, 'mel_shape': (140, 782, 1), 'cqt_shape': (1, 1, 1), 'stft_shape': (1, 1, 1), 's1s2_shape': (100, 313, 1), 'mm_shape': (100, 313, 1), 'envel_shape': (100, 313, 1), 'use_s1s2': False, 'use_mm': False, 'use_envel': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 13:20:01.868999: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-24 13:20:03.200663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 44177 MB memory:  -> device: 5, name: Quadro RTX 8000, pci bus id: 0000:41:00.0, compute capability: 7.5\n",
      "2023-05-24 13:20:05.380669: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 13:20:10.706772: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 0s - loss: 1.1451 - accuracy: 0.7281 - auc: 0.7684"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 13:21:11.472916: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 276327520 exceeds 10% of free system memory.\n",
      "2023-05-24 13:21:11.620465: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 276327520 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 74s 2s/step - loss: 1.1451 - accuracy: 0.7281 - auc: 0.7684 - val_loss: 0.7425 - val_accuracy: 0.7892 - val_auc: 0.8567\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.8857 - accuracy: 0.7742 - auc: 0.8488 - val_loss: 1.0414 - val_accuracy: 0.2631 - val_auc: 0.2237\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.8675 - accuracy: 0.8047 - auc: 0.8624 - val_loss: 0.5330 - val_accuracy: 0.8193 - val_auc: 0.8607\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.8259 - accuracy: 0.8094 - auc: 0.8493 - val_loss: 1.1984 - val_accuracy: 0.2139 - val_auc: 0.2943\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.7843 - accuracy: 0.8250 - auc: 0.8699 - val_loss: 0.4262 - val_accuracy: 0.8415 - val_auc: 0.8990\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.7802 - accuracy: 0.8160 - auc: 0.8719 - val_loss: 0.3846 - val_accuracy: 0.8526 - val_auc: 0.9179\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.7408 - accuracy: 0.8531 - auc: 0.8830 - val_loss: 0.9242 - val_accuracy: 0.2979 - val_auc: 0.3553\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.7411 - accuracy: 0.8398 - auc: 0.8725 - val_loss: 0.4728 - val_accuracy: 0.8526 - val_auc: 0.9083\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7337 - accuracy: 0.8406 - auc: 0.8788 - val_loss: 0.4691 - val_accuracy: 0.8320 - val_auc: 0.8986\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.7261 - accuracy: 0.8559 - auc: 0.8798 - val_loss: 0.4228 - val_accuracy: 0.8415 - val_auc: 0.9119\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.7263 - accuracy: 0.8516 - auc: 0.8800 - val_loss: 0.3763 - val_accuracy: 0.8732 - val_auc: 0.9336\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.6758 - accuracy: 0.8730 - auc: 0.8915 - val_loss: 0.6365 - val_accuracy: 0.5721 - val_auc: 0.6756\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.6800 - accuracy: 0.8668 - auc: 0.8799 - val_loss: 1.2433 - val_accuracy: 0.8352 - val_auc: 0.8571\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.7094 - accuracy: 0.8676 - auc: 0.8869 - val_loss: 0.7356 - val_accuracy: 0.8542 - val_auc: 0.9007\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.7062 - accuracy: 0.8578 - auc: 0.8809 - val_loss: 0.4479 - val_accuracy: 0.8574 - val_auc: 0.9193\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.7015 - accuracy: 0.8691 - auc: 0.8877 - val_loss: 0.4097 - val_accuracy: 0.8780 - val_auc: 0.9295\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.6910 - accuracy: 0.8555 - auc: 0.8778 - val_loss: 0.9232 - val_accuracy: 0.3724 - val_auc: 0.4144\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.7094 - accuracy: 0.8516 - auc: 0.8834 - val_loss: 0.4705 - val_accuracy: 0.8162 - val_auc: 0.8996\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.7122 - accuracy: 0.8461 - auc: 0.8723 - val_loss: 0.4524 - val_accuracy: 0.8637 - val_auc: 0.9214\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.6739 - accuracy: 0.8723 - auc: 0.8870 - val_loss: 0.3717 - val_accuracy: 0.8621 - val_auc: 0.9278\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.6955 - accuracy: 0.8734 - auc: 0.8887 - val_loss: 0.3295 - val_accuracy: 0.8764 - val_auc: 0.9375\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.6599 - accuracy: 0.8719 - auc: 0.8905 - val_loss: 1.0070 - val_accuracy: 0.3645 - val_auc: 0.3943\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.6810 - accuracy: 0.8684 - auc: 0.8933 - val_loss: 0.9314 - val_accuracy: 0.2393 - val_auc: 0.3251\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.6560 - accuracy: 0.8797 - auc: 0.8872 - val_loss: 0.5143 - val_accuracy: 0.7575 - val_auc: 0.8308\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.6306 - accuracy: 0.8883 - auc: 0.8969 - val_loss: 0.8923 - val_accuracy: 0.4057 - val_auc: 0.4112\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.6428 - accuracy: 0.8703 - auc: 0.8846 - val_loss: 0.3482 - val_accuracy: 0.8827 - val_auc: 0.9315\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.6414 - accuracy: 0.8777 - auc: 0.8986 - val_loss: 0.8247 - val_accuracy: 0.4564 - val_auc: 0.5197\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.6552 - accuracy: 0.8766 - auc: 0.8938 - val_loss: 0.6924 - val_accuracy: 0.5610 - val_auc: 0.6198\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.6659 - accuracy: 0.8703 - auc: 0.8918 - val_loss: 0.3813 - val_accuracy: 0.8780 - val_auc: 0.9331\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.6430 - accuracy: 0.8754 - auc: 0.8957 - val_loss: 0.3483 - val_accuracy: 0.8796 - val_auc: 0.9319\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.6201 - accuracy: 0.8832 - auc: 0.9004 - val_loss: 0.3439 - val_accuracy: 0.8732 - val_auc: 0.9334\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.6170 - accuracy: 0.8867 - auc: 0.8980 - val_loss: 0.3236 - val_accuracy: 0.8843 - val_auc: 0.9388\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.6122 - accuracy: 0.8930 - auc: 0.9056 - val_loss: 1.0869 - val_accuracy: 0.2916 - val_auc: 0.3277\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.6513 - accuracy: 0.8840 - auc: 0.8861 - val_loss: 0.7262 - val_accuracy: 0.4057 - val_auc: 0.4509\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.6055 - accuracy: 0.8910 - auc: 0.8886 - val_loss: 0.6130 - val_accuracy: 0.6371 - val_auc: 0.7324\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.6299 - accuracy: 0.8957 - auc: 0.9001 - val_loss: 0.4075 - val_accuracy: 0.8637 - val_auc: 0.9203\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.6408 - accuracy: 0.8801 - auc: 0.8945 - val_loss: 0.5589 - val_accuracy: 0.7147 - val_auc: 0.7898\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.6099 - accuracy: 0.8914 - auc: 0.8974 - val_loss: 1.1128 - val_accuracy: 0.2520 - val_auc: 0.3431\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.6085 - accuracy: 0.8863 - auc: 0.8931 - val_loss: 0.3671 - val_accuracy: 0.8827 - val_auc: 0.9311\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.6027 - accuracy: 0.8980 - auc: 0.8955 - val_loss: 0.3713 - val_accuracy: 0.8796 - val_auc: 0.9250\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.6105 - accuracy: 0.8926 - auc: 0.8935 - val_loss: 0.3587 - val_accuracy: 0.8796 - val_auc: 0.9308\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.5952 - accuracy: 0.8992 - auc: 0.8981 - val_loss: 0.5116 - val_accuracy: 0.7496 - val_auc: 0.8352\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.6160 - accuracy: 0.8816 - auc: 0.8999 - val_loss: 0.4077 - val_accuracy: 0.8273 - val_auc: 0.9039\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.6200 - accuracy: 0.8832 - auc: 0.8947 - val_loss: 0.4839 - val_accuracy: 0.7639 - val_auc: 0.8511\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.6109 - accuracy: 0.8949 - auc: 0.9035 - val_loss: 0.5227 - val_accuracy: 0.7448 - val_auc: 0.8346\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.5860 - accuracy: 0.9059 - auc: 0.9017 - val_loss: 0.3629 - val_accuracy: 0.8621 - val_auc: 0.9310\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.5952 - accuracy: 0.8984 - auc: 0.9047 - val_loss: 0.6468 - val_accuracy: 0.8526 - val_auc: 0.9030\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 60s 1s/step - loss: 0.5767 - accuracy: 0.9012 - auc: 0.9071 - val_loss: 0.3500 - val_accuracy: 0.8811 - val_auc: 0.9305\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.6133 - accuracy: 0.8938 - auc: 0.8997 - val_loss: 0.3873 - val_accuracy: 0.8558 - val_auc: 0.9227\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.5965 - accuracy: 0.9039 - auc: 0.9003 - val_loss: 0.4721 - val_accuracy: 0.7845 - val_auc: 0.8651\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.5742 - accuracy: 0.9051 - auc: 0.9062 - val_loss: 0.5181 - val_accuracy: 0.7655 - val_auc: 0.8337\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.5513 - accuracy: 0.9133 - auc: 0.9124 - val_loss: 0.4324 - val_accuracy: 0.8320 - val_auc: 0.8920\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.5620 - accuracy: 0.9066 - auc: 0.9059 - val_loss: 0.4078 - val_accuracy: 0.8590 - val_auc: 0.9111\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.5777 - accuracy: 0.9020 - auc: 0.9033 - val_loss: 0.5661 - val_accuracy: 0.7179 - val_auc: 0.7860\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.5385 - accuracy: 0.9191 - auc: 0.9059 - val_loss: 0.3783 - val_accuracy: 0.8875 - val_auc: 0.9294\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.5679 - accuracy: 0.9090 - auc: 0.9106 - val_loss: 0.3864 - val_accuracy: 0.8859 - val_auc: 0.9259\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.5311 - accuracy: 0.9262 - auc: 0.9089 - val_loss: 0.4689 - val_accuracy: 0.8716 - val_auc: 0.9182\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.5319 - accuracy: 0.9270 - auc: 0.9106 - val_loss: 0.5321 - val_accuracy: 0.8700 - val_auc: 0.9160\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.5251 - accuracy: 0.9219 - auc: 0.9112 - val_loss: 0.3499 - val_accuracy: 0.8843 - val_auc: 0.9328\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.5278 - accuracy: 0.9273 - auc: 0.9129 - val_loss: 0.3594 - val_accuracy: 0.8796 - val_auc: 0.9285\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.5164 - accuracy: 0.9223 - auc: 0.9139 - val_loss: 0.3541 - val_accuracy: 0.8859 - val_auc: 0.9342\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.5345 - accuracy: 0.9215 - auc: 0.9169 - val_loss: 0.4802 - val_accuracy: 0.8700 - val_auc: 0.9234\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.5252 - accuracy: 0.9293 - auc: 0.9123 - val_loss: 0.4336 - val_accuracy: 0.8748 - val_auc: 0.9189\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.4976 - accuracy: 0.9332 - auc: 0.9168 - val_loss: 0.4770 - val_accuracy: 0.8732 - val_auc: 0.9204\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.5178 - accuracy: 0.9250 - auc: 0.9097 - val_loss: 0.3806 - val_accuracy: 0.8700 - val_auc: 0.9274\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.5008 - accuracy: 0.9324 - auc: 0.9203 - val_loss: 0.3660 - val_accuracy: 0.8764 - val_auc: 0.9310\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.5053 - accuracy: 0.9383 - auc: 0.9175 - val_loss: 0.3614 - val_accuracy: 0.8891 - val_auc: 0.9344\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.5081 - accuracy: 0.9414 - auc: 0.9151 - val_loss: 0.3804 - val_accuracy: 0.8748 - val_auc: 0.9288\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.5172 - accuracy: 0.9285 - auc: 0.9121 - val_loss: 0.3981 - val_accuracy: 0.8685 - val_auc: 0.9246\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.5118 - accuracy: 0.9301 - auc: 0.9117 - val_loss: 0.3949 - val_accuracy: 0.8590 - val_auc: 0.9185\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.5170 - accuracy: 0.9336 - auc: 0.9143 - val_loss: 0.4018 - val_accuracy: 0.8764 - val_auc: 0.9259\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.5291 - accuracy: 0.9328 - auc: 0.9165 - val_loss: 0.3982 - val_accuracy: 0.8653 - val_auc: 0.9223\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.5017 - accuracy: 0.9438 - auc: 0.9130 - val_loss: 0.3934 - val_accuracy: 0.8653 - val_auc: 0.9242\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.4837 - accuracy: 0.9332 - auc: 0.9223 - val_loss: 0.4014 - val_accuracy: 0.8590 - val_auc: 0.9219\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.4878 - accuracy: 0.9355 - auc: 0.9204 - val_loss: 0.4346 - val_accuracy: 0.8669 - val_auc: 0.9222\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.5002 - accuracy: 0.9340 - auc: 0.9155 - val_loss: 0.4080 - val_accuracy: 0.8621 - val_auc: 0.9198\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.4838 - accuracy: 0.9430 - auc: 0.9187 - val_loss: 0.4223 - val_accuracy: 0.8669 - val_auc: 0.9213\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.5036 - accuracy: 0.9383 - auc: 0.9193 - val_loss: 0.4140 - val_accuracy: 0.8685 - val_auc: 0.9218\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.4890 - accuracy: 0.9398 - auc: 0.9247 - val_loss: 0.4032 - val_accuracy: 0.8716 - val_auc: 0.9246\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.4759 - accuracy: 0.9449 - auc: 0.9210 - val_loss: 0.4022 - val_accuracy: 0.8700 - val_auc: 0.9252\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.5033 - accuracy: 0.9406 - auc: 0.9179 - val_loss: 0.3998 - val_accuracy: 0.8669 - val_auc: 0.9246\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.5005 - accuracy: 0.9418 - auc: 0.9147 - val_loss: 0.4146 - val_accuracy: 0.8764 - val_auc: 0.9257\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.4919 - accuracy: 0.9422 - auc: 0.9240 - val_loss: 0.4168 - val_accuracy: 0.8764 - val_auc: 0.9259\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.5012 - accuracy: 0.9445 - auc: 0.9227 - val_loss: 0.4013 - val_accuracy: 0.8637 - val_auc: 0.9252\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.4825 - accuracy: 0.9398 - auc: 0.9186 - val_loss: 0.4027 - val_accuracy: 0.8653 - val_auc: 0.9237\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.4911 - accuracy: 0.9426 - auc: 0.9239 - val_loss: 0.3988 - val_accuracy: 0.8637 - val_auc: 0.9228\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.4951 - accuracy: 0.9441 - auc: 0.9207 - val_loss: 0.3942 - val_accuracy: 0.8685 - val_auc: 0.9228\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.5124 - accuracy: 0.9289 - auc: 0.9200 - val_loss: 0.3943 - val_accuracy: 0.8653 - val_auc: 0.9226\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.4971 - accuracy: 0.9414 - auc: 0.9222 - val_loss: 0.4040 - val_accuracy: 0.8669 - val_auc: 0.9216\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.4888 - accuracy: 0.9457 - auc: 0.9157 - val_loss: 0.3998 - val_accuracy: 0.8669 - val_auc: 0.9249\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.4994 - accuracy: 0.9352 - auc: 0.9217 - val_loss: 0.3998 - val_accuracy: 0.8669 - val_auc: 0.9251\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.4962 - accuracy: 0.9398 - auc: 0.9216 - val_loss: 0.4008 - val_accuracy: 0.8653 - val_auc: 0.9250\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.4758 - accuracy: 0.9438 - auc: 0.9203 - val_loss: 0.4095 - val_accuracy: 0.8700 - val_auc: 0.9253\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.4891 - accuracy: 0.9387 - auc: 0.9226 - val_loss: 0.4077 - val_accuracy: 0.8637 - val_auc: 0.9256\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 60s 1s/step - loss: 0.4761 - accuracy: 0.9480 - auc: 0.9164 - val_loss: 0.4068 - val_accuracy: 0.8653 - val_auc: 0.9257\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.4954 - accuracy: 0.9441 - auc: 0.9220 - val_loss: 0.3999 - val_accuracy: 0.8669 - val_auc: 0.9251\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.4909 - accuracy: 0.9387 - auc: 0.9158 - val_loss: 0.4069 - val_accuracy: 0.8669 - val_auc: 0.9246\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.4972 - accuracy: 0.9367 - auc: 0.9220 - val_loss: 0.4009 - val_accuracy: 0.8637 - val_auc: 0.9246\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.4841 - accuracy: 0.9441 - auc: 0.9203 - val_loss: 0.3979 - val_accuracy: 0.8637 - val_auc: 0.9248\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 59s 1s/step - loss: 0.4963 - accuracy: 0.9434 - auc: 0.9252 - val_loss: 0.4052 - val_accuracy: 0.8716 - val_auc: 0.9247\n",
      "Epoch 1/100\n",
      "40/40 [==============================] - ETA: 0s - loss: 3.3001 - accuracy: 0.5020 - auc: 0.5213"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 14:59:55.768270: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 276327520 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 18s 384ms/step - loss: 3.3001 - accuracy: 0.5020 - auc: 0.5213 - val_loss: 27.9774 - val_accuracy: 0.4992 - val_auc: 0.4992\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 1.4160 - accuracy: 0.4750 - auc: 0.4927 - val_loss: 6.2275 - val_accuracy: 0.4992 - val_auc: 0.4992\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 1.3641 - accuracy: 0.4914 - auc: 0.4849 - val_loss: 1.8328 - val_accuracy: 0.4992 - val_auc: 0.4824\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 1.2862 - accuracy: 0.4898 - auc: 0.5128 - val_loss: 0.9465 - val_accuracy: 0.4929 - val_auc: 0.5726\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.2542 - accuracy: 0.5012 - auc: 0.5214 - val_loss: 0.7363 - val_accuracy: 0.5452 - val_auc: 0.5839\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 1.2635 - accuracy: 0.4941 - auc: 0.5096 - val_loss: 0.8070 - val_accuracy: 0.5040 - val_auc: 0.5821\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 1.2339 - accuracy: 0.5117 - auc: 0.5237 - val_loss: 1.3486 - val_accuracy: 0.4992 - val_auc: 0.5811\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 1.2383 - accuracy: 0.4875 - auc: 0.5126 - val_loss: 0.8715 - val_accuracy: 0.5008 - val_auc: 0.5700\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 14s 339ms/step - loss: 1.2462 - accuracy: 0.4887 - auc: 0.5017 - val_loss: 0.9639 - val_accuracy: 0.4992 - val_auc: 0.5874\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 1.2170 - accuracy: 0.4969 - auc: 0.5216 - val_loss: 0.7325 - val_accuracy: 0.4897 - val_auc: 0.5896\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.2241 - accuracy: 0.4785 - auc: 0.5140 - val_loss: 0.7199 - val_accuracy: 0.4976 - val_auc: 0.5931\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 1.2169 - accuracy: 0.4852 - auc: 0.5202 - val_loss: 0.7933 - val_accuracy: 0.4992 - val_auc: 0.5976\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 1.1819 - accuracy: 0.4707 - auc: 0.5319 - val_loss: 0.7689 - val_accuracy: 0.4992 - val_auc: 0.5987\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 1.2038 - accuracy: 0.4867 - auc: 0.5247 - val_loss: 0.7959 - val_accuracy: 0.4992 - val_auc: 0.5904\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 13s 339ms/step - loss: 1.1962 - accuracy: 0.4723 - auc: 0.5131 - val_loss: 0.6638 - val_accuracy: 0.5452 - val_auc: 0.6021\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 1.2061 - accuracy: 0.4668 - auc: 0.5132 - val_loss: 0.6853 - val_accuracy: 0.4976 - val_auc: 0.6030\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 14s 339ms/step - loss: 1.2057 - accuracy: 0.4730 - auc: 0.5200 - val_loss: 0.7218 - val_accuracy: 0.4992 - val_auc: 0.5741\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 1.2034 - accuracy: 0.4867 - auc: 0.5364 - val_loss: 0.7215 - val_accuracy: 0.5008 - val_auc: 0.5908\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 1.2059 - accuracy: 0.5066 - auc: 0.5320 - val_loss: 0.7312 - val_accuracy: 0.5008 - val_auc: 0.5992\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 1.1878 - accuracy: 0.4723 - auc: 0.5308 - val_loss: 0.8165 - val_accuracy: 0.5008 - val_auc: 0.5888\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.1997 - accuracy: 0.4863 - auc: 0.5334 - val_loss: 0.7134 - val_accuracy: 0.4992 - val_auc: 0.5862\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 1.1938 - accuracy: 0.4801 - auc: 0.5330 - val_loss: 0.7088 - val_accuracy: 0.5040 - val_auc: 0.5803\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 14s 339ms/step - loss: 1.1809 - accuracy: 0.4809 - auc: 0.5330 - val_loss: 0.6390 - val_accuracy: 0.6656 - val_auc: 0.6979\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 1.1839 - accuracy: 0.4914 - auc: 0.5413 - val_loss: 0.7765 - val_accuracy: 0.5008 - val_auc: 0.5979\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 1.2116 - accuracy: 0.4980 - auc: 0.5275 - val_loss: 0.6464 - val_accuracy: 0.5800 - val_auc: 0.6462\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 1.1985 - accuracy: 0.4984 - auc: 0.5294 - val_loss: 0.6938 - val_accuracy: 0.5008 - val_auc: 0.6120\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 13s 329ms/step - loss: 1.2027 - accuracy: 0.4926 - auc: 0.5457 - val_loss: 0.7056 - val_accuracy: 0.4992 - val_auc: 0.6050\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.1721 - accuracy: 0.4910 - auc: 0.5492 - val_loss: 0.6228 - val_accuracy: 0.6307 - val_auc: 0.6867\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 1.1837 - accuracy: 0.4801 - auc: 0.5341 - val_loss: 0.7644 - val_accuracy: 0.4992 - val_auc: 0.5930\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 13s 330ms/step - loss: 1.1816 - accuracy: 0.5016 - auc: 0.5497 - val_loss: 0.6774 - val_accuracy: 0.5832 - val_auc: 0.6095\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 1.1717 - accuracy: 0.4984 - auc: 0.5510 - val_loss: 0.6591 - val_accuracy: 0.5515 - val_auc: 0.5996\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 1.1871 - accuracy: 0.4910 - auc: 0.5302 - val_loss: 0.7809 - val_accuracy: 0.5008 - val_auc: 0.5904\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.1943 - accuracy: 0.5008 - auc: 0.5465 - val_loss: 0.6653 - val_accuracy: 0.6117 - val_auc: 0.6525\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 1.1746 - accuracy: 0.5066 - auc: 0.5507 - val_loss: 0.6913 - val_accuracy: 0.5151 - val_auc: 0.5785\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 13s 330ms/step - loss: 1.1792 - accuracy: 0.5004 - auc: 0.5531 - val_loss: 0.6454 - val_accuracy: 0.5689 - val_auc: 0.6340\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 13s 339ms/step - loss: 1.1792 - accuracy: 0.4961 - auc: 0.5385 - val_loss: 0.7200 - val_accuracy: 0.5071 - val_auc: 0.5991\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 1.1853 - accuracy: 0.4941 - auc: 0.5406 - val_loss: 0.8945 - val_accuracy: 0.4992 - val_auc: 0.5631\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 13s 330ms/step - loss: 1.1846 - accuracy: 0.4996 - auc: 0.5534 - val_loss: 0.7610 - val_accuracy: 0.4992 - val_auc: 0.6013\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 13s 339ms/step - loss: 1.1697 - accuracy: 0.5016 - auc: 0.5573 - val_loss: 0.7148 - val_accuracy: 0.4976 - val_auc: 0.5994\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 1.1775 - accuracy: 0.5215 - auc: 0.5604 - val_loss: 0.6788 - val_accuracy: 0.5166 - val_auc: 0.5971\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.1636 - accuracy: 0.5320 - auc: 0.5678 - val_loss: 0.6443 - val_accuracy: 0.5911 - val_auc: 0.6510\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 1.1722 - accuracy: 0.5547 - auc: 0.5722 - val_loss: 1.9265 - val_accuracy: 0.5008 - val_auc: 0.5810\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 1.1478 - accuracy: 0.5242 - auc: 0.5767 - val_loss: 0.6258 - val_accuracy: 0.6561 - val_auc: 0.7071\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 1.1542 - accuracy: 0.5457 - auc: 0.5766 - val_loss: 0.9513 - val_accuracy: 0.5483 - val_auc: 0.5558\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 1.1553 - accuracy: 0.5219 - auc: 0.5722 - val_loss: 0.8127 - val_accuracy: 0.4992 - val_auc: 0.5825\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 1.1528 - accuracy: 0.5199 - auc: 0.5639 - val_loss: 0.6281 - val_accuracy: 0.6371 - val_auc: 0.7027\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 13s 328ms/step - loss: 1.1647 - accuracy: 0.5301 - auc: 0.5697 - val_loss: 0.9830 - val_accuracy: 0.5452 - val_auc: 0.6068\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 1.1451 - accuracy: 0.5488 - auc: 0.5903 - val_loss: 0.6793 - val_accuracy: 0.5531 - val_auc: 0.6206\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 1.1607 - accuracy: 0.5414 - auc: 0.5781 - val_loss: 0.7903 - val_accuracy: 0.5468 - val_auc: 0.5744\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 1.1448 - accuracy: 0.5762 - auc: 0.5929 - val_loss: 0.6719 - val_accuracy: 0.6149 - val_auc: 0.6543\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 1.1552 - accuracy: 0.5398 - auc: 0.5721 - val_loss: 0.7959 - val_accuracy: 0.5880 - val_auc: 0.6035\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 1.1218 - accuracy: 0.5762 - auc: 0.6092 - val_loss: 0.6561 - val_accuracy: 0.6022 - val_auc: 0.6528\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 1.1430 - accuracy: 0.5637 - auc: 0.5895 - val_loss: 0.6728 - val_accuracy: 0.5880 - val_auc: 0.6451\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 1.1261 - accuracy: 0.5609 - auc: 0.5991 - val_loss: 0.6665 - val_accuracy: 0.5689 - val_auc: 0.6287\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 1.1264 - accuracy: 0.5859 - auc: 0.6020 - val_loss: 1.0687 - val_accuracy: 0.5309 - val_auc: 0.5846\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 1.1194 - accuracy: 0.6000 - auc: 0.6169 - val_loss: 0.7652 - val_accuracy: 0.5293 - val_auc: 0.5957\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 1.1213 - accuracy: 0.6039 - auc: 0.6308 - val_loss: 1.0249 - val_accuracy: 0.5531 - val_auc: 0.5979\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 1.1181 - accuracy: 0.5910 - auc: 0.6270 - val_loss: 0.7055 - val_accuracy: 0.5483 - val_auc: 0.6078\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 1.0979 - accuracy: 0.5949 - auc: 0.6277 - val_loss: 0.6820 - val_accuracy: 0.5610 - val_auc: 0.6290\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 13s 339ms/step - loss: 1.1045 - accuracy: 0.6184 - auc: 0.6459 - val_loss: 0.6670 - val_accuracy: 0.6181 - val_auc: 0.6527\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 13s 330ms/step - loss: 1.0889 - accuracy: 0.6281 - auc: 0.6434 - val_loss: 0.6619 - val_accuracy: 0.6117 - val_auc: 0.6668\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 1.0940 - accuracy: 0.6297 - auc: 0.6616 - val_loss: 0.6452 - val_accuracy: 0.6387 - val_auc: 0.6840\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 14s 339ms/step - loss: 1.0935 - accuracy: 0.6457 - auc: 0.6402 - val_loss: 0.6663 - val_accuracy: 0.6149 - val_auc: 0.6702\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 1.0649 - accuracy: 0.6535 - auc: 0.6740 - val_loss: 0.9021 - val_accuracy: 0.5024 - val_auc: 0.5765\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 1.0710 - accuracy: 0.6594 - auc: 0.6561 - val_loss: 0.8188 - val_accuracy: 0.5246 - val_auc: 0.5743\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 13s 327ms/step - loss: 1.0480 - accuracy: 0.6723 - auc: 0.6801 - val_loss: 0.6684 - val_accuracy: 0.6165 - val_auc: 0.6556\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 1.0545 - accuracy: 0.6711 - auc: 0.6858 - val_loss: 0.7804 - val_accuracy: 0.5515 - val_auc: 0.5953\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 1.0135 - accuracy: 0.6727 - auc: 0.6979 - val_loss: 0.6908 - val_accuracy: 0.6133 - val_auc: 0.6492\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 1.0176 - accuracy: 0.6898 - auc: 0.7077 - val_loss: 0.7007 - val_accuracy: 0.6260 - val_auc: 0.6512\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 1.0209 - accuracy: 0.6934 - auc: 0.6944 - val_loss: 0.7444 - val_accuracy: 0.5531 - val_auc: 0.6096\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.0276 - accuracy: 0.6906 - auc: 0.7029 - val_loss: 0.7763 - val_accuracy: 0.5388 - val_auc: 0.6020\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.0308 - accuracy: 0.7004 - auc: 0.7089 - val_loss: 0.8043 - val_accuracy: 0.5499 - val_auc: 0.5967\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 13s 329ms/step - loss: 1.0062 - accuracy: 0.6902 - auc: 0.7032 - val_loss: 0.7651 - val_accuracy: 0.5547 - val_auc: 0.6051\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 1.0140 - accuracy: 0.7020 - auc: 0.7095 - val_loss: 0.7590 - val_accuracy: 0.5578 - val_auc: 0.6122\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 0.9981 - accuracy: 0.7211 - auc: 0.7166 - val_loss: 0.7312 - val_accuracy: 0.5816 - val_auc: 0.6292\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 13s 328ms/step - loss: 1.0179 - accuracy: 0.7105 - auc: 0.7120 - val_loss: 0.7468 - val_accuracy: 0.5784 - val_auc: 0.6230\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 0.9997 - accuracy: 0.7254 - auc: 0.7230 - val_loss: 0.7434 - val_accuracy: 0.5911 - val_auc: 0.6289\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 0.9875 - accuracy: 0.7199 - auc: 0.7228 - val_loss: 0.7283 - val_accuracy: 0.5864 - val_auc: 0.6342\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 14s 339ms/step - loss: 1.0037 - accuracy: 0.7180 - auc: 0.7276 - val_loss: 0.7482 - val_accuracy: 0.5563 - val_auc: 0.6112\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 0.9719 - accuracy: 0.7246 - auc: 0.7287 - val_loss: 0.7847 - val_accuracy: 0.5404 - val_auc: 0.5998\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 1.0197 - accuracy: 0.7184 - auc: 0.7166 - val_loss: 0.7761 - val_accuracy: 0.5468 - val_auc: 0.6111\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 13s 325ms/step - loss: 0.9995 - accuracy: 0.7320 - auc: 0.7171 - val_loss: 0.7738 - val_accuracy: 0.5483 - val_auc: 0.6153\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 0.9728 - accuracy: 0.7254 - auc: 0.7357 - val_loss: 0.7465 - val_accuracy: 0.5895 - val_auc: 0.6312\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 0.9653 - accuracy: 0.7188 - auc: 0.7410 - val_loss: 0.7670 - val_accuracy: 0.5721 - val_auc: 0.6175\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 0.9593 - accuracy: 0.7379 - auc: 0.7379 - val_loss: 0.8029 - val_accuracy: 0.5642 - val_auc: 0.5966\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 0.9821 - accuracy: 0.7262 - auc: 0.7236 - val_loss: 0.8085 - val_accuracy: 0.5420 - val_auc: 0.5891\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 0.9887 - accuracy: 0.7273 - auc: 0.7272 - val_loss: 0.7673 - val_accuracy: 0.5784 - val_auc: 0.6227\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 0.9937 - accuracy: 0.7293 - auc: 0.7301 - val_loss: 0.7809 - val_accuracy: 0.5563 - val_auc: 0.6120\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 0.9916 - accuracy: 0.7441 - auc: 0.7411 - val_loss: 0.8084 - val_accuracy: 0.5547 - val_auc: 0.6010\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 0.9778 - accuracy: 0.7336 - auc: 0.7349 - val_loss: 0.7770 - val_accuracy: 0.5642 - val_auc: 0.6187\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 0.9509 - accuracy: 0.7395 - auc: 0.7417 - val_loss: 0.8337 - val_accuracy: 0.5420 - val_auc: 0.5937\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 0.9531 - accuracy: 0.7359 - auc: 0.7448 - val_loss: 0.7899 - val_accuracy: 0.5499 - val_auc: 0.6088\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 14s 341ms/step - loss: 0.9820 - accuracy: 0.7203 - auc: 0.7281 - val_loss: 0.7567 - val_accuracy: 0.5800 - val_auc: 0.6295\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 0.9706 - accuracy: 0.7336 - auc: 0.7340 - val_loss: 0.7830 - val_accuracy: 0.5515 - val_auc: 0.6136\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 14s 349ms/step - loss: 0.9825 - accuracy: 0.7320 - auc: 0.7287 - val_loss: 0.7954 - val_accuracy: 0.5452 - val_auc: 0.6040\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 0.9767 - accuracy: 0.7480 - auc: 0.7447 - val_loss: 0.8144 - val_accuracy: 0.5404 - val_auc: 0.5994\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 0.9668 - accuracy: 0.7430 - auc: 0.7382 - val_loss: 0.7977 - val_accuracy: 0.5547 - val_auc: 0.6033\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 13s 328ms/step - loss: 1.0049 - accuracy: 0.7320 - auc: 0.7245 - val_loss: 0.7976 - val_accuracy: 0.5483 - val_auc: 0.6052\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 13s 339ms/step - loss: 0.9631 - accuracy: 0.7500 - auc: 0.7434 - val_loss: 0.7891 - val_accuracy: 0.5483 - val_auc: 0.6072\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 0.9431 - accuracy: 0.7477 - auc: 0.7553 - val_loss: 0.7924 - val_accuracy: 0.5468 - val_auc: 0.6088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Challenge model...\n",
      "Running model on Challenge data...\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f1e1829b550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f1e18125940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.signal import hilbert\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import librosa\n",
    "import librosa.display\n",
    "import math\n",
    "import sys\n",
    "sys.path.insert(0,'/Data/hmd/hmd_sy/evaluation-2022')\n",
    "sys.path.insert(0,'/Data/hmd/hmd_sy/notebooks')\n",
    "sys.path.insert(0,'utils')\n",
    "from helper_code import *\n",
    "from get_feature import *\n",
    "from models import *\n",
    "from Generator0 import *\n",
    "\n",
    "import datetime\n",
    "from evaluate_model import *\n",
    "from scipy import special\n",
    "import scipy.io as sio\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler,ModelCheckpoint\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[5], 'GPU')\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# data_folder =  '/Data/hmd/physionet.org/files/circor-heart-sound/1.0.3/training_data'\n",
    "train_folder =  '/Data/hmd/data_split/murmur/train/'\n",
    "test_folder = '/Data/hmd/data_split/murmur/test/'\n",
    "\n",
    "import typing\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "## filtering (s1, s2 detect)\n",
    "############################\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order):\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    # Get the filter coefficients \n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "############################\n",
    "## feature_extract_bound_melspec\n",
    "############################\n",
    "\n",
    "def feature_extract_bound_melspec(data, samp_sec=20, sr = 4000, pre_emphasis = 0, hop_length=256, win_length = 512, n_mels = 100):\n",
    "    \n",
    "    if samp_sec:\n",
    "        if len(data) > sample_rate * samp_sec :\n",
    "            n_samp = len(data) // int(sample_rate * samp_sec)\n",
    "            signal = []\n",
    "            for i in range(n_samp) :\n",
    "                signal.append(data[ int(sample_rate * samp_sec)*i:(int(sample_rate * samp_sec)*(i+1))])\n",
    "        else :\n",
    "            n_samp = 1\n",
    "            signal = np.zeros(int(sample_rate*samp_sec,))\n",
    "            for i in range(int(sample_rate * samp_sec) // len(data)) :\n",
    "                signal[(i)*len(data):(i+1)*len(data)] = data\n",
    "            num_last = int(sample_rate * samp_sec) - len(data)*(i+1)\n",
    "            signal[(i+1)*len(data):int(sample_rate * samp_sec)] = data[:num_last]\n",
    "            signal = [signal]\n",
    "    else:\n",
    "        n_samp = 1\n",
    "        signal = [data]\n",
    "\n",
    "    Sig = []\n",
    "    for i in range(n_samp) :\n",
    "        if pre_emphasis :\n",
    "            emphasized_signal = np.append(signal[i][0], signal[i][1:] - pre_emphasis * signal[i][:-1])\n",
    "        else :\n",
    "            emphasized_signal = signal[i]\n",
    "\n",
    "        Sig.append(librosa.power_to_db(librosa.feature.melspectrogram(y=emphasized_signal, sr= sr, n_mels=n_mels, n_fft=win_length, hop_length=hop_length, win_length=win_length)))\n",
    "\n",
    "    return Sig\n",
    "\n",
    "\n",
    "\n",
    "class Generator0():\n",
    "    def __init__(self, X_train, y_train, batch_size=32, beta_param=0.2, mixup = True, lowpass = False, highpass = False, ranfilter2 = False, shuffle=True, datagen=None, chaug = False, cout = False):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = beta_param\n",
    "        self.mixup = mixup\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_num = len(y_train)\n",
    "        self.datagen = datagen\n",
    "\n",
    "        ## ffm \n",
    "        \n",
    "        self.lowpass = lowpass\n",
    "        self.highpass = highpass\n",
    "        self.ranfilter = ranfilter2\n",
    "        self.chaug = chaug\n",
    "        self.cutout = cout        \n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            indexes = self.__get_exploration_order()\n",
    "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
    "\n",
    "            for i in range(itr_num):\n",
    "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
    "                X, y = self.__data_generation(batch_ids)\n",
    "\n",
    "                yield X, y\n",
    "\n",
    "    def __get_exploration_order(self):\n",
    "        indexes = np.arange(self.sample_num)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def __data_generation(self, batch_ids):\n",
    "        \n",
    "        \n",
    "        def get_box(lambda_value, nf, nt):\n",
    "            cut_rat = np.sqrt(1.0 - lambda_value)\n",
    "\n",
    "            cut_w = int(nf * cut_rat)  # rw\n",
    "            cut_h = int(nt * cut_rat)  # rh\n",
    "\n",
    "            cut_x = int(np.random.uniform(low=0, high=nf))  # rx\n",
    "            cut_y = int(np.random.uniform(low=0, high=nt))  # ry\n",
    "\n",
    "            boundaryx1 = np.minimum(np.maximum(cut_x - cut_w // 2, 0), nf) #tf.clip_by_value(cut_x - cut_w // 2, 0, IMG_SIZE_x)\n",
    "            boundaryy1 = np.minimum(np.maximum(cut_y - cut_h // 2, 0), nt) #tf.clip_by_value(cut_y - cut_h // 2, 0, IMG_SIZE_y)\n",
    "            bbx2 = np.minimum(np.maximum(cut_x + cut_w // 2, 0), nf) #tf.clip_by_value(cut_x + cut_w // 2, 0, IMG_SIZE_x)\n",
    "            bby2 = np.minimum(np.maximum(cut_y + cut_h // 2, 0), nt) #tf.clip_by_value(cut_y + cut_h // 2, 0, IMG_SIZE_y)\n",
    "\n",
    "            target_h = bby2 - boundaryy1\n",
    "            if target_h == 0:\n",
    "                target_h += 1\n",
    "\n",
    "            target_w = bbx2 - boundaryx1\n",
    "            if target_w == 0:\n",
    "                target_w += 1\n",
    "\n",
    "            return boundaryx1, boundaryy1, target_h, target_w           \n",
    "        \n",
    "        \n",
    "        if isinstance(self.X_train, list):\n",
    "            X = []\n",
    "            for X_temp in self.X_train:\n",
    "                if len(X_temp.shape) == 4: \n",
    "                    _, h, w, c = X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                elif len(X_temp.shape) == 3:\n",
    "                    _, h, w = X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size, 1, 1)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                elif len(X_temp.shape) == 2:\n",
    "                    _, h = X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size, 1)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                elif len(X_temp.shape) == 1:\n",
    "                    _= X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size,)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                \n",
    "                X1 = X_temp[batch_ids[:self.batch_size]].copy()\n",
    "                X2 = X_temp[batch_ids[self.batch_size:]].copy()\n",
    "                \n",
    "                if self.mixup :\n",
    "                    Xn = X1 * X_l + X2 * (1 - X_l)\n",
    "                else :\n",
    "                    Xn = X1\n",
    "                if len(X_temp.shape) == 4: \n",
    "                    _, h, w, c = X_temp.shape\n",
    "                    if h != 1 :\n",
    "                        if self.lowpass :\n",
    "                            uv, lp = self.lowpass\n",
    "                            dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                            for i in range(self.batch_size) :\n",
    "                                loc1 = np.random.choice(lp, size = 1)[0]\n",
    "                                Xn[i,:loc1,:,:] = 0\n",
    "                        if self.highpass :\n",
    "                            uv, hp = self.highpass\n",
    "                            dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                            for i in range(self.batch_size) :\n",
    "                                loc1 = np.random.choice(hp, size = 1)[0]\n",
    "                                Xn[i,loc1:,:,:] = 0\n",
    "                        if self.ranfilter :                \n",
    "                            raniter, ranf = self.ranfilter\n",
    "                            dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "                            for i in range(self.batch_size) :\n",
    "                                if dec1[i] > 0 :\n",
    "                                    for j in range(dec1[i]) :\n",
    "                                        b1 = np.random.choice(ranf, size = 1)[0]\n",
    "                                        loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "                                        Xn[i, loc1:(loc1 + b1 - 1), :] = 0\n",
    "                        if self.chaug :\n",
    "                            for i in range(self.batch_size) :\n",
    "                                noiselv = np.random.uniform(low= - self.chaug, high= self.chaug)\n",
    "                                Xn[i,:] += noiselv\n",
    "                        if self.cutout :\n",
    "                            lambda1 = np.random.beta(self.cutout, self.cutout, size = self.batch_size)   ## beta_param default : 0.7  STC페이퍼 추천은 0.6~0.8\n",
    "                            for i in range(self.batch_size) :\n",
    "                                boundaryx1, boundaryy1, target_h, target_w = get_box(lambda1[i], h, w)\n",
    "                                Xn[i, boundaryx1:(boundaryx1+target_h), boundaryy1:(boundaryy1+target_w),: ] = 0\n",
    "                \n",
    "#                 if len(X_temp.shape) == 3: \n",
    "                    \n",
    "#                     _, h, w = X_temp.shape\n",
    "                    \n",
    "#                     if h != 1 :\n",
    "                        \n",
    "#                         if self.lowpass :\n",
    "#                             uv, lp = self.lowpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(lp, size = 1)[0]\n",
    "#                                 Xn[i,:loc1,:] = 0\n",
    "#                         if self.highpass :\n",
    "#                             uv, hp = self.highpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(hp, size = 1)[0]\n",
    "#                                 Xn[i,loc1:,:] = 0\n",
    "#                         if self.ranfilter :                \n",
    "#                             raniter, ranf = self.ranfilter\n",
    "#                             dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 if dec1[i] > 0 :\n",
    "#                                     for j in range(dec1[i]) :\n",
    "#                                         b1 = np.random.choice(ranf, size = 1)[0]\n",
    "#                                         loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "#                                         Xn[i, loc1:(loc1 + b1 - 1), :] = 0                    \n",
    "                X.append(Xn)\n",
    "        else:\n",
    "            if len(self.X_train.shape) == 4: \n",
    "                _, h, w, c = self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "            elif len(self.X_train.shape) == 3:\n",
    "                _, h, w = self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size, 1, 1)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "            elif len(self.X_train.shape) == 2:\n",
    "                _, h = self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size, 1)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "            elif len(self.X_train.shape) == 1:\n",
    "                _= self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size,)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "\n",
    "            X1 = self.X_train[batch_ids[:self.batch_size]].copy()\n",
    "            X2 = self.X_train[batch_ids[self.batch_size:]].copy()\n",
    "            if self.mixup :\n",
    "                Xn = X1 * X_l + X2 * (1 - X_l)\n",
    "            else :\n",
    "                Xn = X1\n",
    "\n",
    "            if len(self.X_train.shape) == 4: \n",
    "                _, h, w, c = X_temp.shape\n",
    "                if self.lowpass :\n",
    "                    uv, lp = self.lowpass\n",
    "                    dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                    for i in range(self.batch_size) :\n",
    "                        loc1 = np.random.choice(lp, size = 1)[0]\n",
    "                        Xn[i,:loc1,:,:] = 0\n",
    "                if self.highpass :\n",
    "                    uv, hp = self.highpass\n",
    "                    dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                    for i in range(self.batch_size) :\n",
    "                        loc1 = np.random.choice(hp, size = 1)[0]\n",
    "                        Xn[i,loc1:,:,:] = 0\n",
    "                if self.ranfilter :                \n",
    "                    raniter, ranf = self.ranfilter\n",
    "                    dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "                    for i in range(self.batch_size) :\n",
    "                        if dec1[i] > 0 :\n",
    "                            for j in range(dec1[i]) :\n",
    "                                b1 = np.random.choice(ranf, size = 1)[0]\n",
    "                                loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "                                Xn[i, loc1:(loc1 + b1 - 1), :] = 0\n",
    "                \n",
    "#                 if len(self.X_train.shape) == 3:\n",
    "#                     _, h, w = X_temp.shape\n",
    "                    \n",
    "#                     if h != 1 :\n",
    "#                         if self.lowpass :\n",
    "#                             uv, lp = self.lowpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(lp, size = 1)[0]\n",
    "#                                 Xn[i,:loc1,:] = 0\n",
    "#                         if self.highpass :\n",
    "#                             uv, hp = self.highpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(hp, size = 1)[0]\n",
    "#                                 Xn[i,loc1:,:] = 0\n",
    "#                         if self.ranfilter :                \n",
    "#                             raniter, ranf = self.ranfilter\n",
    "#                             dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 if dec1[i] > 0 :\n",
    "#                                     for j in range(dec1[i]) :\n",
    "#                                         b1 = np.random.choice(ranf, size = 1)[0]\n",
    "#                                         loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "#                                         Xn[i, loc1:(loc1 + b1 - 1), :] = 0\n",
    "                \n",
    "            X.append(Xn)\n",
    "\n",
    "                \n",
    "        if self.datagen:\n",
    "            for i in range(self.batch_size):\n",
    "                X[i] = self.datagen.random_transform(X[i])\n",
    "                X[i] = self.datagen.standardize(X[i])\n",
    "\n",
    "        if isinstance(self.y_train, list):\n",
    "            y = []\n",
    "\n",
    "            for y_train_ in self.y_train:\n",
    "                y1 = y_train_[batch_ids[:self.batch_size]].copy()\n",
    "                y2 = y_train_[batch_ids[self.batch_size:]].copy()\n",
    "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
    "        else:\n",
    "            y1 = self.y_train[batch_ids[:self.batch_size]].copy()\n",
    "            y2 = self.y_train[batch_ids[self.batch_size:]].copy()\n",
    "            y = y1 * y_l + y2 * (1 - y_l)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "def get_LCNN_o_1_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape, \n",
    "                       mel_envel_input_shape, use_s1s2 = True,use_mm = True,use_envel = True,\n",
    "                       use_mel = True, use_cqt = True, use_stft = True, \n",
    "                       ord1 = True, dp = .5, fc = False, ext = False, ext2 = False):\n",
    "    # Create a towy model.\n",
    "    age = keras.Input(shape=(6,), name = 'age_cat')\n",
    "    sex = keras.Input(shape=(2,), name = 'sex_cat')\n",
    "    hw = keras.Input(shape=(2,), name = 'height_weight')\n",
    "    preg = keras.Input(shape=(1,), name = 'is_preg')\n",
    "    loc = keras.Input(shape=(5,), name = 'loc')\n",
    "    mel1 = keras.Input(shape=(mel_input_shape), name = 'mel')\n",
    "    cqt1 = keras.Input(shape=(cqt_input_shape), name = 'cqt')\n",
    "    stft1 = keras.Input(shape=(stft_input_shape), name = 'stft')\n",
    "    rr1 = keras.Input(shape=(1,), name = 'rr')\n",
    "\n",
    "    mel1_envel = keras.Input(shape=(mel_envel_input_shape), name = 'mel_envel')\n",
    "        \n",
    "    ## age embeddig\n",
    "    age1 = layers.Dense(2, activation = None)(age)\n",
    "\n",
    "    ## sex embedding\n",
    "    sex1 = layers.Dense(1, activation = None)(sex)\n",
    "\n",
    "    ## hw embedding\n",
    "    hw1 = layers.Dense(1, activation = None)(hw)\n",
    "\n",
    "    ## loc embedding\n",
    "    loc1 = layers.Dense(3, activation = None)(loc)\n",
    "\n",
    "\n",
    "    ## mel embedding\n",
    "    if use_mel :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        max3 = tf.keras.activations.swish(max3)\n",
    "\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "#         batch6 = layers.LeakyReLU()(batch6)\n",
    "\n",
    "\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        batch10 = tf.keras.activations.swish(batch10)\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(max3.shape[1],1), strides=(2,2))(max3)\n",
    "        u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(u)\n",
    "\n",
    "        # u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(max3)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "        batch10 = batch10 + u\n",
    "        ############################\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        batch13 = tf.keras.activations.swish(batch13)\n",
    "\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "        max16 = tf.keras.activations.swish(max16)\n",
    "\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "        batch19 = tf.keras.activations.swish(batch19)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch10.shape[1],1), strides=(2,2))(batch10)\n",
    "        u = Conv2D(filters=64, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=64, kernel_size=1,  padding='same', activation=None)(batch10)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "\n",
    "        batch19 = batch19 + u\n",
    "        ############################\n",
    "\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "#         batch22 = layers.LeakyReLU()(batch22)\n",
    "\n",
    "\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24) \n",
    "        batch25 = tf.keras.activations.swish(batch25)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch19.shape[1],1), strides=(1,1))(batch19)\n",
    "        u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(batch19)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')(u)\n",
    "\n",
    "        batch25 = batch25 + u\n",
    "        ############################\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        mha = layers.MultiHeadAttention(num_heads=8, key_dim=256)(mfm27,mfm27,mfm27)\n",
    "        mel2 = layers.GlobalAveragePooling2D()(mha)\n",
    "        \n",
    "    \n",
    "\n",
    "    if use_envel :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_envel)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_envel)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        max3 = tf.keras.activations.swish(max3)\n",
    "\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "#         batch6 = layers.LeakyReLU()(batch6)\n",
    "\n",
    "\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        batch10 = tf.keras.activations.swish(batch10)\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(max3.shape[1],1), strides=(2,2))(max3)\n",
    "        u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(u)\n",
    "\n",
    "        # u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(max3)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "        batch10 = batch10 + u\n",
    "        ############################\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        batch13 = tf.keras.activations.swish(batch13)\n",
    "\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "        max16 = tf.keras.activations.swish(max16)\n",
    "\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "        batch19 = tf.keras.activations.swish(batch19)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch10.shape[1],1), strides=(2,2))(batch10)\n",
    "        u = Conv2D(filters=64, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=64, kernel_size=1,  padding='same', activation=None)(batch10)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "\n",
    "        batch19 = batch19 + u\n",
    "        ############################\n",
    "\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "#         batch22 = layers.LeakyReLU()(batch22)\n",
    "\n",
    "\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24) \n",
    "        batch25 = tf.keras.activations.swish(batch25)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch19.shape[1],1), strides=(1,1))(batch19)\n",
    "        u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(batch19)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')(u)\n",
    "\n",
    "        batch25 = batch25 + u\n",
    "        ############################\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        mha = layers.MultiHeadAttention(num_heads=8, key_dim=256)(mfm27,mfm27,mfm27)\n",
    "        mel2_envel = layers.GlobalAveragePooling2D()(mha)\n",
    "        \n",
    "   # mel1_s1s2\n",
    "    if use_s1s2 :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_s1s2)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_s1s2)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        max3 = tf.keras.activations.swish(max3)\n",
    "\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        batch10 = tf.keras.activations.swish(batch10)\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(max3.shape[1],1), strides=(2,2))(max3)\n",
    "        u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(u)\n",
    "\n",
    "        # u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(max3)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "        batch10 = batch10 + u\n",
    "        ############################\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        batch13 = tf.keras.activations.swish(batch13)\n",
    "\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "        max16 = tf.keras.activations.swish(max16)\n",
    "\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "        batch19 = tf.keras.activations.swish(batch19)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch10.shape[1],1), strides=(2,2))(batch10)\n",
    "        u = Conv2D(filters=64, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=64, kernel_size=1,  padding='same', activation=None)(batch10)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "\n",
    "        batch19 = batch19 + u\n",
    "        ############################\n",
    "\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24) \n",
    "        batch25 = tf.keras.activations.swish(batch25)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch19.shape[1],1), strides=(1,1))(batch19)\n",
    "        u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(batch19)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')(u)\n",
    "\n",
    "        batch25 = batch25 + u\n",
    "        ############################\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        mha = layers.MultiHeadAttention(num_heads=8, key_dim=256)(mfm27,mfm27,mfm27)\n",
    "        mel2_s1s2 = layers.GlobalAveragePooling2D()(mha)\n",
    "        \n",
    "       \n",
    "   # mel1_s1s2\n",
    "    if use_mm :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_mm)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_mm)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        max3 = tf.keras.activations.swish(max3)\n",
    "\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        batch10 = tf.keras.activations.swish(batch10)\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(max3.shape[1],1), strides=(2,2))(max3)\n",
    "        u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(u)\n",
    "\n",
    "        # u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(max3)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "        batch10 = batch10 + u\n",
    "        ############################\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        batch13 = tf.keras.activations.swish(batch13)\n",
    "\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "        max16 = tf.keras.activations.swish(max16)\n",
    "\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "        batch19 = tf.keras.activations.swish(batch19)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch10.shape[1],1), strides=(2,2))(batch10)\n",
    "        u = Conv2D(filters=64, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=64, kernel_size=1,  padding='same', activation=None)(batch10)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "\n",
    "        batch19 = batch19 + u\n",
    "        ############################\n",
    "\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24) \n",
    "        batch25 = tf.keras.activations.swish(batch25)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch19.shape[1],1), strides=(1,1))(batch19)\n",
    "        u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(batch19)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')(u)\n",
    "\n",
    "        batch25 = batch25 + u\n",
    "        ############################\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        mha = layers.MultiHeadAttention(num_heads=8, key_dim=256)(mfm27,mfm27,mfm27)\n",
    "        mel2_mm = layers.GlobalAveragePooling2D()(mha)\n",
    "\n",
    "    if use_cqt :\n",
    "        ## cqt embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "        \n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        cqt2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            cqt2 = Dropout(dp)(cqt2)\n",
    "\n",
    "    if use_stft :\n",
    "        ## stft embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        \n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "        \n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        stft2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            stft2 = Dropout(dp)(stft2)\n",
    "    \n",
    "    \n",
    "    if use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2, stft2])\n",
    "    if not use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([cqt2, stft2])\n",
    "    if use_mel and not use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, stft2])\n",
    "    if use_mel and use_cqt and not use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2])\n",
    "    if not use_mel and not use_cqt and use_stft :  ## stft 만\n",
    "        concat2 = stft2\n",
    "    if use_mel and not use_cqt and not use_stft :  ### mel만\n",
    "        concat2 = mel2\n",
    "    if not use_mel and use_cqt and not use_stft :  ### cqt만\n",
    "        concat2 = cqt2\n",
    "\n",
    "    if ext :\n",
    "        concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg, rr1, mel2_envel])\n",
    "        d1 = layers.Dense(3, activation = 'relu')(concat1)\n",
    "        d1 = layers.Dense(2, activation = 'relu')(d1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1])\n",
    "\n",
    "    if ext2 :\n",
    "        concat1 = layers.Concatenate()([rr1,mel2_envel])\n",
    "        d1 = layers.Dense(3, activation = 'relu')(concat1)\n",
    "        d1 = layers.Dense(2, activation = 'relu')(d1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1])\n",
    "        \n",
    "    if fc :\n",
    "        concat2 = layers.Dense(10, activation = \"relu\")(concat2)\n",
    "        if dp :\n",
    "            concat2 = Dropout(dp)(concat2)\n",
    "        \n",
    "    if ord1 :\n",
    "        res1 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "    else :\n",
    "        res1 = layers.Dense(3, activation = \"softmax\")(concat2)\n",
    "\n",
    "        \n",
    "    res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "\n",
    "    model = keras.Model(inputs = [age,sex,hw,preg,loc,mel1,cqt1,stft1, rr1, mel1_envel] , outputs = res1 )\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy','AUC'])\n",
    "    return(model)\n",
    "\n",
    "\n",
    "def get_LCNN_2_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape, \n",
    "                    mel_envel_input_shape, use_s1s2 = True,use_mm = True, use_envel= True,\n",
    "                     use_mel = True, use_cqt = True, use_stft = True, \n",
    "                    dp = False, fc = False, ext = False, ext2 = False):\n",
    "        # Create a towy model.\n",
    "    age = keras.Input(shape=(6,), name = 'age_cat')\n",
    "    sex = keras.Input(shape=(2,), name = 'sex_cat')\n",
    "    hw = keras.Input(shape=(2,), name = 'height_weight')\n",
    "    preg = keras.Input(shape=(1,), name = 'is_preg')\n",
    "    loc = keras.Input(shape=(5,), name = 'loc')\n",
    "    mel1 = keras.Input(shape=(mel_input_shape), name = 'mel')\n",
    "    cqt1 = keras.Input(shape=(cqt_input_shape), name = 'cqt')\n",
    "    stft1 = keras.Input(shape=(stft_input_shape), name = 'stft')\n",
    "    rr1 = keras.Input(shape=(1,), name = 'rr')\n",
    "    \n",
    "    mel1_envel = keras.Input(shape=(mel_envel_input_shape), name = 'mel_envel')\n",
    "        \n",
    "        \n",
    "    ## age embeddig\n",
    "    age1 = layers.Dense(2, activation = None)(age)\n",
    "\n",
    "    ## sex embedding\n",
    "    sex1 = layers.Dense(1, activation = None)(sex)\n",
    "\n",
    "    ## hw embedding\n",
    "    hw1 = layers.Dense(1, activation = None)(hw)\n",
    "\n",
    "    ## loc embedding\n",
    "    loc1 = layers.Dense(3, activation = None)(loc)\n",
    "\n",
    "   \n",
    "\n",
    "   ## mel embedding\n",
    "    if use_mel :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        \n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        mel2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            mel2 = Dropout(dp)(mel2)\n",
    "            \n",
    "    \n",
    "   ## use_envel\n",
    "    if use_envel :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_envel)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_envel)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        \n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        mel2_envel = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            mel2_envel = Dropout(dp)(mel2_envel)\n",
    "        \n",
    "    if use_s1s2:\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_s1s2)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_s1s2)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        \n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        mel2_s1s2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            mel2_s1s2 = Dropout(dp)(mel2_s1s2)\n",
    "            \n",
    "    if use_mm:\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_mm)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_mm)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        \n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        mel2_mm = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            mel2_mm = Dropout(dp)(mel2_mm)\n",
    "\n",
    "    if use_cqt :\n",
    "        ## cqt embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        \n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        cqt2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            cqt2 = Dropout(dp)(cqt2)\n",
    "\n",
    "    if use_stft :\n",
    "        ## stft embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "        \n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        stft2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            stft2 = Dropout(dp)(stft2)\n",
    "    \n",
    "#    concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg])\n",
    "#    d1 = layers.Dense(2, activation = 'relu')(concat1)\n",
    "    \n",
    "    if use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2, stft2])\n",
    "    if not use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([cqt2, stft2])\n",
    "    if use_mel and not use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, stft2])\n",
    "    if use_mel and use_cqt and not use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2])\n",
    "    if not use_mel and not use_cqt and use_stft :  ## stft 만\n",
    "        concat2 = stft2\n",
    "    if use_mel and not use_cqt and not use_stft :  ### mel만\n",
    "        concat2 = mel2\n",
    "    if not use_mel and use_cqt and not use_stft :  ### cqt만\n",
    "        concat2 = cqt2\n",
    "\n",
    "    if ext :\n",
    "        concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg, rr1,mel2_envel])\n",
    "        d1 = layers.Dense(3, activation = 'relu')(concat1)\n",
    "        d1 = layers.Dense(2, activation = 'relu')(d1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1])\n",
    "\n",
    "    if ext2 :\n",
    "        concat1 = layers.Concatenate()([rr1,mel2_envel])\n",
    "        d1 = layers.Dense(3, activation = 'relu')(concat1)\n",
    "        d1 = layers.Dense(2, activation = 'relu')(d1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1])\n",
    "        \n",
    "    if fc :\n",
    "        concat2 = layers.Dense(10, activation = 'relu')(concat2)\n",
    "        if dp :\n",
    "            concat2 = Dropout(dp)(concat2)\n",
    "\n",
    "    res1 = layers.Dense(3, activation = \"softmax\")(concat2)\n",
    "    res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "\n",
    "    model = keras.Model(inputs = [age,sex,hw,preg,loc,mel1,cqt1,stft1, rr1, mel1_envel] , outputs = res2 )\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy','AUC'])\n",
    "    return(model)\n",
    "\n",
    "\n",
    "def sigmoidal_decay(e, start=0, end=100, lr_start=1e-3, lr_end=1e-5):\n",
    "    if e < start:\n",
    "        return lr_start\n",
    "    elif e > end:\n",
    "        return lr_end\n",
    "\n",
    "    middle = (start + end) / 2\n",
    "    s = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return s(13 * (-e + middle) / np.abs(end - start)) * np.abs(lr_start - lr_end) + lr_end\n",
    "\n",
    "patient_files_trn = find_patient_files(train_folder)\n",
    "patient_files_test = find_patient_files(test_folder)\n",
    "\n",
    "# Run model.\n",
    "def run_model(model_folder, data_folder, output_folder, allow_failures, verbose):\n",
    "    # Load models.\n",
    "    if verbose >= 1:\n",
    "        print('Loading Challenge model...')\n",
    "\n",
    "    model = load_challenge_model(model_folder, verbose) ### Teams: Implement this function!!!\n",
    "\n",
    "    # Find the patient data files.\n",
    "    patient_files = find_patient_files(data_folder)\n",
    "    num_patient_files = len(patient_files)\n",
    "\n",
    "    if num_patient_files==0:\n",
    "        raise Exception('No data was provided.')\n",
    "\n",
    "    # Create a folder for the Challenge outputs if it does not already exist.\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Run the team's model on the Challenge data.\n",
    "    if verbose >= 1:\n",
    "        print('Running model on Challenge data...')\n",
    "\n",
    "#    @tf.function\n",
    "    # Iterate over the patient files.\n",
    "    for i in range(num_patient_files):\n",
    "        if verbose >= 2:\n",
    "            print('    {}/{}...'.format(i+1, num_patient_files))\n",
    "\n",
    "        patient_data = load_patient_data(patient_files[i])\n",
    "        recordings = load_recordings(data_folder, patient_data)\n",
    "\n",
    "        # Allow or disallow the model to fail on parts of the data; helpful for debugging.\n",
    "        try:\n",
    "            classes, labels, probabilities = run_challenge_model(model, patient_data, recordings, verbose) ### Teams: Implement this function!!!\n",
    "        except:\n",
    "            if allow_failures:\n",
    "                if verbose >= 2:\n",
    "                    print('... failed.')\n",
    "                classes, labels, probabilities = list(), list(), list()\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # Save Challenge outputs.\n",
    "        head, tail = os.path.split(patient_files[i])\n",
    "        root, extension = os.path.splitext(tail)\n",
    "        output_file = os.path.join(output_folder, root + '.csv')\n",
    "        patient_id = get_patient_id(patient_data)\n",
    "        save_challenge_outputs(output_file, patient_id, classes, labels, probabilities)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print('Done.')\n",
    "        \n",
    "import pickle as pk\n",
    "def save_challenge_model(model_folder, model1, model2, m_name1, m_name2, param_feature) :\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    filename1 = os.path.join(model_folder, m_name1 + '_model1.hdf5')\n",
    "    filename2 = os.path.join(model_folder, m_name2 + '_model2.hdf5')\n",
    "    model1.save(filename1)\n",
    "    model2.save(filename2)\n",
    "    param_feature['model1'] = m_name1\n",
    "    param_feature['model2'] = m_name2\n",
    "    param_feature['model_fnm1'] = filename1\n",
    "    param_feature['model_fnm2'] = filename2\n",
    "    with open(info_fnm, 'wb') as f:\n",
    "        pk.dump(param_feature, f, pk.HIGHEST_PROTOCOL)\n",
    "    return 1\n",
    "\n",
    "def load_challenge_model(model_folder, verbose):\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    with open(info_fnm, 'rb') as f:\n",
    "        info_m = pk.load(f)\n",
    "#    if info_m['model'] == 'toy' :\n",
    "#        model = get_toy(info_m['mel_shape'])\n",
    "#    filename = os.path.join(model_folder, info_m['model'] + '_model.hdf5')\n",
    "#    model.load_weights(filename)\n",
    "    return info_m\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run your trained model. This function is *required*. You should edit this function to add your code, but do *not* change the\n",
    "# arguments of this function.\n",
    "def run_challenge_model(model, data, recordings, verbose):\n",
    "\n",
    "    murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "    outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "    \n",
    "    if model['model1'] == 'lcnn1_dr_rr' :\n",
    "        model1 = get_LCNN_o_1_dr_rr(model['mel_shape'],model['cqt_shape'],model['stft_shape'], \n",
    "                                    model['envel_shape'],model['use_s1s2'], model['use_mm'],\n",
    "                                    use_mel = model['use_mel'],use_cqt = model['use_cqt'], use_stft = model['use_stft'],\n",
    "                                    ord1 = model['ord1'], \n",
    "                                    dp = model['dp'], fc = model['fc'], ext = False, ext2 = True)\n",
    "    if model['model2'] == 'lcnn2_dr_rr' :\n",
    "        model2 = get_LCNN_2_dr_rr(model['mel_shape'],model['cqt_shape'],model['stft_shape'], \n",
    "                                  model['envel_shape'],model['use_s1s2'], model['use_mm'],\n",
    "                                  use_mel = model['use_mel'],use_cqt = model['use_cqt'], use_stft = model['use_stft'], \n",
    "                                  dp = model['dp'], fc = model['fc'], ext = True, ext2 = False)\n",
    "    model1.load_weights(model['model_fnm1'])\n",
    "    model2.load_weights(model['model_fnm2'])\n",
    "\n",
    "#    classes = model['classes']\n",
    "    # Load features.\n",
    "    features = get_feature_one(data, verbose = 0)\n",
    "\n",
    "    samp_sec = model['samp_sec']\n",
    "    pre_emphasis = model['pre_emphasis']\n",
    "    hop_length = model['hop_length']\n",
    "    win_length = model['win_length']\n",
    "    n_mels = model['n_mels']\n",
    "    filter_scale = model['filter_scale']\n",
    "    n_bins = model['n_bins']\n",
    "    fmin = model['fmin']\n",
    "    use_mel = model['use_mel']\n",
    "    use_cqt = model['use_cqt']\n",
    "    use_stft = model['use_stft']\n",
    "    use_raw = model['use_raw']\n",
    "    trim = model['trim']\n",
    "    use_rr = model['use_rr']\n",
    "    use_mm = model['use_mm']\n",
    "    use_s1s2 = model['use_s1s2']\n",
    "    envel_shape = model['envel_shape']\n",
    "    use_b_detect = True\n",
    "\n",
    "    features['mel1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_mel :\n",
    "            mel1 = feature_extract_melspec(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length,\n",
    "                                           win_length = win_length, n_mels = n_mels, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1) )\n",
    "        features['mel1'].append(mel1)\n",
    "    M, N = features['mel1'][0].shape\n",
    "\n",
    "    if use_mel :\n",
    "        for i in range(len(features['mel1'])) :\n",
    "            features['mel1'][i] = features['mel1'][i].reshape(M,N,1)\n",
    "    features['mel1'] = np.array(features['mel1'])\n",
    "\n",
    "    features['cqt1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_cqt :\n",
    "            mel1 = feature_extract_cqt(recordings[i], samp_sec=samp_sec, pre_emphasis = pre_emphasis, filter_scale = filter_scale,\n",
    "                                        n_bins = n_bins, fmin = fmin, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1))\n",
    "        features['cqt1'].append(mel1)\n",
    "    M, N = features['cqt1'][0].shape\n",
    "    if use_cqt :\n",
    "        for i in range(len(features['cqt1'])) :\n",
    "            features['cqt1'][i] = features['cqt1'][i].reshape(M,N,1)\n",
    "    features['cqt1'] = np.array(features['cqt1'])\n",
    "\n",
    "    features['stft1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_stft :\n",
    "            mel1 = feature_extract_stft(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length,\n",
    "                                        win_length = win_length, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1) )\n",
    "        features['stft1'].append(mel1)\n",
    "    M, N = features['stft1'][0].shape\n",
    "    if use_stft :\n",
    "        for i in range(len(features['stft1'])) :\n",
    "            features['stft1'][i] = features['stft1'][i].reshape(M,N,1)\n",
    "    features['stft1'] = np.array(features['stft1'])\n",
    "\n",
    "    features['raw1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_raw:\n",
    "            recording1 = recordings[i]\n",
    "            if len(recording1) >= maxlen : \n",
    "                recording1 = recording1[:maxlen]\n",
    "            else :\n",
    "                recording1 = np.pad(recording1, (0, maxlen - len(recording1) ), constant_values=(0,0) )\n",
    "        else :\n",
    "            recording1 = np.zeros((1))\n",
    "        features['raw1'].append(recording1)\n",
    "    features['raw1'] = np.array(features['raw1'])\n",
    "    \n",
    "    \n",
    "    features['rr1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_rr :\n",
    "            try:\n",
    "                recording1 = recordings[i]\n",
    "                ____, info = nk.ecg_process(recording1, sampling_rate=4000)\n",
    "                current_rr = np.mean(np.diff(info['ECG_R_Peaks'])/4000)\n",
    "            except:\n",
    "#                print(filename)\n",
    "                current_rr= 0.6414\n",
    "        else :\n",
    "            current_rr = 0\n",
    "        features['rr1'].append(current_rr)\n",
    "    features['rr1'] = np.array(features['rr1'])\n",
    "    \n",
    "    \n",
    "    features['s1s2_detect1'] = []\n",
    "    features['mm_detect1'] = []\n",
    "    features['s1s2_mel'] = []\n",
    "    features['mm_mel'] = []\n",
    "    features['envelope'] = []\n",
    "    features['envel_mel']= []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_b_detect :\n",
    "            recording1 = recordings[i]\n",
    "         # 1. Amplitude normalization\n",
    "            normal_sig = recording1/np.max(np.abs(recording1))\n",
    "\n",
    "            # 2. Filtering\n",
    "            T = len(recording1)/4000 #time interval; Sample Period ;\n",
    "            fs = 4000 #sample rate \n",
    "            cutoff = 150 #sample frequency \n",
    "\n",
    "            nyq = 0.5 * fs \n",
    "            order = 2  # sin wave can be approx represented as quadratic\n",
    "            n = int(T*fs) #total number of samples \n",
    "\n",
    "            low_ft = butter_lowpass_filter(normal_sig, cutoff, fs, order)\n",
    "\n",
    "            # 3. smoothing of signal envelope\n",
    "            duration = 1.0\n",
    "#                 fs = 4000.0\n",
    "            samples = int(fs*duration)\n",
    "            t = np.arange(len(low_ft)) / fs\n",
    "\n",
    "            analytic_signal = hilbert(low_ft)\n",
    "            amplitude_envelope = np.abs(analytic_signal)\n",
    "\n",
    "            # threshld selection\n",
    "            mu = np.sum(amplitude_envelope)/len(amplitude_envelope)\n",
    "            var = np.sum((amplitude_envelope-mu)**2)/len(amplitude_envelope)\n",
    "            t_sh = mu + var +0.05\n",
    "\n",
    "            thres_list = np.argwhere(amplitude_envelope > t_sh)\n",
    "            save = []\n",
    "            for i in thres_list:\n",
    "                j = i[0]\n",
    "                save.append(j)\n",
    "\n",
    "            packet = []\n",
    "            tmp = []\n",
    "            v = save.pop(0)\n",
    "            tmp.append(v)\n",
    "\n",
    "            while(len(save)>0):\n",
    "                vv = save.pop(0)\n",
    "                if v+1 == vv:\n",
    "                    tmp.append(vv)\n",
    "                    v = vv\n",
    "                else:\n",
    "                    packet.append(tmp)\n",
    "                    tmp = []\n",
    "                    tmp.append(vv)\n",
    "                    v = vv\n",
    "\n",
    "            packet.append(tmp)\n",
    "#                 # thresh hold 보다 크지만 연속값이 3개 미만인 리스트 찾아서 삭제\n",
    "#                 packet2 = []\n",
    "#                 for i in range(len(packet)):\n",
    "#                     if len(packet[i]) < 3:\n",
    "#                         j = packet.index(packet[i])\n",
    "#                         packet2.append(j)\n",
    "\n",
    "#                 for i in packet2:\n",
    "#                     del packet[i]\n",
    "\n",
    "            min_list = []\n",
    "            max_list = []\n",
    "            for i in range(len(packet)):\n",
    "                min_find = min(packet[i])\n",
    "                max_find = max(packet[i])\n",
    "                min_list.append(min_find)\n",
    "                max_list.append(max_find)\n",
    "\n",
    "            # s1, s2 boundary가 여러개 그려짐. 추가적인 전처리 필요. 60개보다 커야함\n",
    "            packet_cp = packet.copy()\n",
    "            new_list = []\n",
    "            first_list = []\n",
    "            last_list = []\n",
    "            for i in range(len(max_list)-1):\n",
    "                j = i + 1\n",
    "                # 연속적인 값인 경우 삭제\n",
    "                if abs(max_list[i]-min_list[j]) < 60:\n",
    "                    first = max_list.index(max_list[i])\n",
    "                    last = min_list.index(min_list[j])\n",
    "                    re_join = packet_cp[first]+packet_cp[last]\n",
    "                    new_list.append(re_join)\n",
    "                    first_list.append(first)\n",
    "                    last_list.append(last)\n",
    "\n",
    "            # 연속적인 값 인덱스 찾아서 final_list 만들기\n",
    "            final_list = first_list+last_list\n",
    "            final_list.sort()\n",
    "            set(final_list)\n",
    "\n",
    "            # 위에서 찾은 final_linst에 들어있는 인덱스 위치는 0으로 처리\n",
    "            drop_list = packet_cp.copy()\n",
    "            for i in final_list:\n",
    "                drop_list[i] = 0\n",
    "#             print(len(drop_list))\n",
    "\n",
    "            seq_remake = drop_list+new_list\n",
    "#             print(len(seq_remake))\n",
    "\n",
    "\n",
    "\n",
    "            # 0으로 전처리한 값 삭제\n",
    "            remove_set = [0]\n",
    "\n",
    "            li = [i for i in seq_remake if i not in remove_set]\n",
    "#             print(li)\n",
    "\n",
    "            # 추가 전처리 후 다시 min, max 출력\n",
    "            min_list1 = []\n",
    "            max_list1 = []\n",
    "            for i in range(len(li)):\n",
    "                min_find1 = min(li[i])\n",
    "                max_find1 = max(li[i])\n",
    "                min_list1.append(min_find1)\n",
    "                max_list1.append(max_find1)\n",
    "\n",
    "            min_list1.sort()\n",
    "            max_list1.sort()\n",
    "\n",
    "\n",
    "            # boundary detected s1 and s2\n",
    "            s_detect = amplitude_envelope.copy()\n",
    "            for i in range(len(max_list1)-1):\n",
    "                j = i+1\n",
    "                s_detect[max_list1[i]:min_list1[j]] = 0\n",
    "            s1s2_detect = s_detect.reshape(1, s_detect.shape[0])\n",
    "            s1s2_detect = s1s2_detect.tolist()\n",
    "            s1s2_detect = pad_sequences(s1s2_detect, maxlen = 80000, dtype ='float64', padding = 'post', truncating ='post', value=0.0)\n",
    "\n",
    "            # s1s2_detect 변수에 mel 적용\n",
    "            s1s2_mel = feature_extract_bound_melspec(s_detect)[0]\n",
    "\n",
    "            # boundary detected systolic and diastolic murmurs present in pcg signal\n",
    "            mm_detect = amplitude_envelope.copy()\n",
    "            for i in range(len(max_list1)):\n",
    "                mm_detect[min_list1[i]:max_list1[i]+1] = 0\n",
    "            murmur_detect = mm_detect.reshape(1, mm_detect.shape[0])\n",
    "            murmur_detect = murmur_detect.tolist()\n",
    "            murmur_detect = pad_sequences(murmur_detect, maxlen = 80000, dtype ='float64', padding = 'post', truncating ='post', value=0.0)\n",
    "\n",
    "            # murmur_dect 변수에 mel 적용\n",
    "            mm_mel = feature_extract_bound_melspec(mm_detect)[0]\n",
    "            \n",
    "            # envelope 변수\n",
    "            envelope = amplitude_envelope.copy()\n",
    "            # envelope_mel\n",
    "            envel_mel = feature_extract_bound_melspec(envelope)[0]\n",
    "\n",
    "        else :\n",
    "            s1s2_detect = np.zeros((1,1))\n",
    "            murmur_detect = np.zeros((1,1))\n",
    "            envelope = np.zeros((1,1))\n",
    "            s1s2_mel = np.zeros( (1,1,1) )\n",
    "            mm_mel = np.zeros( (1,1,1) )\n",
    "            envel_mel = np.zeros( (1,1,1))\n",
    "\n",
    "        features['s1s2_detect1'].append(s1s2_detect)\n",
    "        features['mm_detect1'].append(murmur_detect)\n",
    "        features['s1s2_mel'].append(s1s2_mel)\n",
    "        features['mm_mel'].append(mm_mel)\n",
    "        features['envelope'].append(envelope)\n",
    "        features['envel_mel'].append(envel_mel)\n",
    "\n",
    "    features['s1s2_detect1'] = np.array(features['s1s2_detect1'])\n",
    "    features['mm_detect1'] = np.array(features['mm_detect1'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    M, N = features['s1s2_mel'][0].shape\n",
    "    if use_s1s2:\n",
    "        for i in range(len(features['s1s2_mel'])):\n",
    "            features['s1s2_mel'][i] = features['s1s2_mel'][i].reshape(M,N,1)\n",
    "    features['s1s2_mel'] = np.array(features['s1s2_mel'])\n",
    "    \n",
    "    \n",
    "    M, N = features['mm_mel'][0].shape\n",
    "    if use_mm:\n",
    "        for i in range(len(features['mm_mel'])):\n",
    "            features['mm_mel'][i] = features['mm_mel'][i].reshape(M,N,1)\n",
    "    features['mm_mel'] = np.array(features['mm_mel'])\n",
    "    \n",
    "    \n",
    "    M, N = features['envel_mel'][0].shape\n",
    "    if use_mm:\n",
    "        for i in range(len(features['envel_mel'])):\n",
    "            features['envel_mel'][i] = features['envel_mel'][i].reshape(M,N,1)\n",
    "    features['envel_mel'] = np.array(features['envel_mel'])\n",
    "\n",
    "\n",
    "\n",
    "    # Impute missing data.\n",
    "    res1 = model1.predict([features['age'], features['sex'], features['hw'], features['preg'], features['loc'], \n",
    "                           features['mel1'],features['cqt1'], features['stft1'], features['rr1'], \n",
    "                           features['envel_mel']])\n",
    "    res2 = model2.predict([features['age'], features['sex'], features['hw'], features['preg'], features['loc'], \n",
    "                           features['mel1'], features['cqt1'], features['stft1'], features['rr1'], \n",
    "                           features['envel_mel']])\n",
    "\n",
    "    # Get classifier probabilities.\n",
    "    if model['ord1'] :\n",
    "        idx1 = res1.argmax(axis=0)[0]\n",
    "        murmur_p = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "        murmur_probabilities = np.zeros((3,))\n",
    "        murmur_probabilities[0] = murmur_p[0]\n",
    "        murmur_probabilities[1] = 0\n",
    "        murmur_probabilities[2] = murmur_p[1]\n",
    "        outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "    else :\n",
    "        if model['mm_mean'] :\n",
    "            murmur_probabilities = res1.mean(axis = 0)\n",
    "        else :\n",
    "            idx1 = res1.argmax(axis=0)[0]\n",
    "            murmur_probabilities = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "        outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "\n",
    "\n",
    "\n",
    "    ## 이부분도 생각 필요.. rule 을 cost를 maximize 하는 기준으로 threshold 탐색 필요할지도..\n",
    "    # Choose label with highest probability.\n",
    "    murmur_labels = np.zeros(len(murmur_classes), dtype=np.int_)\n",
    "    if murmur_probabilities[0] > 0.496 :\n",
    "        idx = 0\n",
    "    else :\n",
    "        idx = 2\n",
    "#    idx = np.argmax(murmur_probabilities)\n",
    "    murmur_labels[idx] = 1\n",
    "\n",
    "    outcome_labels = np.zeros(len(outcome_classes), dtype=np.int_)\n",
    "    if outcome_probabilities[0] > 0.617 :\n",
    "        idx = 0\n",
    "    else :\n",
    "        idx = 1\n",
    "#    idx = np.argmax(outcome_probabilities)\n",
    "    outcome_labels[idx] = 1\n",
    "\n",
    "    # Concatenate classes, labels, and probabilities.\n",
    "    classes = murmur_classes + outcome_classes\n",
    "    labels = np.concatenate((murmur_labels, outcome_labels))\n",
    "    probabilities = np.concatenate((murmur_probabilities, outcome_probabilities))\n",
    "\n",
    "    return classes, labels, probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_folder = 'hyper_1_4'\n",
    "output_folder = '/Data/hmd/hmd_sy/2021_hmd/tmp/out_hyper_1_4'\n",
    "\n",
    "# maxlen = np.random.choice([120000,80000, 50000, 15000])\n",
    "winlen = 512\n",
    "hoplen = 256\n",
    "nmel = 140 #np.random.choice([100, 120, 140])\n",
    "nsec = 50\n",
    "trim = 0 #np.random.choice([0,2000, 4000])\n",
    "use_mel = True\n",
    "use_cqt = False #np.random.choice([True,False])\n",
    "use_stft = False#np.random.choice([True, False])\n",
    "use_rr = True\n",
    "# use_rr_seq = False #True\n",
    "use_raw = False #True\n",
    "\n",
    "use_b_detect = True\n",
    "use_s1s2 = False\n",
    "use_mm = False\n",
    "use_envel = True\n",
    "\n",
    "#################\n",
    "# envelope parameter\n",
    "#################\n",
    "samp_sec = 50\n",
    "sample_rate = 4000\n",
    "pre_emphasis  = 0\n",
    "sr = 4000\n",
    "n_mels = 140\n",
    "\n",
    "# maxlen = 120000\n",
    "win_length = 512\n",
    "hop_length = 256\n",
    "\n",
    "fs = 4000 #sample rate \n",
    "cutoff = 150 #sample frequency \n",
    "nyq = 0.5 * fs \n",
    "\n",
    "\n",
    "\n",
    "params_feature = {'samp_sec': nsec,\n",
    "            #### melspec, stft 피쳐 옵션들  \n",
    "            'pre_emphasis': 0,\n",
    "            'hop_length': hoplen,\n",
    "            'win_length':winlen,\n",
    "            'n_mels': nmel,\n",
    "            #### cqt 피쳐 옵션들  \n",
    "            'filter_scale': 1,\n",
    "            'n_bins': 80,\n",
    "            'fmin': 10,\n",
    "\n",
    "            ### 사용할 피쳐 지정\n",
    "                'trim' : trim, # 앞뒤 얼마나 자를지? 4000 이면 1초\n",
    "                'use_rr' : use_rr,\n",
    "                'use_b_detect': use_b_detect,\n",
    "                'use_raw' : use_raw,\n",
    "                'use_mel' : use_mel,\n",
    "                'use_cqt' : use_cqt,\n",
    "                'use_stft' : use_stft          \n",
    "}\n",
    "\n",
    "\n",
    "mm_weight = 3 #np.random.choice([2,3,4,5])\n",
    "oo_weight = 3 #np.random.choice([2,3,4,5,6])\n",
    "ord1 = True #np.random.choice([True,False])\n",
    "mm_mean = False #np.random.choice([True,False])\n",
    "dp = 0 #np.random.choice([0, .1, .2, .3])\n",
    "fc = False #np.random.choice([True,False])\n",
    "\n",
    "\n",
    "ext = True\n",
    "\n",
    "\n",
    "chaug = 10 #np.random.choice([0, 10])\n",
    "mixup = True #np.random.choice([True,False])\n",
    "cout = .8 #np.random.choice([0, 0.8])\n",
    "wunknown = 1 #np.random.choice([1, 0.7, .5, .2])\n",
    "n1 = 0 #np.random.choice([0,2])\n",
    "if n1 == 0 :\n",
    "    ranfil = False\n",
    "else :\n",
    "    ranfil = [n1, [18,19,20,21,22,23]]\n",
    "    \n",
    "use_mel = params_feature['use_mel']\n",
    "use_cqt = params_feature['use_cqt']\n",
    "use_stft = params_feature['use_stft']\n",
    "nep = 100\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/features_trn_envel.pkl','rb') as f:\n",
    "    features_trn = pickle.load(f)\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/mm_lbs_trn_envel.pkl','rb') as f:\n",
    "    mm_lbs_trn = pickle.load(f)\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/out_lbs_trn_envel.pkl','rb') as f:\n",
    "    out_lbs_trn = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/features_test_envel.pkl','rb') as f:\n",
    "    features_test = pickle.load(f)\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/mm_lbs_test_envel.pkl','rb') as f:\n",
    "    mm_lbs_test = pickle.load(f)\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/out_lbs_test_envel.pkl','rb') as f:\n",
    "    out_lbs_test = pickle.load(f)\n",
    "    \n",
    "# (2532, 140, 782) 에서 (2532, 140, 782, 1)로 변경\n",
    "a, b, c = features_trn['mel1'].shape\n",
    "features_trn['mel1']= features_trn['mel1'].reshape(a,b,c,1)\n",
    "\n",
    "a, b, c = features_trn['s1s2_mel'].shape\n",
    "features_trn['s1s2_mel'] = features_trn['s1s2_mel'].reshape(a,b,c,1)\n",
    "\n",
    "a, b, c = features_trn['mm_mel'].shape\n",
    "features_trn['mm_mel'] = features_trn['mm_mel'].reshape(a,b,c,1)\n",
    "\n",
    "a, b, c = features_trn['envel_mel'].shape\n",
    "features_trn['envel_mel'] = features_trn['envel_mel'].reshape(a,b,c,1)\n",
    "\n",
    "mel_input_shape = features_trn['mel1'][0].shape\n",
    "cqt_input_shape = features_trn['cqt1'][0].shape\n",
    "stft_input_shape = features_trn['stft1'][0].shape\n",
    "\n",
    "mel_s1s2_input_shape = features_trn['s1s2_mel'][0].shape\n",
    "mel_mm_input_shape = features_trn['mm_mel'][0].shape\n",
    "mel_envel_input_shape = features_trn['envel_mel'][0].shape\n",
    "\n",
    "\n",
    "params_feature['ord1'] = ord1\n",
    "params_feature['mm_mean'] = mm_mean\n",
    "params_feature['dp'] = dp\n",
    "params_feature['fc'] = fc\n",
    "params_feature['ext'] = ext\n",
    "params_feature['oo_weight'] = oo_weight\n",
    "params_feature['mm_weight'] = mm_weight\n",
    "params_feature['chaug'] = chaug\n",
    "params_feature['cout'] = cout\n",
    "params_feature['wunknown'] = wunknown\n",
    "params_feature['mixup'] = mixup\n",
    "params_feature['n1'] = n1\n",
    "\n",
    "params_feature['mel_shape'] = mel_input_shape\n",
    "params_feature['cqt_shape'] = cqt_input_shape\n",
    "params_feature['stft_shape'] = stft_input_shape\n",
    "\n",
    "params_feature['s1s2_shape'] = mel_s1s2_input_shape\n",
    "params_feature['mm_shape'] = mel_mm_input_shape\n",
    "params_feature['envel_shape'] = mel_envel_input_shape\n",
    "\n",
    "params_feature['use_mel'] = use_mel\n",
    "params_feature['use_cqt'] = use_cqt\n",
    "params_feature['use_stft'] = use_stft\n",
    "\n",
    "params_feature['use_rr'] = use_rr\n",
    "params_feature['use_s1s2'] = use_s1s2\n",
    "params_feature['use_mm'] = use_mm\n",
    "params_feature['use_envel'] = use_envel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(params_feature)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "model1 = get_LCNN_o_1_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape, \n",
    "                            mel_envel_input_shape, use_s1s2 = use_s1s2, use_mm = use_mm,use_envel= use_envel,\n",
    "                            use_mel = use_mel, use_cqt = use_cqt, use_stft = use_stft, ord1 = ord1, dp = dp, fc = fc, ext = False, ext2 = True)\n",
    "model2 = get_LCNN_2_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape, \n",
    "                        mel_envel_input_shape, use_s1s2 = use_s1s2, use_mm = use_mm,use_envel= use_envel,\n",
    "                        use_mel = use_mel, use_cqt = use_cqt, use_stft = use_stft, dp = dp, fc = fc, ext = True, ext2 = False)\n",
    "\n",
    "n_epoch = nep\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "batch_size = 64\n",
    "\n",
    "if mixup :\n",
    "    beta_param = .7\n",
    "else :\n",
    "    beta_param = 0\n",
    "\n",
    "params = {'batch_size': batch_size,\n",
    "        #          'input_shape': (100, 313, 1),\n",
    "        'shuffle': True,\n",
    "        'chaug': chaug,\n",
    "        'beta_param': beta_param,\n",
    "        'cout': cout\n",
    "#              'mixup': mixup,\n",
    "        #          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "        #          'highpass': [.5, [78,79,80,81,82,83,84,85]]\n",
    "#              'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "        #           'dropblock' : [30, 100]\n",
    "        #'device' : device\n",
    "}\n",
    "\n",
    "if mixup :\n",
    "    params['mixup'] = mixup\n",
    "    params['ranfilter2'] = ranfil\n",
    "else :\n",
    "    params['cutout'] = cout\n",
    "\n",
    "params_no_shuffle = {'batch_size': batch_size,\n",
    "                    #          'input_shape': (100, 313, 1),\n",
    "                    'shuffle': False,\n",
    "                    'beta_param': 0.7,\n",
    "                    'mixup': False\n",
    "                    #'device': device\n",
    "}\n",
    "\n",
    "if ord1 :\n",
    "    class_weight = {0: mm_weight, 1: 1.}\n",
    "else :\n",
    "    class_weight = {0: mm_weight, 1: wunknown, 2:1.}\n",
    "\n",
    "\n",
    "if mixup :\n",
    "        TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                                features_trn['preg'], features_trn['loc'], features_trn['mel1'],\n",
    "                                features_trn['cqt1'],features_trn['stft1'],features_trn['rr1'], \n",
    "                                features_trn['envel_mel']], \n",
    "                        mm_lbs_trn,  ## our Y\n",
    "                            **params)()\n",
    "        model1.fit(TrainDGen_1,\n",
    "            validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                                features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                                features_test['cqt1'], features_test['stft1'],features_test['rr1'],\n",
    "                                features_test['envel_mel']], \n",
    "                                mm_lbs_test), \n",
    "            callbacks=[lr],\n",
    "            steps_per_epoch=np.ceil(len(mm_lbs_trn)/64),\n",
    "            class_weight=class_weight, \n",
    "            epochs = n_epoch)\n",
    "\n",
    "else :\n",
    "    TrainGen = DataGenerator([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                            features_trn['preg'], features_trn['loc'], features_trn['mel1'],\n",
    "                            features_trn['cqt1'],features_trn['stft1'],features_trn['rr1'], \n",
    "                            features_trn['envel_mel']], \n",
    "                mm_lbs_trn,  ## our Y\n",
    "                **params)\n",
    "    model1.fit(TrainGen,\n",
    "        validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                            features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                            features_test['cqt1'], features_test['stft1'],features_test['rr1'], \n",
    "                            features_test['envel_mel']], \n",
    "                            mm_lbs_test), \n",
    "        callbacks=[lr],\n",
    "        #        steps_per_epoch=np.ceil(len(mm_lbs_trn)/64),\n",
    "        class_weight=class_weight, \n",
    "        epochs = n_epoch)\n",
    "    \n",
    "n_epoch = nep\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "batch_size = 64\n",
    "params = {'batch_size': batch_size,\n",
    "        #          'input_shape': (100, 313, 1),\n",
    "        'shuffle': True,\n",
    "        'chaug': chaug,\n",
    "        'beta_param': beta_param,\n",
    "        'cout': cout,\n",
    "#              'mixup': True,\n",
    "        #          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "#            'highpass': [.5, [78,79,80,81,82,83,84,85]],\n",
    "#              'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "        #           'dropblock' : [30, 100]\n",
    "        #'device' : device\n",
    "}\n",
    "\n",
    "\n",
    "if mixup :\n",
    "    params['mixup'] = mixup\n",
    "    params['ranfilter2'] = ranfil\n",
    "else :\n",
    "    params['cutout'] = cout\n",
    "\n",
    "\n",
    "params_no_shuffle = {'batch_size': batch_size,\n",
    "                    #          'input_shape': (100, 313, 1),\n",
    "                    'shuffle': False,\n",
    "                    'beta_param': 0.7,\n",
    "                    'mixup': False\n",
    "                    #'device': device\n",
    "}\n",
    "\n",
    "class_weight = {0: oo_weight, 1: 1.}\n",
    "\n",
    "\n",
    "\n",
    "if mixup :\n",
    "    TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                                features_trn['preg'], features_trn['loc'], features_trn['mel1'],\n",
    "                                features_trn['cqt1'],features_trn['stft1'],features_trn['rr1'], \n",
    "                                features_trn['envel_mel']], \n",
    "                                out_lbs_trn,  ## our Y\n",
    "                    **params)()\n",
    "\n",
    "    model2.fit(TrainDGen_1,\n",
    "    validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                                features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                                features_test['cqt1'], features_test['stft1'],features_test['rr1'],\n",
    "                                features_test['envel_mel']], \n",
    "                                out_lbs_test), \n",
    "        callbacks=[lr],\n",
    "        steps_per_epoch=np.ceil(len(out_lbs_trn)/64),\n",
    "        class_weight=class_weight, \n",
    "        epochs = n_epoch)\n",
    "else :\n",
    "    TrainGen = DataGenerator([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                            features_trn['preg'], features_trn['loc'], features_trn['mel1'],\n",
    "                            features_trn['cqt1'],features_trn['stft1'],features_trn['rr1'], \n",
    "                            features_trn['envel_mel']], \n",
    "                            out_lbs_trn,  ## our Y\n",
    "                **params)\n",
    "    model2.fit(TrainGen,\n",
    "        validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                            features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                            features_test['cqt1'], features_test['stft1'],features_test['rr1'],\n",
    "                            features_test['envel_mel']], \n",
    "                            out_lbs_test), \n",
    "        callbacks=[lr],\n",
    "        class_weight=class_weight, \n",
    "        epochs = n_epoch)\n",
    "    \n",
    "\n",
    "\n",
    "# params_feature['mel_shape'] = mel_input_shape\n",
    "# params_feature['cqt_shape'] = cqt_input_shape\n",
    "# params_feature['stft_shape'] = stft_input_shape\n",
    "\n",
    "# params_feature['use_mel'] = use_mel\n",
    "# params_feature['use_cqt'] = use_cqt\n",
    "# params_feature['use_stft'] = use_stft\n",
    "\n",
    "save_challenge_model(model_folder, model1, model2, m_name1 = 'lcnn1_dr_rr', m_name2 = 'lcnn2_dr_rr', param_feature = params_feature)\n",
    "\n",
    "run_model(model_folder, test_folder, output_folder, allow_failures = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd748ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.520,0.796,0.744,18402.411\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.612,0.613,0.633,13136.526\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.689,0.000,0.871\n",
      "Accuracy,0.816,0.000,0.871\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.630,0.593\n",
      "Accuracy,0.643,0.581\n",
      "\n",
      "0.31054684702942853\n",
      "0.6894531491851308\n",
      "0.6200114107295793\n",
      "0.37998858928992485\n"
     ]
    }
   ],
   "source": [
    "murmur_scores, outcome_scores = evaluate_model(test_folder, output_folder)\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "','.join(classes),\n",
    "','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "params_feature['mm_weighted_accuracy'] = weighted_accuracy\n",
    "\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "','.join(classes),\n",
    "','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "+ '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "print(output_string)\n",
    "\n",
    "\n",
    "\n",
    "label_folder = test_folder\n",
    "murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "# Load and parse label and model output files.\n",
    "label_files, output_files = find_challenge_files(label_folder, output_folder)\n",
    "murmur_labels = load_murmurs(label_files, murmur_classes)\n",
    "murmur_binary_outputs, murmur_scalar_outputs = load_classifier_outputs(output_files, murmur_classes)\n",
    "outcome_labels = load_outcomes(label_files, outcome_classes)\n",
    "outcome_binary_outputs, outcome_scalar_outputs = load_classifier_outputs(output_files, outcome_classes)\n",
    "\n",
    "\n",
    "print(np.mean(murmur_scalar_outputs[:,0]))\n",
    "print(np.mean(murmur_scalar_outputs[:,2]))\n",
    "print(np.mean(outcome_scalar_outputs[:,0]))\n",
    "print(np.mean(outcome_scalar_outputs[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d91fb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold:  0.01\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.192,0.288,0.558,14542.944\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.364,0.000,0.214\n",
      "Accuracy,1.000,0.000,0.122\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.05\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.304,0.440,0.636,13113.117\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.425,0.000,0.487\n",
      "Accuracy,1.000,0.000,0.331\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.1\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.400,0.597,0.706,14198.589\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.337,0.508,0.832,15131.464\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.503,0.000,0.697\n",
      "Accuracy,0.974,0.000,0.554\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.674,0.000\n",
      "Accuracy,0.990,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.15\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.430,0.649,0.722,14634.902\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.337,0.508,0.832,15131.464\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.541,0.000,0.749\n",
      "Accuracy,0.947,0.000,0.633\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.674,0.000\n",
      "Accuracy,0.990,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.2\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.462,0.702,0.749,14854.862\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.334,0.503,0.823,15125.621\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.590,0.000,0.797\n",
      "Accuracy,0.947,0.000,0.705\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.669,0.000\n",
      "Accuracy,0.980,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.25\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.482,0.733,0.765,15617.456\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.344,0.503,0.816,14915.104\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.621,0.000,0.825\n",
      "Accuracy,0.947,0.000,0.748\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.667,0.021\n",
      "Accuracy,0.969,0.011\n",
      "\n",
      "-------------\n",
      "threshold:  0.3\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.492,0.749,0.763,16600.658\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.379,0.508,0.798,14180.830\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.642,0.000,0.834\n",
      "Accuracy,0.921,0.000,0.777\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.662,0.096\n",
      "Accuracy,0.939,0.054\n",
      "\n",
      "-------------\n",
      "threshold:  0.35\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.491,0.754,0.733,17599.653\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.390,0.492,0.751,13914.671\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.634,0.000,0.839\n",
      "Accuracy,0.842,0.000,0.806\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.639,0.142\n",
      "Accuracy,0.878,0.086\n",
      "\n",
      "-------------\n",
      "threshold:  0.4\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.497,0.764,0.728,17998.945\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.466,0.534,0.758,12705.236\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.646,0.000,0.846\n",
      "Accuracy,0.816,0.000,0.827\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.656,0.276\n",
      "Accuracy,0.867,0.183\n",
      "\n",
      "-------------\n",
      "threshold:  0.45\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.508,0.780,0.736,17990.290\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.506,0.555,0.751,12173.980\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.667,0.000,0.858\n",
      "Accuracy,0.816,0.000,0.849\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.661,0.351\n",
      "Accuracy,0.847,0.247\n",
      "\n",
      "-------------\n",
      "threshold:  0.5\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.520,0.796,0.744,18402.411\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.550,0.576,0.724,11901.880\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.689,0.000,0.871\n",
      "Accuracy,0.816,0.000,0.871\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.658,0.441\n",
      "Accuracy,0.796,0.344\n",
      "\n",
      "-------------\n",
      "threshold:  0.55\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.536,0.817,0.755,18605.355\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.561,0.571,0.660,12672.152\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.721,0.000,0.887\n",
      "Accuracy,0.816,0.000,0.899\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.627,0.494\n",
      "Accuracy,0.704,0.430\n",
      "\n",
      "-------------\n",
      "threshold:  0.6\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.544,0.827,0.760,18602.999\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.601,0.602,0.630,13195.486\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.738,0.000,0.894\n",
      "Accuracy,0.816,0.000,0.914\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.624,0.578\n",
      "Accuracy,0.643,0.559\n",
      "\n",
      "-------------\n",
      "threshold:  0.65\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.547,0.832,0.752,18809.775\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.644,0.644,0.636,13147.104\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.741,0.000,0.899\n",
      "Accuracy,0.789,0.000,0.928\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.646,0.642\n",
      "Accuracy,0.633,0.656\n",
      "\n",
      "-------------\n",
      "threshold:  0.7\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.542,0.827,0.739,19227.344\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.619,0.623,0.547,15368.694\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.734,0.000,0.893\n",
      "Accuracy,0.763,0.000,0.928\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.581,0.657\n",
      "Accuracy,0.510,0.742\n",
      "\n",
      "-------------\n",
      "threshold:  0.75\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.549,0.832,0.741,19645.222\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.608,0.623,0.485,17155.250\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.753,0.000,0.893\n",
      "Accuracy,0.763,0.000,0.935\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.532,0.684\n",
      "Accuracy,0.418,0.839\n",
      "\n",
      "-------------\n",
      "threshold:  0.8\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.755,0.583,0.556,0.843,0.736,20062.797\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.659,0.649,0.601,0.628,0.446,18390.927\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.929,0.500,0.836\n",
      "AUPRC,0.797,0.073,0.879\n",
      "F-measure,0.767,0.000,0.902\n",
      "Accuracy,0.737,0.000,0.957\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.659,0.659\n",
      "AUPRC,0.730,0.568\n",
      "F-measure,0.496,0.705\n",
      "Accuracy,0.357,0.914\n",
      "\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "# 8/14 15:59\n",
    "for th1 in [0.01, 0.05, 0.1, 0.15,0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8] :\n",
    "    murmur_binary_outputs[:,0] = murmur_scalar_outputs[:,0] > th1\n",
    "    murmur_binary_outputs[:,2] = murmur_scalar_outputs[:,2] > 1 - th1\n",
    "    outcome_binary_outputs[:,0] = outcome_scalar_outputs[:,0] > th1\n",
    "    outcome_binary_outputs[:,1] = outcome_scalar_outputs[:,1] > 1 - th1\n",
    "    # For each patient, set the 'Present' or 'Abnormal' class to positive if no class is positive or if multiple classes are positive.\n",
    "    murmur_labels = enforce_positives(murmur_labels, murmur_classes, 'Present')\n",
    "    murmur_binary_outputs = enforce_positives(murmur_binary_outputs, murmur_classes, 'Present')\n",
    "    outcome_labels = enforce_positives(outcome_labels, outcome_classes, 'Abnormal')\n",
    "    outcome_binary_outputs = enforce_positives(outcome_binary_outputs, outcome_classes, 'Abnormal')\n",
    "    # Evaluate the murmur model by comparing the labels and model outputs.\n",
    "    murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes = compute_auc(murmur_labels, murmur_scalar_outputs)\n",
    "    murmur_f_measure, murmur_f_measure_classes = compute_f_measure(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_accuracy, murmur_accuracy_classes = compute_accuracy(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_weighted_accuracy = compute_weighted_accuracy(murmur_labels, murmur_binary_outputs, murmur_classes) # This is the murmur scoring metric.\n",
    "    murmur_cost = compute_cost(outcome_labels, murmur_binary_outputs, outcome_classes, murmur_classes) # Use *outcomes* to score *murmurs* for the Challenge cost metric, but this is not the actual murmur scoring metric.\n",
    "    murmur_scores = (murmur_classes, murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes, \\\n",
    "                 murmur_f_measure, murmur_f_measure_classes, murmur_accuracy, murmur_accuracy_classes, murmur_weighted_accuracy, murmur_cost)\n",
    "\n",
    "    # Evaluate the outcome model by comparing the labels and model outputs.\n",
    "    outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes = compute_auc(outcome_labels, outcome_scalar_outputs)\n",
    "    outcome_f_measure, outcome_f_measure_classes = compute_f_measure(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_accuracy, outcome_accuracy_classes = compute_accuracy(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_weighted_accuracy = compute_weighted_accuracy(outcome_labels, outcome_binary_outputs, outcome_classes)\n",
    "    outcome_cost = compute_cost(outcome_labels, outcome_binary_outputs, outcome_classes, outcome_classes) # This is the clinical outcomes scoring metric.\n",
    "    outcome_scores = (outcome_classes, outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes, \\\n",
    "                  outcome_f_measure, outcome_f_measure_classes, outcome_accuracy, outcome_accuracy_classes, outcome_weighted_accuracy, outcome_cost)\n",
    "\n",
    "\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "    murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "    outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "                + '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "    print(\"threshold: \", th1)\n",
    "    print(output_string)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea30363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

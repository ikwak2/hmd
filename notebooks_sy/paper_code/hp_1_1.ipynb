{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "608149cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 24 13:32:58 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     On   | 00000000:1A:00.0 Off |                    0 |\n",
      "| 33%   47C    P2    64W / 260W |  39631MiB / 46080MiB |     21%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    On   | 00000000:1D:00.0 Off |                    0 |\n",
      "| 30%   28C    P8    14W / 300W |      3MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    On   | 00000000:1E:00.0 Off |                    0 |\n",
      "| 30%   30C    P8    28W / 300W |  22719MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro RTX 8000     On   | 00000000:3D:00.0 Off |                    0 |\n",
      "| 33%   28C    P8     5W / 260W |      3MiB / 46080MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Quadro RTX 8000     On   | 00000000:3E:00.0 Off |                    0 |\n",
      "| 46%   69C    P2   254W / 260W |  45065MiB / 46080MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Quadro RTX 8000     On   | 00000000:41:00.0 Off |                    0 |\n",
      "| 49%   72C    P2   201W / 260W |  45065MiB / 46080MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f06e4d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'samp_sec': 50, 'pre_emphasis': 0, 'hop_length': 256, 'win_length': 512, 'n_mels': 140, 'filter_scale': 1, 'n_bins': 80, 'fmin': 10, 'trim': 0, 'use_rr': True, 'use_b_detect': True, 'use_raw': False, 'use_mel': True, 'use_cqt': False, 'use_stft': False, 'ord1': True, 'mm_mean': False, 'dp': 0, 'fc': False, 'ext': True, 'oo_weight': 3, 'mm_weight': 3, 'chaug': 10, 'cout': 0.8, 'wunknown': 1, 'mixup': True, 'n1': 0, 'mel_shape': (140, 782, 1), 'cqt_shape': (1, 1, 1), 'stft_shape': (1, 1, 1), 's1s2_shape': (100, 313, 1), 'mm_shape': (100, 313, 1), 'use_s1s2': True, 'use_mm': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 13:33:49.386157: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-24 13:33:50.573387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43263 MB memory:  -> device: 3, name: Quadro RTX 8000, pci bus id: 0000:3d:00.0, compute capability: 7.5\n",
      "2023-05-24 13:33:53.243944: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 13:34:02.535818: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 0s - loss: 1.2876 - accuracy: 0.6973 - auc: 0.7281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 13:35:10.499824: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 276327520 exceeds 10% of free system memory.\n",
      "2023-05-24 13:35:10.870253: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 276327520 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 87s 2s/step - loss: 1.2876 - accuracy: 0.6973 - auc: 0.7281 - val_loss: 0.6180 - val_accuracy: 0.7179 - val_auc: 0.7379\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.8755 - accuracy: 0.8031 - auc: 0.8572 - val_loss: 1.1822 - val_accuracy: 0.2124 - val_auc: 0.2358\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.8880 - accuracy: 0.7910 - auc: 0.8528 - val_loss: 1.2483 - val_accuracy: 0.2108 - val_auc: 0.2898\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.8619 - accuracy: 0.8152 - auc: 0.8634 - val_loss: 0.6678 - val_accuracy: 0.5547 - val_auc: 0.6234\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.8274 - accuracy: 0.8215 - auc: 0.8753 - val_loss: 0.8760 - val_accuracy: 0.2948 - val_auc: 0.3365\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.8164 - accuracy: 0.8133 - auc: 0.8755 - val_loss: 0.4365 - val_accuracy: 0.8399 - val_auc: 0.8996\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.8413 - accuracy: 0.8145 - auc: 0.8677 - val_loss: 0.6076 - val_accuracy: 0.6751 - val_auc: 0.7641\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 70s 2s/step - loss: 0.8187 - accuracy: 0.8172 - auc: 0.8701 - val_loss: 0.8982 - val_accuracy: 0.2678 - val_auc: 0.3094\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.8275 - accuracy: 0.8074 - auc: 0.8749 - val_loss: 0.4134 - val_accuracy: 0.8669 - val_auc: 0.9193\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.8172 - accuracy: 0.8184 - auc: 0.8730 - val_loss: 0.7376 - val_accuracy: 0.4406 - val_auc: 0.4604\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.8303 - accuracy: 0.8164 - auc: 0.8652 - val_loss: 0.4957 - val_accuracy: 0.8764 - val_auc: 0.9202\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.8537 - accuracy: 0.8027 - auc: 0.8753 - val_loss: 0.4510 - val_accuracy: 0.8447 - val_auc: 0.9236\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.8113 - accuracy: 0.8156 - auc: 0.8760 - val_loss: 0.3959 - val_accuracy: 0.8304 - val_auc: 0.9195\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.8004 - accuracy: 0.8215 - auc: 0.8802 - val_loss: 0.8097 - val_accuracy: 0.4881 - val_auc: 0.5037\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.8181 - accuracy: 0.8184 - auc: 0.8702 - val_loss: 0.4654 - val_accuracy: 0.8193 - val_auc: 0.9108\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7957 - accuracy: 0.8230 - auc: 0.8909 - val_loss: 0.3371 - val_accuracy: 0.8954 - val_auc: 0.9386\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7913 - accuracy: 0.8277 - auc: 0.8860 - val_loss: 0.4034 - val_accuracy: 0.8827 - val_auc: 0.9385\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7866 - accuracy: 0.8242 - auc: 0.8819 - val_loss: 0.4873 - val_accuracy: 0.8447 - val_auc: 0.9239\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 70s 2s/step - loss: 0.8089 - accuracy: 0.8164 - auc: 0.8790 - val_loss: 0.6808 - val_accuracy: 0.5277 - val_auc: 0.5561\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.8093 - accuracy: 0.8188 - auc: 0.8766 - val_loss: 1.1655 - val_accuracy: 0.2139 - val_auc: 0.3094\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.8238 - accuracy: 0.8121 - auc: 0.8856 - val_loss: 0.3673 - val_accuracy: 0.8748 - val_auc: 0.9298\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7876 - accuracy: 0.8172 - auc: 0.8825 - val_loss: 0.6418 - val_accuracy: 0.6323 - val_auc: 0.6721\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7811 - accuracy: 0.8277 - auc: 0.8852 - val_loss: 0.3962 - val_accuracy: 0.8415 - val_auc: 0.9161\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.8064 - accuracy: 0.8227 - auc: 0.8888 - val_loss: 0.3304 - val_accuracy: 0.8938 - val_auc: 0.9467\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 70s 2s/step - loss: 0.7823 - accuracy: 0.8227 - auc: 0.8843 - val_loss: 0.3322 - val_accuracy: 0.8875 - val_auc: 0.9417\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7639 - accuracy: 0.8379 - auc: 0.8869 - val_loss: 0.3935 - val_accuracy: 0.8669 - val_auc: 0.9212\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.8011 - accuracy: 0.8215 - auc: 0.8894 - val_loss: 0.4736 - val_accuracy: 0.8526 - val_auc: 0.9139\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.8113 - accuracy: 0.8238 - auc: 0.8844 - val_loss: 0.5208 - val_accuracy: 0.7655 - val_auc: 0.8496\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.8001 - accuracy: 0.8184 - auc: 0.8844 - val_loss: 0.4676 - val_accuracy: 0.8463 - val_auc: 0.9218\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7833 - accuracy: 0.8305 - auc: 0.8891 - val_loss: 0.4298 - val_accuracy: 0.8906 - val_auc: 0.9419\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.8026 - accuracy: 0.8195 - auc: 0.8849 - val_loss: 0.4949 - val_accuracy: 0.8368 - val_auc: 0.8985\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7873 - accuracy: 0.8219 - auc: 0.8943 - val_loss: 0.4838 - val_accuracy: 0.8542 - val_auc: 0.9222\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7953 - accuracy: 0.8242 - auc: 0.8855 - val_loss: 0.4697 - val_accuracy: 0.8732 - val_auc: 0.9383\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7816 - accuracy: 0.8234 - auc: 0.8864 - val_loss: 0.4006 - val_accuracy: 0.8954 - val_auc: 0.9384\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.8074 - accuracy: 0.8059 - auc: 0.8788 - val_loss: 0.6237 - val_accuracy: 0.6355 - val_auc: 0.6976\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7833 - accuracy: 0.8227 - auc: 0.8826 - val_loss: 0.4419 - val_accuracy: 0.8780 - val_auc: 0.9292\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7805 - accuracy: 0.8270 - auc: 0.8921 - val_loss: 0.4784 - val_accuracy: 0.8716 - val_auc: 0.9283\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7893 - accuracy: 0.8215 - auc: 0.8878 - val_loss: 0.5240 - val_accuracy: 0.8811 - val_auc: 0.9368\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7744 - accuracy: 0.8250 - auc: 0.8870 - val_loss: 0.3491 - val_accuracy: 0.8891 - val_auc: 0.9383\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7669 - accuracy: 0.8391 - auc: 0.8884 - val_loss: 0.5277 - val_accuracy: 0.8352 - val_auc: 0.8842\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7700 - accuracy: 0.8297 - auc: 0.8926 - val_loss: 0.4822 - val_accuracy: 0.8716 - val_auc: 0.9186\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7570 - accuracy: 0.8230 - auc: 0.8986 - val_loss: 0.4692 - val_accuracy: 0.8970 - val_auc: 0.9438\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7933 - accuracy: 0.8211 - auc: 0.8953 - val_loss: 0.3677 - val_accuracy: 0.8954 - val_auc: 0.9413\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7811 - accuracy: 0.8211 - auc: 0.8960 - val_loss: 0.3491 - val_accuracy: 0.8938 - val_auc: 0.9423\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7721 - accuracy: 0.8281 - auc: 0.8918 - val_loss: 0.4708 - val_accuracy: 0.8906 - val_auc: 0.9423\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7965 - accuracy: 0.8238 - auc: 0.8904 - val_loss: 0.4338 - val_accuracy: 0.8796 - val_auc: 0.9280\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7645 - accuracy: 0.8316 - auc: 0.8943 - val_loss: 0.4346 - val_accuracy: 0.9017 - val_auc: 0.9408\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 69s 2s/step - loss: 0.7674 - accuracy: 0.8195 - auc: 0.8907 - val_loss: 0.3789 - val_accuracy: 0.9081 - val_auc: 0.9470\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7653 - accuracy: 0.8281 - auc: 0.8997 - val_loss: 0.3537 - val_accuracy: 0.8986 - val_auc: 0.9515\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7882 - accuracy: 0.8191 - auc: 0.8897 - val_loss: 0.4354 - val_accuracy: 0.9081 - val_auc: 0.9464\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7769 - accuracy: 0.8168 - auc: 0.9013 - val_loss: 0.3513 - val_accuracy: 0.8843 - val_auc: 0.9356\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7684 - accuracy: 0.8219 - auc: 0.8988 - val_loss: 0.3872 - val_accuracy: 0.8843 - val_auc: 0.9380\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7820 - accuracy: 0.8227 - auc: 0.8944 - val_loss: 0.4184 - val_accuracy: 0.8986 - val_auc: 0.9500\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7909 - accuracy: 0.8215 - auc: 0.8884 - val_loss: 0.4478 - val_accuracy: 0.8891 - val_auc: 0.9403\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7606 - accuracy: 0.8258 - auc: 0.9024 - val_loss: 0.4631 - val_accuracy: 0.8811 - val_auc: 0.9302\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7753 - accuracy: 0.8262 - auc: 0.8889 - val_loss: 0.3684 - val_accuracy: 0.9017 - val_auc: 0.9412\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7812 - accuracy: 0.8164 - auc: 0.9060 - val_loss: 0.3581 - val_accuracy: 0.9002 - val_auc: 0.9483\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7634 - accuracy: 0.8172 - auc: 0.9014 - val_loss: 0.3631 - val_accuracy: 0.9033 - val_auc: 0.9453\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7799 - accuracy: 0.8234 - auc: 0.8981 - val_loss: 0.3385 - val_accuracy: 0.8986 - val_auc: 0.9452\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7547 - accuracy: 0.8336 - auc: 0.9079 - val_loss: 0.3926 - val_accuracy: 0.9033 - val_auc: 0.9481\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7683 - accuracy: 0.8297 - auc: 0.9048 - val_loss: 0.3922 - val_accuracy: 0.9017 - val_auc: 0.9468\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7721 - accuracy: 0.8277 - auc: 0.8977 - val_loss: 0.4243 - val_accuracy: 0.8796 - val_auc: 0.9342\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7671 - accuracy: 0.8285 - auc: 0.9017 - val_loss: 0.4009 - val_accuracy: 0.9017 - val_auc: 0.9469\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7402 - accuracy: 0.8418 - auc: 0.9063 - val_loss: 0.4371 - val_accuracy: 0.8938 - val_auc: 0.9396\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7647 - accuracy: 0.8219 - auc: 0.9049 - val_loss: 0.3966 - val_accuracy: 0.8938 - val_auc: 0.9364\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7360 - accuracy: 0.8355 - auc: 0.9055 - val_loss: 0.3310 - val_accuracy: 0.9081 - val_auc: 0.9481\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7658 - accuracy: 0.8227 - auc: 0.9059 - val_loss: 0.3668 - val_accuracy: 0.9002 - val_auc: 0.9425\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7534 - accuracy: 0.8184 - auc: 0.9086 - val_loss: 0.3614 - val_accuracy: 0.9033 - val_auc: 0.9473\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7381 - accuracy: 0.8285 - auc: 0.9132 - val_loss: 0.3911 - val_accuracy: 0.9033 - val_auc: 0.9393\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7893 - accuracy: 0.8168 - auc: 0.9022 - val_loss: 0.3889 - val_accuracy: 0.9081 - val_auc: 0.9416\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7456 - accuracy: 0.8383 - auc: 0.9064 - val_loss: 0.3788 - val_accuracy: 0.9065 - val_auc: 0.9491\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7639 - accuracy: 0.8227 - auc: 0.9031 - val_loss: 0.3878 - val_accuracy: 0.9065 - val_auc: 0.9478\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7587 - accuracy: 0.8285 - auc: 0.9070 - val_loss: 0.3987 - val_accuracy: 0.8970 - val_auc: 0.9440\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7524 - accuracy: 0.8285 - auc: 0.9064 - val_loss: 0.3880 - val_accuracy: 0.9049 - val_auc: 0.9454\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7502 - accuracy: 0.8270 - auc: 0.9159 - val_loss: 0.3855 - val_accuracy: 0.9049 - val_auc: 0.9476\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7452 - accuracy: 0.8246 - auc: 0.9069 - val_loss: 0.3886 - val_accuracy: 0.9065 - val_auc: 0.9470\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7444 - accuracy: 0.8227 - auc: 0.9134 - val_loss: 0.3800 - val_accuracy: 0.8970 - val_auc: 0.9483\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 70s 2s/step - loss: 0.7529 - accuracy: 0.8227 - auc: 0.9101 - val_loss: 0.3830 - val_accuracy: 0.8986 - val_auc: 0.9480\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7232 - accuracy: 0.8469 - auc: 0.9162 - val_loss: 0.3738 - val_accuracy: 0.9065 - val_auc: 0.9465\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7532 - accuracy: 0.8262 - auc: 0.9126 - val_loss: 0.4310 - val_accuracy: 0.8843 - val_auc: 0.9371\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7292 - accuracy: 0.8277 - auc: 0.9085 - val_loss: 0.3982 - val_accuracy: 0.8986 - val_auc: 0.9451\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7563 - accuracy: 0.8371 - auc: 0.9027 - val_loss: 0.4029 - val_accuracy: 0.9033 - val_auc: 0.9448\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7442 - accuracy: 0.8250 - auc: 0.9073 - val_loss: 0.3893 - val_accuracy: 0.9033 - val_auc: 0.9462\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7366 - accuracy: 0.8344 - auc: 0.9058 - val_loss: 0.3720 - val_accuracy: 0.9049 - val_auc: 0.9472\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7791 - accuracy: 0.8227 - auc: 0.9055 - val_loss: 0.3909 - val_accuracy: 0.9033 - val_auc: 0.9473\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7221 - accuracy: 0.8430 - auc: 0.9128 - val_loss: 0.3818 - val_accuracy: 0.9097 - val_auc: 0.9478\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7877 - accuracy: 0.8184 - auc: 0.9061 - val_loss: 0.4078 - val_accuracy: 0.9002 - val_auc: 0.9449\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7599 - accuracy: 0.8301 - auc: 0.9073 - val_loss: 0.4036 - val_accuracy: 0.9017 - val_auc: 0.9453\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7443 - accuracy: 0.8270 - auc: 0.9043 - val_loss: 0.3727 - val_accuracy: 0.9017 - val_auc: 0.9478\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7139 - accuracy: 0.8445 - auc: 0.9181 - val_loss: 0.3687 - val_accuracy: 0.9033 - val_auc: 0.9475\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7528 - accuracy: 0.8340 - auc: 0.9089 - val_loss: 0.3819 - val_accuracy: 0.9065 - val_auc: 0.9468\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7563 - accuracy: 0.8230 - auc: 0.9106 - val_loss: 0.3790 - val_accuracy: 0.9049 - val_auc: 0.9484\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7712 - accuracy: 0.8246 - auc: 0.9047 - val_loss: 0.3883 - val_accuracy: 0.9081 - val_auc: 0.9483\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7720 - accuracy: 0.8203 - auc: 0.9022 - val_loss: 0.4026 - val_accuracy: 0.9049 - val_auc: 0.9471\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 69s 2s/step - loss: 0.7494 - accuracy: 0.8379 - auc: 0.9052 - val_loss: 0.4033 - val_accuracy: 0.9049 - val_auc: 0.9462\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7372 - accuracy: 0.8348 - auc: 0.9048 - val_loss: 0.3988 - val_accuracy: 0.9049 - val_auc: 0.9458\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7432 - accuracy: 0.8301 - auc: 0.9107 - val_loss: 0.3932 - val_accuracy: 0.9049 - val_auc: 0.9464\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 69s 2s/step - loss: 0.7287 - accuracy: 0.8352 - auc: 0.9125 - val_loss: 0.3913 - val_accuracy: 0.9033 - val_auc: 0.9457\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7401 - accuracy: 0.8246 - auc: 0.9108 - val_loss: 0.3907 - val_accuracy: 0.9049 - val_auc: 0.9459\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 68s 2s/step - loss: 0.7708 - accuracy: 0.8223 - auc: 0.9081 - val_loss: 0.4012 - val_accuracy: 0.9033 - val_auc: 0.9438\n",
      "Epoch 1/100\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.2578 - accuracy: 0.4832 - auc: 0.4931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 15:29:11.412806: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 276327520 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 22s 449ms/step - loss: 1.2578 - accuracy: 0.4832 - auc: 0.4931 - val_loss: 1.3088 - val_accuracy: 0.4992 - val_auc: 0.4743\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 16s 408ms/step - loss: 1.2448 - accuracy: 0.4770 - auc: 0.4813 - val_loss: 7.2410 - val_accuracy: 0.4992 - val_auc: 0.4976\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 16s 406ms/step - loss: 1.2377 - accuracy: 0.4840 - auc: 0.4992 - val_loss: 0.6478 - val_accuracy: 0.6434 - val_auc: 0.6871\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 16s 404ms/step - loss: 1.2299 - accuracy: 0.4738 - auc: 0.5031 - val_loss: 0.7259 - val_accuracy: 0.4992 - val_auc: 0.5971\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 16s 405ms/step - loss: 1.2286 - accuracy: 0.4828 - auc: 0.4966 - val_loss: 0.8216 - val_accuracy: 0.5008 - val_auc: 0.5494\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 16s 408ms/step - loss: 1.2327 - accuracy: 0.5008 - auc: 0.5133 - val_loss: 0.6849 - val_accuracy: 0.4992 - val_auc: 0.6127\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 16s 400ms/step - loss: 1.2133 - accuracy: 0.4695 - auc: 0.5067 - val_loss: 0.7213 - val_accuracy: 0.5040 - val_auc: 0.5576\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 16s 414ms/step - loss: 1.2215 - accuracy: 0.4699 - auc: 0.4976 - val_loss: 0.9691 - val_accuracy: 0.5024 - val_auc: 0.4987\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 16s 405ms/step - loss: 1.2130 - accuracy: 0.4914 - auc: 0.5245 - val_loss: 0.7251 - val_accuracy: 0.4992 - val_auc: 0.5615\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 16s 409ms/step - loss: 1.2117 - accuracy: 0.4738 - auc: 0.5282 - val_loss: 0.7615 - val_accuracy: 0.4992 - val_auc: 0.6085\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 16s 412ms/step - loss: 1.2196 - accuracy: 0.4695 - auc: 0.5034 - val_loss: 0.6888 - val_accuracy: 0.4992 - val_auc: 0.6169\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 16s 409ms/step - loss: 1.2121 - accuracy: 0.4797 - auc: 0.5221 - val_loss: 0.6470 - val_accuracy: 0.5800 - val_auc: 0.6471\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 16s 404ms/step - loss: 1.2063 - accuracy: 0.4750 - auc: 0.5248 - val_loss: 0.7936 - val_accuracy: 0.4992 - val_auc: 0.5678\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 16s 408ms/step - loss: 1.2136 - accuracy: 0.4781 - auc: 0.5269 - val_loss: 0.6863 - val_accuracy: 0.5055 - val_auc: 0.6095\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 16s 406ms/step - loss: 1.2200 - accuracy: 0.4824 - auc: 0.5197 - val_loss: 0.8020 - val_accuracy: 0.4992 - val_auc: 0.4476\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 16s 409ms/step - loss: 1.2172 - accuracy: 0.4762 - auc: 0.5168 - val_loss: 0.7051 - val_accuracy: 0.4992 - val_auc: 0.6023\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 16s 400ms/step - loss: 1.2210 - accuracy: 0.4871 - auc: 0.5336 - val_loss: 0.9931 - val_accuracy: 0.5040 - val_auc: 0.5098\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 16s 408ms/step - loss: 1.2127 - accuracy: 0.4715 - auc: 0.5166 - val_loss: 0.7942 - val_accuracy: 0.5198 - val_auc: 0.5042\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 16s 412ms/step - loss: 1.2110 - accuracy: 0.4844 - auc: 0.5309 - val_loss: 0.7258 - val_accuracy: 0.5040 - val_auc: 0.5797\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 16s 408ms/step - loss: 1.2068 - accuracy: 0.4762 - auc: 0.5204 - val_loss: 0.8239 - val_accuracy: 0.5071 - val_auc: 0.5959\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 16s 405ms/step - loss: 1.2101 - accuracy: 0.4773 - auc: 0.5264 - val_loss: 0.7460 - val_accuracy: 0.4992 - val_auc: 0.5870\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 16s 411ms/step - loss: 1.2059 - accuracy: 0.4742 - auc: 0.5284 - val_loss: 0.6980 - val_accuracy: 0.4992 - val_auc: 0.5985\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 16s 406ms/step - loss: 1.2110 - accuracy: 0.4938 - auc: 0.5301 - val_loss: 0.7570 - val_accuracy: 0.4992 - val_auc: 0.5487\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 16s 410ms/step - loss: 1.2153 - accuracy: 0.4867 - auc: 0.5319 - val_loss: 0.6212 - val_accuracy: 0.6672 - val_auc: 0.7211\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 16s 413ms/step - loss: 1.1976 - accuracy: 0.4707 - auc: 0.5349 - val_loss: 0.6588 - val_accuracy: 0.5626 - val_auc: 0.6227\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 16s 405ms/step - loss: 1.2123 - accuracy: 0.4812 - auc: 0.5330 - val_loss: 0.6314 - val_accuracy: 0.5990 - val_auc: 0.6722\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 16s 400ms/step - loss: 1.2182 - accuracy: 0.4809 - auc: 0.5330 - val_loss: 0.6706 - val_accuracy: 0.5119 - val_auc: 0.6057\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 16s 398ms/step - loss: 1.2118 - accuracy: 0.4887 - auc: 0.5431 - val_loss: 0.7277 - val_accuracy: 0.5119 - val_auc: 0.5671\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 16s 403ms/step - loss: 1.1930 - accuracy: 0.4691 - auc: 0.5440 - val_loss: 0.7093 - val_accuracy: 0.4992 - val_auc: 0.5819\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 16s 400ms/step - loss: 1.2146 - accuracy: 0.4906 - auc: 0.5414 - val_loss: 0.6403 - val_accuracy: 0.5610 - val_auc: 0.6419\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 16s 404ms/step - loss: 1.2119 - accuracy: 0.4812 - auc: 0.5279 - val_loss: 0.7300 - val_accuracy: 0.4992 - val_auc: 0.5484\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 16s 410ms/step - loss: 1.2059 - accuracy: 0.4809 - auc: 0.5399 - val_loss: 0.8581 - val_accuracy: 0.5008 - val_auc: 0.5079\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 16s 404ms/step - loss: 1.2132 - accuracy: 0.4867 - auc: 0.5340 - val_loss: 0.6457 - val_accuracy: 0.5578 - val_auc: 0.6254\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 16s 398ms/step - loss: 1.1954 - accuracy: 0.4777 - auc: 0.5476 - val_loss: 0.6913 - val_accuracy: 0.5055 - val_auc: 0.6082\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 16s 403ms/step - loss: 1.2117 - accuracy: 0.4844 - auc: 0.5363 - val_loss: 0.7646 - val_accuracy: 0.5008 - val_auc: 0.5951\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 16s 400ms/step - loss: 1.2111 - accuracy: 0.4797 - auc: 0.5324 - val_loss: 0.7331 - val_accuracy: 0.5008 - val_auc: 0.5735\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 16s 405ms/step - loss: 1.1947 - accuracy: 0.4863 - auc: 0.5542 - val_loss: 0.6404 - val_accuracy: 0.6022 - val_auc: 0.6616\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 16s 400ms/step - loss: 1.1885 - accuracy: 0.4812 - auc: 0.5512 - val_loss: 0.6435 - val_accuracy: 0.5737 - val_auc: 0.6466\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 16s 398ms/step - loss: 1.2020 - accuracy: 0.4895 - auc: 0.5466 - val_loss: 0.6658 - val_accuracy: 0.5246 - val_auc: 0.5963\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 16s 395ms/step - loss: 1.2102 - accuracy: 0.4883 - auc: 0.5465 - val_loss: 0.6963 - val_accuracy: 0.4976 - val_auc: 0.5607\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 16s 409ms/step - loss: 1.2008 - accuracy: 0.4789 - auc: 0.5436 - val_loss: 0.7583 - val_accuracy: 0.5008 - val_auc: 0.6044\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 16s 400ms/step - loss: 1.2020 - accuracy: 0.4898 - auc: 0.5552 - val_loss: 0.7487 - val_accuracy: 0.4992 - val_auc: 0.5958\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 16s 402ms/step - loss: 1.2123 - accuracy: 0.4750 - auc: 0.5400 - val_loss: 0.7044 - val_accuracy: 0.4992 - val_auc: 0.5923\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 16s 404ms/step - loss: 1.2078 - accuracy: 0.4871 - auc: 0.5461 - val_loss: 0.6925 - val_accuracy: 0.5008 - val_auc: 0.5927\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 16s 409ms/step - loss: 1.2032 - accuracy: 0.4750 - auc: 0.5503 - val_loss: 0.6787 - val_accuracy: 0.5071 - val_auc: 0.6007\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 16s 399ms/step - loss: 1.1843 - accuracy: 0.4812 - auc: 0.5516 - val_loss: 0.6276 - val_accuracy: 0.6593 - val_auc: 0.7185\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 16s 409ms/step - loss: 1.1883 - accuracy: 0.4762 - auc: 0.5512 - val_loss: 0.6739 - val_accuracy: 0.5135 - val_auc: 0.6012\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 17s 412ms/step - loss: 1.1999 - accuracy: 0.4957 - auc: 0.5583 - val_loss: 0.6444 - val_accuracy: 0.6323 - val_auc: 0.6860\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 16s 393ms/step - loss: 1.2046 - accuracy: 0.4852 - auc: 0.5492 - val_loss: 0.7101 - val_accuracy: 0.4992 - val_auc: 0.5669\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 16s 406ms/step - loss: 1.1879 - accuracy: 0.4949 - auc: 0.5631 - val_loss: 0.6970 - val_accuracy: 0.5055 - val_auc: 0.5894\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 16s 408ms/step - loss: 1.1895 - accuracy: 0.4664 - auc: 0.5467 - val_loss: 0.6354 - val_accuracy: 0.5959 - val_auc: 0.6688\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 16s 407ms/step - loss: 1.2084 - accuracy: 0.4930 - auc: 0.5486 - val_loss: 0.6378 - val_accuracy: 0.6212 - val_auc: 0.6679\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 16s 403ms/step - loss: 1.2112 - accuracy: 0.4895 - auc: 0.5519 - val_loss: 0.7214 - val_accuracy: 0.4992 - val_auc: 0.5901\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 16s 399ms/step - loss: 1.2081 - accuracy: 0.4930 - auc: 0.5510 - val_loss: 0.7389 - val_accuracy: 0.4976 - val_auc: 0.5827\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 16s 402ms/step - loss: 1.1887 - accuracy: 0.4891 - auc: 0.5519 - val_loss: 0.6798 - val_accuracy: 0.5008 - val_auc: 0.5728\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 16s 409ms/step - loss: 1.2051 - accuracy: 0.4828 - auc: 0.5515 - val_loss: 0.6702 - val_accuracy: 0.5182 - val_auc: 0.5864\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 16s 413ms/step - loss: 1.1963 - accuracy: 0.4777 - auc: 0.5547 - val_loss: 0.7838 - val_accuracy: 0.4992 - val_auc: 0.5919\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 16s 409ms/step - loss: 1.2076 - accuracy: 0.4828 - auc: 0.5482 - val_loss: 0.6926 - val_accuracy: 0.5119 - val_auc: 0.5933\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 16s 404ms/step - loss: 1.1849 - accuracy: 0.4961 - auc: 0.5651 - val_loss: 0.6696 - val_accuracy: 0.5182 - val_auc: 0.5993\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 16s 411ms/step - loss: 1.1950 - accuracy: 0.4828 - auc: 0.5545 - val_loss: 0.7104 - val_accuracy: 0.5135 - val_auc: 0.5816\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 16s 405ms/step - loss: 1.1888 - accuracy: 0.4926 - auc: 0.5677 - val_loss: 0.6774 - val_accuracy: 0.5198 - val_auc: 0.5904\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 16s 408ms/step - loss: 1.1903 - accuracy: 0.4898 - auc: 0.5493 - val_loss: 0.6311 - val_accuracy: 0.6339 - val_auc: 0.6868\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 16s 405ms/step - loss: 1.1843 - accuracy: 0.4914 - auc: 0.5712 - val_loss: 0.6994 - val_accuracy: 0.5182 - val_auc: 0.5927\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 16s 405ms/step - loss: 1.1768 - accuracy: 0.4801 - auc: 0.5671 - val_loss: 0.6792 - val_accuracy: 0.5087 - val_auc: 0.5893\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 16s 403ms/step - loss: 1.1883 - accuracy: 0.5000 - auc: 0.5715 - val_loss: 0.6763 - val_accuracy: 0.5261 - val_auc: 0.5931\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 16s 404ms/step - loss: 1.1737 - accuracy: 0.4762 - auc: 0.5577 - val_loss: 0.6527 - val_accuracy: 0.5689 - val_auc: 0.6256\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 16s 410ms/step - loss: 1.1888 - accuracy: 0.4922 - auc: 0.5697 - val_loss: 0.7387 - val_accuracy: 0.5008 - val_auc: 0.5747\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 16s 412ms/step - loss: 1.1868 - accuracy: 0.4840 - auc: 0.5628 - val_loss: 0.6935 - val_accuracy: 0.5071 - val_auc: 0.5804\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 16s 403ms/step - loss: 1.1694 - accuracy: 0.4836 - auc: 0.5709 - val_loss: 0.7153 - val_accuracy: 0.5103 - val_auc: 0.5901\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 16s 397ms/step - loss: 1.1766 - accuracy: 0.4930 - auc: 0.5774 - val_loss: 0.7050 - val_accuracy: 0.5087 - val_auc: 0.5820\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 16s 407ms/step - loss: 1.1790 - accuracy: 0.4832 - auc: 0.5701 - val_loss: 0.7183 - val_accuracy: 0.5024 - val_auc: 0.5743\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 16s 404ms/step - loss: 1.1630 - accuracy: 0.4699 - auc: 0.5760 - val_loss: 0.6732 - val_accuracy: 0.5246 - val_auc: 0.5890\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 16s 398ms/step - loss: 1.1697 - accuracy: 0.4926 - auc: 0.5808 - val_loss: 0.6896 - val_accuracy: 0.5135 - val_auc: 0.5852\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 16s 406ms/step - loss: 1.1730 - accuracy: 0.5043 - auc: 0.5799 - val_loss: 0.6755 - val_accuracy: 0.5309 - val_auc: 0.5898\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 16s 410ms/step - loss: 1.1874 - accuracy: 0.4898 - auc: 0.5715 - val_loss: 0.6783 - val_accuracy: 0.5214 - val_auc: 0.5901\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 16s 404ms/step - loss: 1.1615 - accuracy: 0.4879 - auc: 0.5799 - val_loss: 0.7095 - val_accuracy: 0.5087 - val_auc: 0.5770\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 16s 407ms/step - loss: 1.1698 - accuracy: 0.4898 - auc: 0.5751 - val_loss: 0.7245 - val_accuracy: 0.5071 - val_auc: 0.5831\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 16s 405ms/step - loss: 1.1838 - accuracy: 0.5004 - auc: 0.5765 - val_loss: 0.7125 - val_accuracy: 0.5135 - val_auc: 0.5924\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 17s 421ms/step - loss: 1.1778 - accuracy: 0.4875 - auc: 0.5674 - val_loss: 0.7208 - val_accuracy: 0.5087 - val_auc: 0.5852\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 16s 404ms/step - loss: 1.1804 - accuracy: 0.4984 - auc: 0.5784 - val_loss: 0.7087 - val_accuracy: 0.5151 - val_auc: 0.5862\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 16s 404ms/step - loss: 1.1805 - accuracy: 0.5000 - auc: 0.5736 - val_loss: 0.6973 - val_accuracy: 0.5214 - val_auc: 0.5855\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 16s 410ms/step - loss: 1.1643 - accuracy: 0.4941 - auc: 0.5787 - val_loss: 0.6915 - val_accuracy: 0.5214 - val_auc: 0.5866\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 16s 409ms/step - loss: 1.1855 - accuracy: 0.4977 - auc: 0.5761 - val_loss: 0.7119 - val_accuracy: 0.5119 - val_auc: 0.5828\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 16s 404ms/step - loss: 1.1779 - accuracy: 0.5008 - auc: 0.5800 - val_loss: 0.7079 - val_accuracy: 0.5151 - val_auc: 0.5839\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 16s 410ms/step - loss: 1.1735 - accuracy: 0.4891 - auc: 0.5748 - val_loss: 0.7099 - val_accuracy: 0.5135 - val_auc: 0.5846\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 16s 397ms/step - loss: 1.1613 - accuracy: 0.4883 - auc: 0.5823 - val_loss: 0.7000 - val_accuracy: 0.5135 - val_auc: 0.5862\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 16s 402ms/step - loss: 1.1697 - accuracy: 0.5043 - auc: 0.5870 - val_loss: 0.6959 - val_accuracy: 0.5166 - val_auc: 0.5887\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 16s 398ms/step - loss: 1.1788 - accuracy: 0.4949 - auc: 0.5713 - val_loss: 0.6936 - val_accuracy: 0.5166 - val_auc: 0.5884\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 16s 403ms/step - loss: 1.1935 - accuracy: 0.5004 - auc: 0.5726 - val_loss: 0.7019 - val_accuracy: 0.5166 - val_auc: 0.5891\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 16s 414ms/step - loss: 1.1755 - accuracy: 0.5043 - auc: 0.5768 - val_loss: 0.7024 - val_accuracy: 0.5166 - val_auc: 0.5886\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 16s 403ms/step - loss: 1.1650 - accuracy: 0.4879 - auc: 0.5816 - val_loss: 0.7003 - val_accuracy: 0.5182 - val_auc: 0.5865\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 16s 402ms/step - loss: 1.1620 - accuracy: 0.4965 - auc: 0.5870 - val_loss: 0.7022 - val_accuracy: 0.5182 - val_auc: 0.5842\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 16s 407ms/step - loss: 1.1695 - accuracy: 0.5023 - auc: 0.5792 - val_loss: 0.7078 - val_accuracy: 0.5103 - val_auc: 0.5841\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 16s 402ms/step - loss: 1.1599 - accuracy: 0.4754 - auc: 0.5767 - val_loss: 0.6847 - val_accuracy: 0.5182 - val_auc: 0.5896\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 16s 405ms/step - loss: 1.1811 - accuracy: 0.5039 - auc: 0.5783 - val_loss: 0.7129 - val_accuracy: 0.5103 - val_auc: 0.5822\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 16s 401ms/step - loss: 1.1771 - accuracy: 0.4926 - auc: 0.5694 - val_loss: 0.7188 - val_accuracy: 0.5055 - val_auc: 0.5783\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 16s 406ms/step - loss: 1.1796 - accuracy: 0.5027 - auc: 0.5723 - val_loss: 0.7098 - val_accuracy: 0.5103 - val_auc: 0.5777\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 16s 405ms/step - loss: 1.1753 - accuracy: 0.4992 - auc: 0.5775 - val_loss: 0.7108 - val_accuracy: 0.5119 - val_auc: 0.5782\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 16s 398ms/step - loss: 1.1856 - accuracy: 0.4918 - auc: 0.5762 - val_loss: 0.7101 - val_accuracy: 0.5103 - val_auc: 0.5798\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 16s 410ms/step - loss: 1.1601 - accuracy: 0.4988 - auc: 0.5898 - val_loss: 0.7012 - val_accuracy: 0.5166 - val_auc: 0.5820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Challenge model...\n",
      "Running model on Challenge data...\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f91e0fe7f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8df87eed30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.signal import hilbert\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import librosa\n",
    "import librosa.display\n",
    "import math\n",
    "import sys\n",
    "sys.path.insert(0,'/Data/hmd/hmd_sy/evaluation-2022')\n",
    "sys.path.insert(0,'/Data/hmd/hmd_sy/notebooks')\n",
    "sys.path.insert(0,'utils')\n",
    "from helper_code import *\n",
    "from get_feature import *\n",
    "from models import *\n",
    "from Generator0 import *\n",
    "\n",
    "import datetime\n",
    "from evaluate_model import *\n",
    "from scipy import special\n",
    "import scipy.io as sio\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler,ModelCheckpoint\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[3], 'GPU')\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# data_folder =  '/Data/hmd/physionet.org/files/circor-heart-sound/1.0.3/training_data'\n",
    "train_folder =  '/Data/hmd/data_split/murmur/train/'\n",
    "test_folder = '/Data/hmd/data_split/murmur/test/'\n",
    "\n",
    "import typing\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "## filtering (s1, s2 detect)\n",
    "############################\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order):\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    # Get the filter coefficients \n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "############################\n",
    "## feature_extract_bound_melspec\n",
    "############################\n",
    "\n",
    "def feature_extract_bound_melspec(data, samp_sec=20, sr = 4000, pre_emphasis = 0, hop_length=256, win_length = 512, n_mels = 100):\n",
    "    \n",
    "    if samp_sec:\n",
    "        if len(data) > sample_rate * samp_sec :\n",
    "            n_samp = len(data) // int(sample_rate * samp_sec)\n",
    "            signal = []\n",
    "            for i in range(n_samp) :\n",
    "                signal.append(data[ int(sample_rate * samp_sec)*i:(int(sample_rate * samp_sec)*(i+1))])\n",
    "        else :\n",
    "            n_samp = 1\n",
    "            signal = np.zeros(int(sample_rate*samp_sec,))\n",
    "            for i in range(int(sample_rate * samp_sec) // len(data)) :\n",
    "                signal[(i)*len(data):(i+1)*len(data)] = data\n",
    "            num_last = int(sample_rate * samp_sec) - len(data)*(i+1)\n",
    "            signal[(i+1)*len(data):int(sample_rate * samp_sec)] = data[:num_last]\n",
    "            signal = [signal]\n",
    "    else:\n",
    "        n_samp = 1\n",
    "        signal = [data]\n",
    "\n",
    "    Sig = []\n",
    "    for i in range(n_samp) :\n",
    "        if pre_emphasis :\n",
    "            emphasized_signal = np.append(signal[i][0], signal[i][1:] - pre_emphasis * signal[i][:-1])\n",
    "        else :\n",
    "            emphasized_signal = signal[i]\n",
    "\n",
    "        Sig.append(librosa.power_to_db(librosa.feature.melspectrogram(y=emphasized_signal, sr= sr, n_mels=n_mels, n_fft=win_length, hop_length=hop_length, win_length=win_length)))\n",
    "\n",
    "    return Sig\n",
    "\n",
    "\n",
    "\n",
    "class Generator0():\n",
    "    def __init__(self, X_train, y_train, batch_size=32, beta_param=0.2, mixup = True, lowpass = False, highpass = False, ranfilter2 = False, shuffle=True, datagen=None, chaug = False, cout = False):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = beta_param\n",
    "        self.mixup = mixup\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_num = len(y_train)\n",
    "        self.datagen = datagen\n",
    "\n",
    "        ## ffm \n",
    "        \n",
    "        self.lowpass = lowpass\n",
    "        self.highpass = highpass\n",
    "        self.ranfilter = ranfilter2\n",
    "        self.chaug = chaug\n",
    "        self.cutout = cout        \n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            indexes = self.__get_exploration_order()\n",
    "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
    "\n",
    "            for i in range(itr_num):\n",
    "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
    "                X, y = self.__data_generation(batch_ids)\n",
    "\n",
    "                yield X, y\n",
    "\n",
    "    def __get_exploration_order(self):\n",
    "        indexes = np.arange(self.sample_num)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def __data_generation(self, batch_ids):\n",
    "        \n",
    "        \n",
    "        def get_box(lambda_value, nf, nt):\n",
    "            cut_rat = np.sqrt(1.0 - lambda_value)\n",
    "\n",
    "            cut_w = int(nf * cut_rat)  # rw\n",
    "            cut_h = int(nt * cut_rat)  # rh\n",
    "\n",
    "            cut_x = int(np.random.uniform(low=0, high=nf))  # rx\n",
    "            cut_y = int(np.random.uniform(low=0, high=nt))  # ry\n",
    "\n",
    "            boundaryx1 = np.minimum(np.maximum(cut_x - cut_w // 2, 0), nf) #tf.clip_by_value(cut_x - cut_w // 2, 0, IMG_SIZE_x)\n",
    "            boundaryy1 = np.minimum(np.maximum(cut_y - cut_h // 2, 0), nt) #tf.clip_by_value(cut_y - cut_h // 2, 0, IMG_SIZE_y)\n",
    "            bbx2 = np.minimum(np.maximum(cut_x + cut_w // 2, 0), nf) #tf.clip_by_value(cut_x + cut_w // 2, 0, IMG_SIZE_x)\n",
    "            bby2 = np.minimum(np.maximum(cut_y + cut_h // 2, 0), nt) #tf.clip_by_value(cut_y + cut_h // 2, 0, IMG_SIZE_y)\n",
    "\n",
    "            target_h = bby2 - boundaryy1\n",
    "            if target_h == 0:\n",
    "                target_h += 1\n",
    "\n",
    "            target_w = bbx2 - boundaryx1\n",
    "            if target_w == 0:\n",
    "                target_w += 1\n",
    "\n",
    "            return boundaryx1, boundaryy1, target_h, target_w           \n",
    "        \n",
    "        \n",
    "        if isinstance(self.X_train, list):\n",
    "            X = []\n",
    "            for X_temp in self.X_train:\n",
    "                if len(X_temp.shape) == 4: \n",
    "                    _, h, w, c = X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                elif len(X_temp.shape) == 3:\n",
    "                    _, h, w = X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size, 1, 1)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                elif len(X_temp.shape) == 2:\n",
    "                    _, h = X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size, 1)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                elif len(X_temp.shape) == 1:\n",
    "                    _= X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size,)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                \n",
    "                X1 = X_temp[batch_ids[:self.batch_size]].copy()\n",
    "                X2 = X_temp[batch_ids[self.batch_size:]].copy()\n",
    "                \n",
    "                if self.mixup :\n",
    "                    Xn = X1 * X_l + X2 * (1 - X_l)\n",
    "                else :\n",
    "                    Xn = X1\n",
    "                if len(X_temp.shape) == 4: \n",
    "                    _, h, w, c = X_temp.shape\n",
    "                    if h != 1 :\n",
    "                        if self.lowpass :\n",
    "                            uv, lp = self.lowpass\n",
    "                            dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                            for i in range(self.batch_size) :\n",
    "                                loc1 = np.random.choice(lp, size = 1)[0]\n",
    "                                Xn[i,:loc1,:,:] = 0\n",
    "                        if self.highpass :\n",
    "                            uv, hp = self.highpass\n",
    "                            dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                            for i in range(self.batch_size) :\n",
    "                                loc1 = np.random.choice(hp, size = 1)[0]\n",
    "                                Xn[i,loc1:,:,:] = 0\n",
    "                        if self.ranfilter :                \n",
    "                            raniter, ranf = self.ranfilter\n",
    "                            dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "                            for i in range(self.batch_size) :\n",
    "                                if dec1[i] > 0 :\n",
    "                                    for j in range(dec1[i]) :\n",
    "                                        b1 = np.random.choice(ranf, size = 1)[0]\n",
    "                                        loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "                                        Xn[i, loc1:(loc1 + b1 - 1), :] = 0\n",
    "                        if self.chaug :\n",
    "                            for i in range(self.batch_size) :\n",
    "                                noiselv = np.random.uniform(low= - self.chaug, high= self.chaug)\n",
    "                                Xn[i,:] += noiselv\n",
    "                        if self.cutout :\n",
    "                            lambda1 = np.random.beta(self.cutout, self.cutout, size = self.batch_size)   ## beta_param default : 0.7  STC페이퍼 추천은 0.6~0.8\n",
    "                            for i in range(self.batch_size) :\n",
    "                                boundaryx1, boundaryy1, target_h, target_w = get_box(lambda1[i], h, w)\n",
    "                                Xn[i, boundaryx1:(boundaryx1+target_h), boundaryy1:(boundaryy1+target_w),: ] = 0\n",
    "                \n",
    "#                 if len(X_temp.shape) == 3: \n",
    "                    \n",
    "#                     _, h, w = X_temp.shape\n",
    "                    \n",
    "#                     if h != 1 :\n",
    "                        \n",
    "#                         if self.lowpass :\n",
    "#                             uv, lp = self.lowpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(lp, size = 1)[0]\n",
    "#                                 Xn[i,:loc1,:] = 0\n",
    "#                         if self.highpass :\n",
    "#                             uv, hp = self.highpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(hp, size = 1)[0]\n",
    "#                                 Xn[i,loc1:,:] = 0\n",
    "#                         if self.ranfilter :                \n",
    "#                             raniter, ranf = self.ranfilter\n",
    "#                             dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 if dec1[i] > 0 :\n",
    "#                                     for j in range(dec1[i]) :\n",
    "#                                         b1 = np.random.choice(ranf, size = 1)[0]\n",
    "#                                         loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "#                                         Xn[i, loc1:(loc1 + b1 - 1), :] = 0                    \n",
    "                X.append(Xn)\n",
    "        else:\n",
    "            if len(self.X_train.shape) == 4: \n",
    "                _, h, w, c = self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "            elif len(self.X_train.shape) == 3:\n",
    "                _, h, w = self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size, 1, 1)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "            elif len(self.X_train.shape) == 2:\n",
    "                _, h = self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size, 1)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "            elif len(self.X_train.shape) == 1:\n",
    "                _= self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size,)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "\n",
    "            X1 = self.X_train[batch_ids[:self.batch_size]].copy()\n",
    "            X2 = self.X_train[batch_ids[self.batch_size:]].copy()\n",
    "            if self.mixup :\n",
    "                Xn = X1 * X_l + X2 * (1 - X_l)\n",
    "            else :\n",
    "                Xn = X1\n",
    "\n",
    "            if len(self.X_train.shape) == 4: \n",
    "                _, h, w, c = X_temp.shape\n",
    "                if self.lowpass :\n",
    "                    uv, lp = self.lowpass\n",
    "                    dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                    for i in range(self.batch_size) :\n",
    "                        loc1 = np.random.choice(lp, size = 1)[0]\n",
    "                        Xn[i,:loc1,:,:] = 0\n",
    "                if self.highpass :\n",
    "                    uv, hp = self.highpass\n",
    "                    dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                    for i in range(self.batch_size) :\n",
    "                        loc1 = np.random.choice(hp, size = 1)[0]\n",
    "                        Xn[i,loc1:,:,:] = 0\n",
    "                if self.ranfilter :                \n",
    "                    raniter, ranf = self.ranfilter\n",
    "                    dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "                    for i in range(self.batch_size) :\n",
    "                        if dec1[i] > 0 :\n",
    "                            for j in range(dec1[i]) :\n",
    "                                b1 = np.random.choice(ranf, size = 1)[0]\n",
    "                                loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "                                Xn[i, loc1:(loc1 + b1 - 1), :] = 0\n",
    "                \n",
    "#                 if len(self.X_train.shape) == 3:\n",
    "#                     _, h, w = X_temp.shape\n",
    "                    \n",
    "#                     if h != 1 :\n",
    "#                         if self.lowpass :\n",
    "#                             uv, lp = self.lowpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(lp, size = 1)[0]\n",
    "#                                 Xn[i,:loc1,:] = 0\n",
    "#                         if self.highpass :\n",
    "#                             uv, hp = self.highpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(hp, size = 1)[0]\n",
    "#                                 Xn[i,loc1:,:] = 0\n",
    "#                         if self.ranfilter :                \n",
    "#                             raniter, ranf = self.ranfilter\n",
    "#                             dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 if dec1[i] > 0 :\n",
    "#                                     for j in range(dec1[i]) :\n",
    "#                                         b1 = np.random.choice(ranf, size = 1)[0]\n",
    "#                                         loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "#                                         Xn[i, loc1:(loc1 + b1 - 1), :] = 0\n",
    "                \n",
    "            X.append(Xn)\n",
    "\n",
    "                \n",
    "        if self.datagen:\n",
    "            for i in range(self.batch_size):\n",
    "                X[i] = self.datagen.random_transform(X[i])\n",
    "                X[i] = self.datagen.standardize(X[i])\n",
    "\n",
    "        if isinstance(self.y_train, list):\n",
    "            y = []\n",
    "\n",
    "            for y_train_ in self.y_train:\n",
    "                y1 = y_train_[batch_ids[:self.batch_size]].copy()\n",
    "                y2 = y_train_[batch_ids[self.batch_size:]].copy()\n",
    "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
    "        else:\n",
    "            y1 = self.y_train[batch_ids[:self.batch_size]].copy()\n",
    "            y2 = self.y_train[batch_ids[self.batch_size:]].copy()\n",
    "            y = y1 * y_l + y2 * (1 - y_l)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "def get_LCNN_o_1_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape, \n",
    "                       mel_s1s2_input_shape, mel_mm_input_shape, use_s1s2 = True,use_mm = True,\n",
    "                       use_mel = True, use_cqt = True, use_stft = True, \n",
    "                       ord1 = True, dp = .5, fc = False, ext = False, ext2 = False):\n",
    "    # Create a towy model.\n",
    "    age = keras.Input(shape=(6,), name = 'age_cat')\n",
    "    sex = keras.Input(shape=(2,), name = 'sex_cat')\n",
    "    hw = keras.Input(shape=(2,), name = 'height_weight')\n",
    "    preg = keras.Input(shape=(1,), name = 'is_preg')\n",
    "    loc = keras.Input(shape=(5,), name = 'loc')\n",
    "    mel1 = keras.Input(shape=(mel_input_shape), name = 'mel')\n",
    "    cqt1 = keras.Input(shape=(cqt_input_shape), name = 'cqt')\n",
    "    stft1 = keras.Input(shape=(stft_input_shape), name = 'stft')\n",
    "    rr1 = keras.Input(shape=(1,), name = 'rr')\n",
    "    # qrs1 = keras.Input(shape=(1,), name = 'qrs')\n",
    "    \n",
    "    mel1_s1s2 = keras.Input(shape=(mel_s1s2_input_shape), name = 'mel_s1s2')\n",
    "    mel1_mm = keras.Input(shape=(mel_mm_input_shape), name = 'mel_mm')\n",
    "        \n",
    "    ## age embeddig\n",
    "    age1 = layers.Dense(2, activation = None)(age)\n",
    "\n",
    "    ## sex embedding\n",
    "    sex1 = layers.Dense(1, activation = None)(sex)\n",
    "\n",
    "    ## hw embedding\n",
    "    hw1 = layers.Dense(1, activation = None)(hw)\n",
    "\n",
    "    ## loc embedding\n",
    "    loc1 = layers.Dense(3, activation = None)(loc)\n",
    "\n",
    "\n",
    "    ## mel embedding\n",
    "    if use_mel :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        max3 = tf.keras.activations.swish(max3)\n",
    "\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "#         batch6 = layers.LeakyReLU()(batch6)\n",
    "\n",
    "\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        batch10 = tf.keras.activations.swish(batch10)\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(max3.shape[1],1), strides=(2,2))(max3)\n",
    "        u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(u)\n",
    "\n",
    "        # u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(max3)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "        batch10 = batch10 + u\n",
    "        ############################\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        batch13 = tf.keras.activations.swish(batch13)\n",
    "\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "        max16 = tf.keras.activations.swish(max16)\n",
    "\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "        batch19 = tf.keras.activations.swish(batch19)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch10.shape[1],1), strides=(2,2))(batch10)\n",
    "        u = Conv2D(filters=64, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=64, kernel_size=1,  padding='same', activation=None)(batch10)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "\n",
    "        batch19 = batch19 + u\n",
    "        ############################\n",
    "\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "#         batch22 = layers.LeakyReLU()(batch22)\n",
    "\n",
    "\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24) \n",
    "        batch25 = tf.keras.activations.swish(batch25)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch19.shape[1],1), strides=(1,1))(batch19)\n",
    "        u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(batch19)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')(u)\n",
    "\n",
    "        batch25 = batch25 + u\n",
    "        ############################\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        mha = layers.MultiHeadAttention(num_heads=8, key_dim=256)(mfm27,mfm27,mfm27)\n",
    "        mel2 = layers.GlobalAveragePooling2D()(mha)\n",
    "        \n",
    "   # mel1_s1s2\n",
    "    if use_s1s2 :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_s1s2)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_s1s2)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        max3 = tf.keras.activations.swish(max3)\n",
    "\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        batch10 = tf.keras.activations.swish(batch10)\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(max3.shape[1],1), strides=(2,2))(max3)\n",
    "        u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(u)\n",
    "\n",
    "        # u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(max3)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "        batch10 = batch10 + u\n",
    "        ############################\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        batch13 = tf.keras.activations.swish(batch13)\n",
    "\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "        max16 = tf.keras.activations.swish(max16)\n",
    "\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "        batch19 = tf.keras.activations.swish(batch19)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch10.shape[1],1), strides=(2,2))(batch10)\n",
    "        u = Conv2D(filters=64, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=64, kernel_size=1,  padding='same', activation=None)(batch10)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "\n",
    "        batch19 = batch19 + u\n",
    "        ############################\n",
    "\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24) \n",
    "        batch25 = tf.keras.activations.swish(batch25)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch19.shape[1],1), strides=(1,1))(batch19)\n",
    "        u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(batch19)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')(u)\n",
    "\n",
    "        batch25 = batch25 + u\n",
    "        ############################\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        mha = layers.MultiHeadAttention(num_heads=8, key_dim=256)(mfm27,mfm27,mfm27)\n",
    "        mel2_s1s2 = layers.GlobalAveragePooling2D()(mha)\n",
    "        \n",
    "       \n",
    "   # mel1_s1s2\n",
    "    if use_mm :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_mm)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_mm)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        max3 = tf.keras.activations.swish(max3)\n",
    "\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        batch10 = tf.keras.activations.swish(batch10)\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(max3.shape[1],1), strides=(2,2))(max3)\n",
    "        u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(u)\n",
    "\n",
    "        # u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(max3)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "        batch10 = batch10 + u\n",
    "        ############################\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        batch13 = tf.keras.activations.swish(batch13)\n",
    "\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "        max16 = tf.keras.activations.swish(max16)\n",
    "\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "        batch19 = tf.keras.activations.swish(batch19)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch10.shape[1],1), strides=(2,2))(batch10)\n",
    "        u = Conv2D(filters=64, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=64, kernel_size=1,  padding='same', activation=None)(batch10)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "\n",
    "        batch19 = batch19 + u\n",
    "        ############################\n",
    "\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24) \n",
    "        batch25 = tf.keras.activations.swish(batch25)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch19.shape[1],1), strides=(1,1))(batch19)\n",
    "        u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(batch19)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')(u)\n",
    "\n",
    "        batch25 = batch25 + u\n",
    "        ############################\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        mha = layers.MultiHeadAttention(num_heads=8, key_dim=256)(mfm27,mfm27,mfm27)\n",
    "        mel2_mm = layers.GlobalAveragePooling2D()(mha)\n",
    "\n",
    "    if use_cqt :\n",
    "        ## cqt embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "        \n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        cqt2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            cqt2 = Dropout(dp)(cqt2)\n",
    "\n",
    "    if use_stft :\n",
    "        ## stft embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        \n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "        \n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        stft2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            stft2 = Dropout(dp)(stft2)\n",
    "    \n",
    "    \n",
    "    if use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2, stft2])\n",
    "    if not use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([cqt2, stft2])\n",
    "    if use_mel and not use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, stft2])\n",
    "    if use_mel and use_cqt and not use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2])\n",
    "    if not use_mel and not use_cqt and use_stft :  ## stft 만\n",
    "        concat2 = stft2\n",
    "    if use_mel and not use_cqt and not use_stft :  ### mel만\n",
    "        concat2 = mel2\n",
    "    if not use_mel and use_cqt and not use_stft :  ### cqt만\n",
    "        concat2 = cqt2\n",
    "\n",
    "    if ext :\n",
    "        concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg, rr1, mel2_s1s2, mel2_mm])\n",
    "        d1 = layers.Dense(3, activation = 'relu')(concat1)\n",
    "        d1 = layers.Dense(2, activation = 'relu')(d1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1])\n",
    "\n",
    "    if ext2 :\n",
    "        concat1 = layers.Concatenate()([rr1,mel2_s1s2, mel2_mm])\n",
    "        d1 = layers.Dense(3, activation = 'relu')(concat1)\n",
    "        d1 = layers.Dense(2, activation = 'relu')(d1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1])\n",
    "        \n",
    "    if fc :\n",
    "        concat2 = layers.Dense(10, activation = \"relu\")(concat2)\n",
    "        if dp :\n",
    "            concat2 = Dropout(dp)(concat2)\n",
    "        \n",
    "    if ord1 :\n",
    "        res1 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "    else :\n",
    "        res1 = layers.Dense(3, activation = \"softmax\")(concat2)\n",
    "\n",
    "        \n",
    "    res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "\n",
    "    model = keras.Model(inputs = [age,sex,hw,preg,loc,mel1,cqt1,stft1, rr1, mel1_s1s2, mel1_mm] , outputs = res1 )\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy','AUC'])\n",
    "    return(model)\n",
    "\n",
    "\n",
    "def get_LCNN_2_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape, \n",
    "                    mel_s1s2_input_shape, mel_mm_input_shape, use_s1s2 = True,use_mm = True,\n",
    "                     use_mel = True, use_cqt = True, use_stft = True, \n",
    "                    dp = False, fc = False, ext = False, ext2 = False):\n",
    "        # Create a towy model.\n",
    "    age = keras.Input(shape=(6,), name = 'age_cat')\n",
    "    sex = keras.Input(shape=(2,), name = 'sex_cat')\n",
    "    hw = keras.Input(shape=(2,), name = 'height_weight')\n",
    "    preg = keras.Input(shape=(1,), name = 'is_preg')\n",
    "    loc = keras.Input(shape=(5,), name = 'loc')\n",
    "    mel1 = keras.Input(shape=(mel_input_shape), name = 'mel')\n",
    "    cqt1 = keras.Input(shape=(cqt_input_shape), name = 'cqt')\n",
    "    stft1 = keras.Input(shape=(stft_input_shape), name = 'stft')\n",
    "    rr1 = keras.Input(shape=(1,), name = 'rr')\n",
    "    # qrs1 = keras.Input(shape=(1,), name = 'qrs')\n",
    "    \n",
    "    mel1_s1s2 = keras.Input(shape=(mel_s1s2_input_shape), name = 'mel_s1s2')\n",
    "    mel1_mm = keras.Input(shape=(mel_mm_input_shape), name = 'mel_mm')\n",
    "        \n",
    "        \n",
    "    ## age embeddig\n",
    "    age1 = layers.Dense(2, activation = None)(age)\n",
    "\n",
    "    ## sex embedding\n",
    "    sex1 = layers.Dense(1, activation = None)(sex)\n",
    "\n",
    "    ## hw embedding\n",
    "    hw1 = layers.Dense(1, activation = None)(hw)\n",
    "\n",
    "    ## loc embedding\n",
    "    loc1 = layers.Dense(3, activation = None)(loc)\n",
    "\n",
    "   \n",
    "\n",
    "   ## mel embedding\n",
    "    if use_mel :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        \n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        mel2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            mel2 = Dropout(dp)(mel2)\n",
    "        \n",
    "    if use_s1s2:\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_s1s2)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_s1s2)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        \n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        mel2_s1s2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            mel2_s1s2 = Dropout(dp)(mel2_s1s2)\n",
    "            \n",
    "    if use_mm:\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_mm)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_mm)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        \n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        mel2_mm = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            mel2_mm = Dropout(dp)(mel2_mm)\n",
    "\n",
    "    if use_cqt :\n",
    "        ## cqt embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        \n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        cqt2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            cqt2 = Dropout(dp)(cqt2)\n",
    "\n",
    "    if use_stft :\n",
    "        ## stft embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "        \n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        stft2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            stft2 = Dropout(dp)(stft2)\n",
    "    \n",
    "#    concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg])\n",
    "#    d1 = layers.Dense(2, activation = 'relu')(concat1)\n",
    "    \n",
    "    if use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2, stft2])\n",
    "    if not use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([cqt2, stft2])\n",
    "    if use_mel and not use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, stft2])\n",
    "    if use_mel and use_cqt and not use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2])\n",
    "    if not use_mel and not use_cqt and use_stft :  ## stft 만\n",
    "        concat2 = stft2\n",
    "    if use_mel and not use_cqt and not use_stft :  ### mel만\n",
    "        concat2 = mel2\n",
    "    if not use_mel and use_cqt and not use_stft :  ### cqt만\n",
    "        concat2 = cqt2\n",
    "\n",
    "    if ext :\n",
    "        concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg, rr1,mel2_s1s2, mel2_mm])\n",
    "        d1 = layers.Dense(3, activation = 'relu')(concat1)\n",
    "        d1 = layers.Dense(2, activation = 'relu')(d1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1])\n",
    "\n",
    "    if ext2 :\n",
    "        concat1 = layers.Concatenate()([rr1,mel2_s1s2, mel2_mm])\n",
    "        d1 = layers.Dense(3, activation = 'relu')(concat1)\n",
    "        d1 = layers.Dense(2, activation = 'relu')(d1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1])\n",
    "        \n",
    "    if fc :\n",
    "        concat2 = layers.Dense(10, activation = 'relu')(concat2)\n",
    "        if dp :\n",
    "            concat2 = Dropout(dp)(concat2)\n",
    "\n",
    "    res1 = layers.Dense(3, activation = \"softmax\")(concat2)\n",
    "    res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "\n",
    "    model = keras.Model(inputs = [age,sex,hw,preg,loc,mel1,cqt1,stft1, rr1, mel1_s1s2, mel1_mm] , outputs = res2 )\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy','AUC'])\n",
    "    return(model)\n",
    "\n",
    "\n",
    "def sigmoidal_decay(e, start=0, end=100, lr_start=1e-3, lr_end=1e-5):\n",
    "    if e < start:\n",
    "        return lr_start\n",
    "    elif e > end:\n",
    "        return lr_end\n",
    "\n",
    "    middle = (start + end) / 2\n",
    "    s = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return s(13 * (-e + middle) / np.abs(end - start)) * np.abs(lr_start - lr_end) + lr_end\n",
    "\n",
    "patient_files_trn = find_patient_files(train_folder)\n",
    "patient_files_test = find_patient_files(test_folder)\n",
    "\n",
    "# Run model.\n",
    "def run_model(model_folder, data_folder, output_folder, allow_failures, verbose):\n",
    "    # Load models.\n",
    "    if verbose >= 1:\n",
    "        print('Loading Challenge model...')\n",
    "\n",
    "    model = load_challenge_model(model_folder, verbose) ### Teams: Implement this function!!!\n",
    "\n",
    "    # Find the patient data files.\n",
    "    patient_files = find_patient_files(data_folder)\n",
    "    num_patient_files = len(patient_files)\n",
    "\n",
    "    if num_patient_files==0:\n",
    "        raise Exception('No data was provided.')\n",
    "\n",
    "    # Create a folder for the Challenge outputs if it does not already exist.\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Run the team's model on the Challenge data.\n",
    "    if verbose >= 1:\n",
    "        print('Running model on Challenge data...')\n",
    "\n",
    "#    @tf.function\n",
    "    # Iterate over the patient files.\n",
    "    for i in range(num_patient_files):\n",
    "        if verbose >= 2:\n",
    "            print('    {}/{}...'.format(i+1, num_patient_files))\n",
    "\n",
    "        patient_data = load_patient_data(patient_files[i])\n",
    "        recordings = load_recordings(data_folder, patient_data)\n",
    "\n",
    "        # Allow or disallow the model to fail on parts of the data; helpful for debugging.\n",
    "        try:\n",
    "            classes, labels, probabilities = run_challenge_model(model, patient_data, recordings, verbose) ### Teams: Implement this function!!!\n",
    "        except:\n",
    "            if allow_failures:\n",
    "                if verbose >= 2:\n",
    "                    print('... failed.')\n",
    "                classes, labels, probabilities = list(), list(), list()\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # Save Challenge outputs.\n",
    "        head, tail = os.path.split(patient_files[i])\n",
    "        root, extension = os.path.splitext(tail)\n",
    "        output_file = os.path.join(output_folder, root + '.csv')\n",
    "        patient_id = get_patient_id(patient_data)\n",
    "        save_challenge_outputs(output_file, patient_id, classes, labels, probabilities)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print('Done.')\n",
    "        \n",
    "import pickle as pk\n",
    "def save_challenge_model(model_folder, model1, model2, m_name1, m_name2, param_feature) :\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    filename1 = os.path.join(model_folder, m_name1 + '_model1.hdf5')\n",
    "    filename2 = os.path.join(model_folder, m_name2 + '_model2.hdf5')\n",
    "    model1.save(filename1)\n",
    "    model2.save(filename2)\n",
    "    param_feature['model1'] = m_name1\n",
    "    param_feature['model2'] = m_name2\n",
    "    param_feature['model_fnm1'] = filename1\n",
    "    param_feature['model_fnm2'] = filename2\n",
    "    with open(info_fnm, 'wb') as f:\n",
    "        pk.dump(param_feature, f, pk.HIGHEST_PROTOCOL)\n",
    "    return 1\n",
    "\n",
    "def load_challenge_model(model_folder, verbose):\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    with open(info_fnm, 'rb') as f:\n",
    "        info_m = pk.load(f)\n",
    "#    if info_m['model'] == 'toy' :\n",
    "#        model = get_toy(info_m['mel_shape'])\n",
    "#    filename = os.path.join(model_folder, info_m['model'] + '_model.hdf5')\n",
    "#    model.load_weights(filename)\n",
    "    return info_m\n",
    "\n",
    "\n",
    "\n",
    "# Run your trained model. This function is *required*. You should edit this function to add your code, but do *not* change the\n",
    "# arguments of this function.\n",
    "def run_challenge_model(model, data, recordings, verbose):\n",
    "\n",
    "    murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "    outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "    \n",
    "    if model['model1'] == 'lcnn1_dr_rr' :\n",
    "        model1 = get_LCNN_o_1_dr_rr(model['mel_shape'],model['cqt_shape'],model['stft_shape'], \n",
    "                                    model['s1s2_shape'],model['mm_shape'],model['use_s1s2'], model['use_mm'],\n",
    "                                    use_mel = model['use_mel'],use_cqt = model['use_cqt'], use_stft = model['use_stft'],\n",
    "                                    ord1 = model['ord1'], \n",
    "                                    dp = model['dp'], fc = model['fc'], ext = False, ext2 = True)\n",
    "    if model['model2'] == 'lcnn2_dr_rr' :\n",
    "        model2 = get_LCNN_2_dr_rr(model['mel_shape'],model['cqt_shape'],model['stft_shape'], \n",
    "                                  model['s1s2_shape'],model['mm_shape'],model['use_s1s2'], model['use_mm'],\n",
    "                                  use_mel = model['use_mel'],use_cqt = model['use_cqt'], use_stft = model['use_stft'], \n",
    "                                  dp = model['dp'], fc = model['fc'], ext = True, ext2 = False)\n",
    "    model1.load_weights(model['model_fnm1'])\n",
    "    model2.load_weights(model['model_fnm2'])\n",
    "\n",
    "#    classes = model['classes']\n",
    "    # Load features.\n",
    "    features = get_feature_one(data, verbose = 0)\n",
    "\n",
    "    samp_sec = model['samp_sec']\n",
    "    pre_emphasis = model['pre_emphasis']\n",
    "    hop_length = model['hop_length']\n",
    "    win_length = model['win_length']\n",
    "    n_mels = model['n_mels']\n",
    "    filter_scale = model['filter_scale']\n",
    "    n_bins = model['n_bins']\n",
    "    fmin = model['fmin']\n",
    "    use_mel = model['use_mel']\n",
    "    use_cqt = model['use_cqt']\n",
    "    use_stft = model['use_stft']\n",
    "    use_raw = model['use_raw']\n",
    "    trim = model['trim']\n",
    "    use_rr = model['use_rr']\n",
    "    use_mm = model['use_mm']\n",
    "    use_s1s2 = model['use_s1s2']\n",
    "    use_b_detect = True\n",
    "    \n",
    "\n",
    "    features['mel1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_mel :\n",
    "            mel1 = feature_extract_melspec(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length,\n",
    "                                           win_length = win_length, n_mels = n_mels, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1) )\n",
    "        features['mel1'].append(mel1)\n",
    "    M, N = features['mel1'][0].shape\n",
    "\n",
    "    if use_mel :\n",
    "        for i in range(len(features['mel1'])) :\n",
    "            features['mel1'][i] = features['mel1'][i].reshape(M,N,1)\n",
    "    features['mel1'] = np.array(features['mel1'])\n",
    "\n",
    "    features['cqt1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_cqt :\n",
    "            mel1 = feature_extract_cqt(recordings[i], samp_sec=samp_sec, pre_emphasis = pre_emphasis, filter_scale = filter_scale,\n",
    "                                        n_bins = n_bins, fmin = fmin, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1))\n",
    "        features['cqt1'].append(mel1)\n",
    "    M, N = features['cqt1'][0].shape\n",
    "    if use_cqt :\n",
    "        for i in range(len(features['cqt1'])) :\n",
    "            features['cqt1'][i] = features['cqt1'][i].reshape(M,N,1)\n",
    "    features['cqt1'] = np.array(features['cqt1'])\n",
    "\n",
    "    features['stft1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_stft :\n",
    "            mel1 = feature_extract_stft(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length,\n",
    "                                        win_length = win_length, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1) )\n",
    "        features['stft1'].append(mel1)\n",
    "    M, N = features['stft1'][0].shape\n",
    "    if use_stft :\n",
    "        for i in range(len(features['stft1'])) :\n",
    "            features['stft1'][i] = features['stft1'][i].reshape(M,N,1)\n",
    "    features['stft1'] = np.array(features['stft1'])\n",
    "\n",
    "    features['raw1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_raw:\n",
    "            recording1 = recordings[i]\n",
    "            if len(recording1) >= maxlen : \n",
    "                recording1 = recording1[:maxlen]\n",
    "            else :\n",
    "                recording1 = np.pad(recording1, (0, maxlen - len(recording1) ), constant_values=(0,0) )\n",
    "        else :\n",
    "            recording1 = np.zeros((1))\n",
    "        features['raw1'].append(recording1)\n",
    "    features['raw1'] = np.array(features['raw1'])\n",
    "    \n",
    "    \n",
    "    features['rr1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_rr :\n",
    "            try:\n",
    "                recording1 = recordings[i]\n",
    "                ____, info = nk.ecg_process(recording1, sampling_rate=4000)\n",
    "                current_rr = np.mean(np.diff(info['ECG_R_Peaks'])/4000)\n",
    "            except:\n",
    "#                print(filename)\n",
    "                current_rr= 0.6414\n",
    "        else :\n",
    "            current_rr = 0\n",
    "        features['rr1'].append(current_rr)\n",
    "    features['rr1'] = np.array(features['rr1'])\n",
    "    \n",
    "    \n",
    "    features['s1s2_detect1'] = []\n",
    "    features['mm_detect1'] = []\n",
    "    features['s1s2_mel'] = []\n",
    "    features['mm_mel'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_b_detect :\n",
    "            recording1 = recordings[i]\n",
    "         # 1. Amplitude normalization\n",
    "            normal_sig = recording1/np.max(np.abs(recording1))\n",
    "\n",
    "            # 2. Filtering\n",
    "            T = len(recording1)/4000 #time interval; Sample Period ;\n",
    "            fs = 4000 #sample rate \n",
    "            cutoff = 150 #sample frequency \n",
    "\n",
    "            nyq = 0.5 * fs \n",
    "            order = 2  # sin wave can be approx represented as quadratic\n",
    "            n = int(T*fs) #total number of samples \n",
    "\n",
    "            low_ft = butter_lowpass_filter(normal_sig, cutoff, fs, order)\n",
    "\n",
    "            # 3. smoothing of signal envelope\n",
    "            duration = 1.0\n",
    "#                 fs = 4000.0\n",
    "            samples = int(fs*duration)\n",
    "            t = np.arange(len(low_ft)) / fs\n",
    "\n",
    "            analytic_signal = hilbert(low_ft)\n",
    "            amplitude_envelope = np.abs(analytic_signal)\n",
    "\n",
    "            # threshld selection\n",
    "            mu = np.sum(amplitude_envelope)/len(amplitude_envelope)\n",
    "            var = np.sum((amplitude_envelope-mu)**2)/len(amplitude_envelope)\n",
    "            t_sh = mu + var +0.05\n",
    "\n",
    "            thres_list = np.argwhere(amplitude_envelope > t_sh)\n",
    "            save = []\n",
    "            for i in thres_list:\n",
    "                j = i[0]\n",
    "                save.append(j)\n",
    "\n",
    "            packet = []\n",
    "            tmp = []\n",
    "            v = save.pop(0)\n",
    "            tmp.append(v)\n",
    "\n",
    "            while(len(save)>0):\n",
    "                vv = save.pop(0)\n",
    "                if v+1 == vv:\n",
    "                    tmp.append(vv)\n",
    "                    v = vv\n",
    "                else:\n",
    "                    packet.append(tmp)\n",
    "                    tmp = []\n",
    "                    tmp.append(vv)\n",
    "                    v = vv\n",
    "\n",
    "            packet.append(tmp)\n",
    "#                 # thresh hold 보다 크지만 연속값이 3개 미만인 리스트 찾아서 삭제\n",
    "#                 packet2 = []\n",
    "#                 for i in range(len(packet)):\n",
    "#                     if len(packet[i]) < 3:\n",
    "#                         j = packet.index(packet[i])\n",
    "#                         packet2.append(j)\n",
    "\n",
    "#                 for i in packet2:\n",
    "#                     del packet[i]\n",
    "\n",
    "            min_list = []\n",
    "            max_list = []\n",
    "            for i in range(len(packet)):\n",
    "                min_find = min(packet[i])\n",
    "                max_find = max(packet[i])\n",
    "                min_list.append(min_find)\n",
    "                max_list.append(max_find)\n",
    "\n",
    "            # s1, s2 boundary가 여러개 그려짐. 추가적인 전처리 필요. 60개보다 커야함\n",
    "            packet_cp = packet.copy()\n",
    "            new_list = []\n",
    "            first_list = []\n",
    "            last_list = []\n",
    "            for i in range(len(max_list)-1):\n",
    "                j = i + 1\n",
    "                # 연속적인 값인 경우 삭제\n",
    "                if abs(max_list[i]-min_list[j]) < 60:\n",
    "                    first = max_list.index(max_list[i])\n",
    "                    last = min_list.index(min_list[j])\n",
    "                    re_join = packet_cp[first]+packet_cp[last]\n",
    "                    new_list.append(re_join)\n",
    "                    first_list.append(first)\n",
    "                    last_list.append(last)\n",
    "\n",
    "            # 연속적인 값 인덱스 찾아서 final_list 만들기\n",
    "            final_list = first_list+last_list\n",
    "            final_list.sort()\n",
    "            set(final_list)\n",
    "\n",
    "            # 위에서 찾은 final_linst에 들어있는 인덱스 위치는 0으로 처리\n",
    "            drop_list = packet_cp.copy()\n",
    "            for i in final_list:\n",
    "                drop_list[i] = 0\n",
    "#             print(len(drop_list))\n",
    "\n",
    "            seq_remake = drop_list+new_list\n",
    "#             print(len(seq_remake))\n",
    "\n",
    "\n",
    "\n",
    "            # 0으로 전처리한 값 삭제\n",
    "            remove_set = [0]\n",
    "\n",
    "            li = [i for i in seq_remake if i not in remove_set]\n",
    "#             print(li)\n",
    "\n",
    "            # 추가 전처리 후 다시 min, max 출력\n",
    "            min_list1 = []\n",
    "            max_list1 = []\n",
    "            for i in range(len(li)):\n",
    "                min_find1 = min(li[i])\n",
    "                max_find1 = max(li[i])\n",
    "                min_list1.append(min_find1)\n",
    "                max_list1.append(max_find1)\n",
    "\n",
    "            min_list1.sort()\n",
    "            max_list1.sort()\n",
    "\n",
    "\n",
    "            # boundary detected s1 and s2\n",
    "            s_detect = amplitude_envelope.copy()\n",
    "            for i in range(len(max_list1)-1):\n",
    "                j = i+1\n",
    "                s_detect[max_list1[i]:min_list1[j]] = 0\n",
    "            s1s2_detect = s_detect.reshape(1, s_detect.shape[0])\n",
    "            s1s2_detect = s1s2_detect.tolist()\n",
    "            s1s2_detect = pad_sequences(s1s2_detect, maxlen = 80000, dtype ='float64', padding = 'post', truncating ='post', value=0.0)\n",
    "\n",
    "            # s1s2_detect 변수에 mel 적용\n",
    "            s1s2_mel = feature_extract_bound_melspec(s_detect)[0]\n",
    "\n",
    "            # boundary detected systolic and diastolic murmurs present in pcg signal\n",
    "            mm_detect = amplitude_envelope.copy()\n",
    "            for i in range(len(max_list1)):\n",
    "                mm_detect[min_list1[i]:max_list1[i]+1] = 0\n",
    "            murmur_detect = mm_detect.reshape(1, mm_detect.shape[0])\n",
    "            murmur_detect = murmur_detect.tolist()\n",
    "            murmur_detect = pad_sequences(murmur_detect, maxlen = 80000, dtype ='float64', padding = 'post', truncating ='post', value=0.0)\n",
    "\n",
    "            # murmur_dect 변수에 mel 적용\n",
    "            mm_mel = feature_extract_bound_melspec(mm_detect)[0]\n",
    "\n",
    "        else :\n",
    "            s1s2_detect = np.zeros((1,1))\n",
    "            murmur_detect = np.zeros((1,1))\n",
    "            s1s2_mel = np.zeros( (1,1,1) )\n",
    "            mm_mel = np.zeros( (1,1,1) )\n",
    "\n",
    "        features['s1s2_detect1'].append(s1s2_detect)\n",
    "        features['mm_detect1'].append(murmur_detect)\n",
    "        features['s1s2_mel'].append(s1s2_mel)\n",
    "        features['mm_mel'].append(mm_mel)\n",
    "\n",
    "    features['s1s2_detect1'] = np.array(features['s1s2_detect1'])\n",
    "    features['mm_detect1'] = np.array(features['mm_detect1'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    M, N = features['s1s2_mel'][0].shape\n",
    "    if use_s1s2:\n",
    "        for i in range(len(features['s1s2_mel'])):\n",
    "            features['s1s2_mel'][i] = features['s1s2_mel'][i].reshape(M,N,1)\n",
    "    features['s1s2_mel'] = np.array(features['s1s2_mel'])\n",
    "    \n",
    "    \n",
    "    M, N = features['mm_mel'][0].shape\n",
    "    if use_mm:\n",
    "        for i in range(len(features['mm_mel'])):\n",
    "            features['mm_mel'][i] = features['mm_mel'][i].reshape(M,N,1)\n",
    "    features['mm_mel'] = np.array(features['mm_mel'])\n",
    "\n",
    "\n",
    "    # Impute missing data.\n",
    "    res1 = model1.predict([features['age'], features['sex'], features['hw'], features['preg'], features['loc'], \n",
    "                           features['mel1'],features['cqt1'], features['stft1'], features['rr1'], \n",
    "                           features['s1s2_mel'], features['mm_mel']])\n",
    "    res2 = model2.predict([features['age'], features['sex'], features['hw'], features['preg'], features['loc'], \n",
    "                           features['mel1'], features['cqt1'], features['stft1'], features['rr1'], \n",
    "                           features['s1s2_mel'], features['mm_mel']])\n",
    "\n",
    "    # Get classifier probabilities.\n",
    "    if model['ord1'] :\n",
    "        idx1 = res1.argmax(axis=0)[0]\n",
    "        murmur_p = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "        murmur_probabilities = np.zeros((3,))\n",
    "        murmur_probabilities[0] = murmur_p[0]\n",
    "        murmur_probabilities[1] = 0\n",
    "        murmur_probabilities[2] = murmur_p[1]\n",
    "        outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "    else :\n",
    "        if model['mm_mean'] :\n",
    "            murmur_probabilities = res1.mean(axis = 0)\n",
    "        else :\n",
    "            idx1 = res1.argmax(axis=0)[0]\n",
    "            murmur_probabilities = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "        outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "\n",
    "\n",
    "\n",
    "    ## 이부분도 생각 필요.. rule 을 cost를 maximize 하는 기준으로 threshold 탐색 필요할지도..\n",
    "    # Choose label with highest probability.\n",
    "    murmur_labels = np.zeros(len(murmur_classes), dtype=np.int_)\n",
    "    if murmur_probabilities[0] > 0.496 :\n",
    "        idx = 0\n",
    "    else :\n",
    "        idx = 2\n",
    "#    idx = np.argmax(murmur_probabilities)\n",
    "    murmur_labels[idx] = 1\n",
    "\n",
    "    outcome_labels = np.zeros(len(outcome_classes), dtype=np.int_)\n",
    "    if outcome_probabilities[0] > 0.617 :\n",
    "        idx = 0\n",
    "    else :\n",
    "        idx = 1\n",
    "#    idx = np.argmax(outcome_probabilities)\n",
    "    outcome_labels[idx] = 1\n",
    "\n",
    "    # Concatenate classes, labels, and probabilities.\n",
    "    classes = murmur_classes + outcome_classes\n",
    "    labels = np.concatenate((murmur_labels, outcome_labels))\n",
    "    probabilities = np.concatenate((murmur_probabilities, outcome_probabilities))\n",
    "\n",
    "    return classes, labels, probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_folder = 'hyper_1_1'\n",
    "output_folder = '/Data/hmd/hmd_sy/2021_hmd/tmp/out_hyper_1_1'\n",
    "\n",
    "# maxlen = np.random.choice([120000,80000, 50000, 15000])\n",
    "winlen = 512\n",
    "hoplen = 256\n",
    "nmel = 140 #np.random.choice([100, 120, 140])\n",
    "nsec = 50\n",
    "trim = 0 #np.random.choice([0,2000, 4000])\n",
    "use_mel = True\n",
    "use_cqt = False #np.random.choice([True,False])\n",
    "use_stft = False#np.random.choice([True, False])\n",
    "use_rr = True\n",
    "# use_rr_seq = False #True\n",
    "use_raw = False #True\n",
    "\n",
    "use_b_detect = True\n",
    "use_s1s2 = True\n",
    "use_mm = True\n",
    "\n",
    "#################\n",
    "# envelope parameter\n",
    "#################\n",
    "samp_sec = 50\n",
    "sample_rate = 4000\n",
    "pre_emphasis  = 0\n",
    "sr = 4000\n",
    "n_mels = 140\n",
    "\n",
    "# maxlen = 120000\n",
    "win_length = 512\n",
    "hop_length = 256\n",
    "\n",
    "fs = 4000 #sample rate \n",
    "cutoff = 150 #sample frequency \n",
    "nyq = 0.5 * fs \n",
    "\n",
    "\n",
    "\n",
    "params_feature = {'samp_sec': nsec,\n",
    "            #### melspec, stft 피쳐 옵션들  \n",
    "            'pre_emphasis': 0,\n",
    "            'hop_length': hoplen,\n",
    "            'win_length':winlen,\n",
    "            'n_mels': nmel,\n",
    "            #### cqt 피쳐 옵션들  \n",
    "            'filter_scale': 1,\n",
    "            'n_bins': 80,\n",
    "            'fmin': 10,\n",
    "\n",
    "            ### 사용할 피쳐 지정\n",
    "                'trim' : trim, # 앞뒤 얼마나 자를지? 4000 이면 1초\n",
    "                'use_rr' : use_rr,\n",
    "                'use_b_detect': use_b_detect,\n",
    "                'use_raw' : use_raw,\n",
    "                'use_mel' : use_mel,\n",
    "                'use_cqt' : use_cqt,\n",
    "                'use_stft' : use_stft          \n",
    "}\n",
    "\n",
    "\n",
    "mm_weight = 3 #np.random.choice([2,3,4,5])\n",
    "oo_weight = 3 #np.random.choice([2,3,4,5,6])\n",
    "ord1 = True #np.random.choice([True,False])\n",
    "mm_mean = False #np.random.choice([True,False])\n",
    "dp = 0 #np.random.choice([0, .1, .2, .3])\n",
    "fc = False #np.random.choice([True,False])\n",
    "\n",
    "\n",
    "ext = True\n",
    "\n",
    "\n",
    "chaug = 10 #np.random.choice([0, 10])\n",
    "mixup = True #np.random.choice([True,False])\n",
    "cout = .8 #np.random.choice([0, 0.8])\n",
    "wunknown = 1 #np.random.choice([1, 0.7, .5, .2])\n",
    "n1 = 0 #np.random.choice([0,2])\n",
    "if n1 == 0 :\n",
    "    ranfil = False\n",
    "else :\n",
    "    ranfil = [n1, [18,19,20,21,22,23]]\n",
    "    \n",
    "use_mel = params_feature['use_mel']\n",
    "use_cqt = params_feature['use_cqt']\n",
    "use_stft = params_feature['use_stft']\n",
    "nep = 100\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/features_trn_envel.pkl','rb') as f:\n",
    "    features_trn = pickle.load(f)\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/mm_lbs_trn_envel.pkl','rb') as f:\n",
    "    mm_lbs_trn = pickle.load(f)\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/out_lbs_trn_envel.pkl','rb') as f:\n",
    "    out_lbs_trn = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/features_test_envel.pkl','rb') as f:\n",
    "    features_test = pickle.load(f)\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/mm_lbs_test_envel.pkl','rb') as f:\n",
    "    mm_lbs_test = pickle.load(f)\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/out_lbs_test_envel.pkl','rb') as f:\n",
    "    out_lbs_test = pickle.load(f)\n",
    "    \n",
    "# (2532, 140, 782) 에서 (2532, 140, 782, 1)로 변경\n",
    "a, b, c = features_trn['mel1'].shape\n",
    "features_trn['mel1']= features_trn['mel1'].reshape(a,b,c,1)\n",
    "\n",
    "a, b, c = features_trn['s1s2_mel'].shape\n",
    "features_trn['s1s2_mel'] = features_trn['s1s2_mel'].reshape(a,b,c,1)\n",
    "\n",
    "a, b, c = features_trn['mm_mel'].shape\n",
    "features_trn['mm_mel'] = features_trn['mm_mel'].reshape(a,b,c,1)\n",
    "\n",
    "mel_input_shape = features_trn['mel1'][0].shape\n",
    "cqt_input_shape = features_trn['cqt1'][0].shape\n",
    "stft_input_shape = features_trn['stft1'][0].shape\n",
    "\n",
    "mel_s1s2_input_shape = features_trn['s1s2_mel'][0].shape\n",
    "mel_mm_input_shape = features_trn['mm_mel'][0].shape\n",
    "\n",
    "\n",
    "params_feature['ord1'] = ord1\n",
    "params_feature['mm_mean'] = mm_mean\n",
    "params_feature['dp'] = dp\n",
    "params_feature['fc'] = fc\n",
    "params_feature['ext'] = ext\n",
    "params_feature['oo_weight'] = oo_weight\n",
    "params_feature['mm_weight'] = mm_weight\n",
    "params_feature['chaug'] = chaug\n",
    "params_feature['cout'] = cout\n",
    "params_feature['wunknown'] = wunknown\n",
    "params_feature['mixup'] = mixup\n",
    "params_feature['n1'] = n1\n",
    "\n",
    "params_feature['mel_shape'] = mel_input_shape\n",
    "params_feature['cqt_shape'] = cqt_input_shape\n",
    "params_feature['stft_shape'] = stft_input_shape\n",
    "\n",
    "params_feature['s1s2_shape'] = mel_s1s2_input_shape\n",
    "params_feature['mm_shape'] = mel_mm_input_shape\n",
    "\n",
    "params_feature['use_mel'] = use_mel\n",
    "params_feature['use_cqt'] = use_cqt\n",
    "params_feature['use_stft'] = use_stft\n",
    "\n",
    "params_feature['use_rr'] = use_rr\n",
    "params_feature['use_s1s2'] = use_s1s2\n",
    "params_feature['use_mm'] = use_mm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(params_feature)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "model1 = get_LCNN_o_1_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape, \n",
    "                            mel_s1s2_input_shape, mel_mm_input_shape, use_s1s2 = use_s1s2, use_mm = use_mm,\n",
    "                            use_mel = use_mel, use_cqt = use_cqt, use_stft = use_stft, ord1 = ord1, dp = dp, fc = fc, ext = False, ext2 = True)\n",
    "model2 = get_LCNN_2_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape, \n",
    "                        mel_s1s2_input_shape, mel_mm_input_shape, use_s1s2 = use_s1s2, use_mm = use_mm,\n",
    "                        use_mel = use_mel, use_cqt = use_cqt, use_stft = use_stft, dp = dp, fc = fc, ext = True, ext2 = False)\n",
    "\n",
    "n_epoch = nep\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "batch_size = 64\n",
    "\n",
    "if mixup :\n",
    "    beta_param = .7\n",
    "else :\n",
    "    beta_param = 0\n",
    "\n",
    "params = {'batch_size': batch_size,\n",
    "        #          'input_shape': (100, 313, 1),\n",
    "        'shuffle': True,\n",
    "        'chaug': chaug,\n",
    "        'beta_param': beta_param,\n",
    "        'cout': cout\n",
    "#              'mixup': mixup,\n",
    "        #          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "        #          'highpass': [.5, [78,79,80,81,82,83,84,85]]\n",
    "#              'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "        #           'dropblock' : [30, 100]\n",
    "        #'device' : device\n",
    "}\n",
    "\n",
    "if mixup :\n",
    "    params['mixup'] = mixup\n",
    "    params['ranfilter2'] = ranfil\n",
    "else :\n",
    "    params['cutout'] = cout\n",
    "\n",
    "params_no_shuffle = {'batch_size': batch_size,\n",
    "                    #          'input_shape': (100, 313, 1),\n",
    "                    'shuffle': False,\n",
    "                    'beta_param': 0.7,\n",
    "                    'mixup': False\n",
    "                    #'device': device\n",
    "}\n",
    "\n",
    "if ord1 :\n",
    "    class_weight = {0: mm_weight, 1: 1.}\n",
    "else :\n",
    "    class_weight = {0: mm_weight, 1: wunknown, 2:1.}\n",
    "\n",
    "\n",
    "if mixup :\n",
    "        TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                                features_trn['preg'], features_trn['loc'], features_trn['mel1'],\n",
    "                                features_trn['cqt1'],features_trn['stft1'],features_trn['rr1'], \n",
    "                                features_trn['s1s2_mel'],features_trn['mm_mel']], \n",
    "                        mm_lbs_trn,  ## our Y\n",
    "                            **params)()\n",
    "        model1.fit(TrainDGen_1,\n",
    "            validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                                features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                                features_test['cqt1'], features_test['stft1'],features_test['rr1'],\n",
    "                                features_test['s1s2_mel'],features_test['mm_mel']], \n",
    "                                mm_lbs_test), \n",
    "            callbacks=[lr],\n",
    "            steps_per_epoch=np.ceil(len(mm_lbs_trn)/64),\n",
    "            class_weight=class_weight, \n",
    "            epochs = n_epoch)\n",
    "\n",
    "else :\n",
    "    TrainGen = DataGenerator([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                            features_trn['preg'], features_trn['loc'], features_trn['mel1'],\n",
    "                            features_trn['cqt1'],features_trn['stft1'],features_trn['rr1'], \n",
    "                            features_trn['s1s2_mel'],features_trn['mm_mel']], \n",
    "                mm_lbs_trn,  ## our Y\n",
    "                **params)\n",
    "    model1.fit(TrainGen,\n",
    "        validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                            features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                            features_test['cqt1'], features_test['stft1'],features_test['rr1'], \n",
    "                            features_test['s1s2_mel'],features_test['mm_mel']], \n",
    "                            mm_lbs_test), \n",
    "        callbacks=[lr],\n",
    "        #        steps_per_epoch=np.ceil(len(mm_lbs_trn)/64),\n",
    "        class_weight=class_weight, \n",
    "        epochs = n_epoch)\n",
    "    \n",
    "n_epoch = nep\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "batch_size = 64\n",
    "params = {'batch_size': batch_size,\n",
    "        #          'input_shape': (100, 313, 1),\n",
    "        'shuffle': True,\n",
    "        'chaug': chaug,\n",
    "        'beta_param': beta_param,\n",
    "        'cout': cout,\n",
    "#              'mixup': True,\n",
    "        #          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "#            'highpass': [.5, [78,79,80,81,82,83,84,85]],\n",
    "#              'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "        #           'dropblock' : [30, 100]\n",
    "        #'device' : device\n",
    "}\n",
    "\n",
    "\n",
    "if mixup :\n",
    "    params['mixup'] = mixup\n",
    "    params['ranfilter2'] = ranfil\n",
    "else :\n",
    "    params['cutout'] = cout\n",
    "\n",
    "\n",
    "params_no_shuffle = {'batch_size': batch_size,\n",
    "                    #          'input_shape': (100, 313, 1),\n",
    "                    'shuffle': False,\n",
    "                    'beta_param': 0.7,\n",
    "                    'mixup': False\n",
    "                    #'device': device\n",
    "}\n",
    "\n",
    "class_weight = {0: oo_weight, 1: 1.}\n",
    "\n",
    "\n",
    "\n",
    "if mixup :\n",
    "    TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                                features_trn['preg'], features_trn['loc'], features_trn['mel1'],\n",
    "                                features_trn['cqt1'],features_trn['stft1'],features_trn['rr1'], \n",
    "                                features_trn['s1s2_mel'],features_trn['mm_mel']], \n",
    "                                out_lbs_trn,  ## our Y\n",
    "                    **params)()\n",
    "\n",
    "    model2.fit(TrainDGen_1,\n",
    "    validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                                features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                                features_test['cqt1'], features_test['stft1'],features_test['rr1'],\n",
    "                                features_test['s1s2_mel'],features_test['mm_mel']], \n",
    "                                out_lbs_test), \n",
    "        callbacks=[lr],\n",
    "        steps_per_epoch=np.ceil(len(out_lbs_trn)/64),\n",
    "        class_weight=class_weight, \n",
    "        epochs = n_epoch)\n",
    "else :\n",
    "    TrainGen = DataGenerator([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                            features_trn['preg'], features_trn['loc'], features_trn['mel1'],\n",
    "                            features_trn['cqt1'],features_trn['stft1'],features_trn['rr1'], \n",
    "                            features_trn['s1s2_mel'],features_trn['mm_mel']], \n",
    "                            out_lbs_trn,  ## our Y\n",
    "                **params)\n",
    "    model2.fit(TrainGen,\n",
    "        validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                            features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                            features_test['cqt1'], features_test['stft1'],features_test['rr1'],\n",
    "                            features_test['s1s2_mel'],features_test['mm_mel']], \n",
    "                            out_lbs_test), \n",
    "        callbacks=[lr],\n",
    "        class_weight=class_weight, \n",
    "        epochs = n_epoch)\n",
    "    \n",
    "\n",
    "\n",
    "# params_feature['mel_shape'] = mel_input_shape\n",
    "# params_feature['cqt_shape'] = cqt_input_shape\n",
    "# params_feature['stft_shape'] = stft_input_shape\n",
    "\n",
    "# params_feature['use_mel'] = use_mel\n",
    "# params_feature['use_cqt'] = use_cqt\n",
    "# params_feature['use_stft'] = use_stft\n",
    "\n",
    "save_challenge_model(model_folder, model1, model2, m_name1 = 'lcnn1_dr_rr', m_name2 = 'lcnn2_dr_rr', param_feature = params_feature)\n",
    "\n",
    "run_model(model_folder, test_folder, output_folder, allow_failures = True, verbose = 1)\n",
    "\n",
    "murmur_scores, outcome_scores = evaluate_model(test_folder, output_folder)\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "','.join(classes),\n",
    "','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "params_feature['mm_weighted_accuracy'] = weighted_accuracy\n",
    "\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "','.join(classes),\n",
    "','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "+ '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "\n",
    "params_feature['out_cost'] = cost\n",
    "\n",
    "label_folder = test_folder\n",
    "murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "# Load and parse label and model output files.\n",
    "label_files, output_files = find_challenge_files(label_folder, output_folder)\n",
    "murmur_labels = load_murmurs(label_files, murmur_classes)\n",
    "murmur_binary_outputs, murmur_scalar_outputs = load_classifier_outputs(output_files, murmur_classes)\n",
    "outcome_labels = load_outcomes(label_files, outcome_classes)\n",
    "outcome_binary_outputs, outcome_scalar_outputs = load_classifier_outputs(output_files, outcome_classes)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e15f5c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.539,0.817,0.765,18397.315\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.611,0.618,0.696,11893.454\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.736,0.000,0.883\n",
      "Accuracy,0.842,0.000,0.892\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.664,0.558\n",
      "Accuracy,0.735,0.495\n",
      "\n",
      "0.4236521530525847\n",
      "0.5763478509262594\n",
      "0.6618753261279061\n",
      "0.3381246747692842\n"
     ]
    }
   ],
   "source": [
    "murmur_scores, outcome_scores = evaluate_model(test_folder, output_folder)\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "','.join(classes),\n",
    "','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "params_feature['mm_weighted_accuracy'] = weighted_accuracy\n",
    "\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "','.join(classes),\n",
    "','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "+ '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "print(output_string)\n",
    "\n",
    "\n",
    "\n",
    "label_folder = test_folder\n",
    "murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "# Load and parse label and model output files.\n",
    "label_files, output_files = find_challenge_files(label_folder, output_folder)\n",
    "murmur_labels = load_murmurs(label_files, murmur_classes)\n",
    "murmur_binary_outputs, murmur_scalar_outputs = load_classifier_outputs(output_files, murmur_classes)\n",
    "outcome_labels = load_outcomes(label_files, outcome_classes)\n",
    "outcome_binary_outputs, outcome_scalar_outputs = load_classifier_outputs(output_files, outcome_classes)\n",
    "\n",
    "\n",
    "print(np.mean(murmur_scalar_outputs[:,0]))\n",
    "print(np.mean(murmur_scalar_outputs[:,2]))\n",
    "print(np.mean(outcome_scalar_outputs[:,0]))\n",
    "print(np.mean(outcome_scalar_outputs[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a6f07a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold:  0.01\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.111,0.199,0.512,15140.890\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.332,0.000,0.000\n",
      "Accuracy,1.000,0.000,0.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.05\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.111,0.199,0.512,15140.890\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.332,0.000,0.000\n",
      "Accuracy,1.000,0.000,0.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.1\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.111,0.199,0.512,15140.890\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.332,0.000,0.000\n",
      "Accuracy,1.000,0.000,0.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.15\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.116,0.204,0.515,15131.464\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.333,0.000,0.014\n",
      "Accuracy,1.000,0.000,0.007\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.2\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.146,0.236,0.531,14520.528\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.342,0.000,0.096\n",
      "Accuracy,1.000,0.000,0.050\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.25\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.230,0.335,0.582,12893.220\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.376,0.000,0.313\n",
      "Accuracy,1.000,0.000,0.187\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.3\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.311,0.450,0.642,12611.260\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.427,0.000,0.505\n",
      "Accuracy,1.000,0.000,0.345\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.35\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.395,0.586,0.712,13054.506\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.507,0.000,0.679\n",
      "Accuracy,1.000,0.000,0.532\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.4\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.458,0.691,0.755,14675.289\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.597,0.000,0.779\n",
      "Accuracy,0.974,0.000,0.683\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.45\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.514,0.775,0.787,16149.041\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.349,0.513,0.834,14916.196\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.692,0.000,0.848\n",
      "Accuracy,0.947,0.000,0.806\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.676,0.021\n",
      "Accuracy,0.990,0.011\n",
      "\n",
      "-------------\n",
      "threshold:  0.5\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.539,0.817,0.765,18397.315\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.388,0.524,0.823,14116.529\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.736,0.000,0.883\n",
      "Accuracy,0.842,0.000,0.892\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.676,0.099\n",
      "Accuracy,0.969,0.054\n",
      "\n",
      "-------------\n",
      "threshold:  0.55\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.553,0.838,0.755,19227.344\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.422,0.524,0.789,13394.259\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.759,0.000,0.900\n",
      "Accuracy,0.789,0.000,0.935\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.664,0.180\n",
      "Accuracy,0.918,0.108\n",
      "\n",
      "-------------\n",
      "threshold:  0.6\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.550,0.838,0.722,20062.573\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.540,0.571,0.729,11941.513\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.750,0.000,0.899\n",
      "Accuracy,0.711,0.000,0.957\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.658,0.423\n",
      "Accuracy,0.806,0.323\n",
      "\n",
      "-------------\n",
      "threshold:  0.65\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.559,0.848,0.728,20271.614\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.623,0.623,0.609,13734.114\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.771,0.000,0.906\n",
      "Accuracy,0.711,0.000,0.971\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.621,0.625\n",
      "Accuracy,0.602,0.645\n",
      "\n",
      "-------------\n",
      "threshold:  0.7\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.565,0.853,0.720,20689.767\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.665,0.675,0.551,15702.272\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.788,0.000,0.907\n",
      "Accuracy,0.684,0.000,0.986\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.608,0.723\n",
      "Accuracy,0.490,0.871\n",
      "\n",
      "-------------\n",
      "threshold:  0.75\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.424,0.775,0.507,23612.499\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.610,0.639,0.449,18389.648\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.408,0.000,0.865\n",
      "Accuracy,0.263,0.000,0.993\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.504,0.716\n",
      "Accuracy,0.357,0.935\n",
      "\n",
      "-------------\n",
      "threshold:  0.8\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.769,0.621,0.348,0.749,0.429,24859.317\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.687,0.682,0.502,0.571,0.317,21735.316\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.951,0.500,0.857\n",
      "AUPRC,0.861,0.073,0.930\n",
      "F-measure,0.190,0.000,0.853\n",
      "Accuracy,0.105,0.000,1.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.687,0.687\n",
      "AUPRC,0.742,0.621\n",
      "F-measure,0.317,0.687\n",
      "Accuracy,0.194,0.968\n",
      "\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "# 8/14 15:59\n",
    "for th1 in [0.01, 0.05, 0.1, 0.15,0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8] :\n",
    "    murmur_binary_outputs[:,0] = murmur_scalar_outputs[:,0] > th1\n",
    "    murmur_binary_outputs[:,2] = murmur_scalar_outputs[:,2] > 1 - th1\n",
    "    outcome_binary_outputs[:,0] = outcome_scalar_outputs[:,0] > th1\n",
    "    outcome_binary_outputs[:,1] = outcome_scalar_outputs[:,1] > 1 - th1\n",
    "    # For each patient, set the 'Present' or 'Abnormal' class to positive if no class is positive or if multiple classes are positive.\n",
    "    murmur_labels = enforce_positives(murmur_labels, murmur_classes, 'Present')\n",
    "    murmur_binary_outputs = enforce_positives(murmur_binary_outputs, murmur_classes, 'Present')\n",
    "    outcome_labels = enforce_positives(outcome_labels, outcome_classes, 'Abnormal')\n",
    "    outcome_binary_outputs = enforce_positives(outcome_binary_outputs, outcome_classes, 'Abnormal')\n",
    "    # Evaluate the murmur model by comparing the labels and model outputs.\n",
    "    murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes = compute_auc(murmur_labels, murmur_scalar_outputs)\n",
    "    murmur_f_measure, murmur_f_measure_classes = compute_f_measure(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_accuracy, murmur_accuracy_classes = compute_accuracy(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_weighted_accuracy = compute_weighted_accuracy(murmur_labels, murmur_binary_outputs, murmur_classes) # This is the murmur scoring metric.\n",
    "    murmur_cost = compute_cost(outcome_labels, murmur_binary_outputs, outcome_classes, murmur_classes) # Use *outcomes* to score *murmurs* for the Challenge cost metric, but this is not the actual murmur scoring metric.\n",
    "    murmur_scores = (murmur_classes, murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes, \\\n",
    "                 murmur_f_measure, murmur_f_measure_classes, murmur_accuracy, murmur_accuracy_classes, murmur_weighted_accuracy, murmur_cost)\n",
    "\n",
    "    # Evaluate the outcome model by comparing the labels and model outputs.\n",
    "    outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes = compute_auc(outcome_labels, outcome_scalar_outputs)\n",
    "    outcome_f_measure, outcome_f_measure_classes = compute_f_measure(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_accuracy, outcome_accuracy_classes = compute_accuracy(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_weighted_accuracy = compute_weighted_accuracy(outcome_labels, outcome_binary_outputs, outcome_classes)\n",
    "    outcome_cost = compute_cost(outcome_labels, outcome_binary_outputs, outcome_classes, outcome_classes) # This is the clinical outcomes scoring metric.\n",
    "    outcome_scores = (outcome_classes, outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes, \\\n",
    "                  outcome_f_measure, outcome_f_measure_classes, outcome_accuracy, outcome_accuracy_classes, outcome_weighted_accuracy, outcome_cost)\n",
    "\n",
    "\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "    murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "    outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "                + '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "    print(\"threshold: \", th1)\n",
    "    print(output_string)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb861297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

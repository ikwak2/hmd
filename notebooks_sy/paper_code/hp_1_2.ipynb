{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a68704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 24 15:51:02 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     On   | 00000000:1A:00.0 Off |                    0 |\n",
      "| 34%   47C    P2    64W / 260W |  39631MiB / 46080MiB |     21%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    On   | 00000000:1D:00.0 Off |                    0 |\n",
      "| 30%   31C    P8    14W / 300W |  22577MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    On   | 00000000:1E:00.0 Off |                    0 |\n",
      "| 30%   29C    P8    28W / 300W |      3MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro RTX 8000     On   | 00000000:3D:00.0 Off |                    0 |\n",
      "| 49%   71C    P2   230W / 260W |  44151MiB / 46080MiB |     73%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Quadro RTX 8000     On   | 00000000:3E:00.0 Off |                    0 |\n",
      "| 33%   29C    P8    17W / 260W |      3MiB / 46080MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Quadro RTX 8000     On   | 00000000:41:00.0 Off |                    0 |\n",
      "| 33%   26C    P8     6W / 260W |      3MiB / 46080MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79355e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'samp_sec': 50, 'pre_emphasis': 0, 'hop_length': 256, 'win_length': 512, 'n_mels': 140, 'filter_scale': 1, 'n_bins': 80, 'fmin': 10, 'trim': 0, 'use_rr': True, 'use_b_detect': True, 'use_raw': False, 'use_mel': True, 'use_cqt': False, 'use_stft': False, 'ord1': True, 'mm_mean': False, 'dp': 0, 'fc': False, 'ext': True, 'oo_weight': 3, 'mm_weight': 3, 'chaug': 10, 'cout': 0.8, 'wunknown': 1, 'mixup': True, 'n1': 0, 'mel_shape': (140, 782, 1), 'cqt_shape': (1, 1, 1), 'stft_shape': (1, 1, 1), 's1s2_shape': (100, 313, 1), 'mm_shape': (100, 313, 1), 'use_s1s2': True, 'use_mm': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 15:51:36.592898: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-24 15:51:39.018352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 44177 MB memory:  -> device: 4, name: Quadro RTX 8000, pci bus id: 0000:3e:00.0, compute capability: 7.5\n",
      "2023-05-24 15:51:41.159177: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 15:51:47.454516: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 79s 2s/step - loss: 1.1064 - accuracy: 0.6875 - auc: 0.7603 - val_loss: 0.8959 - val_accuracy: 0.7861 - val_auc: 0.8023\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.9070 - accuracy: 0.7965 - auc: 0.8382 - val_loss: 0.5072 - val_accuracy: 0.7892 - val_auc: 0.8332\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.8515 - accuracy: 0.8203 - auc: 0.8596 - val_loss: 2.1323 - val_accuracy: 0.2108 - val_auc: 0.2819\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8751 - accuracy: 0.8059 - auc: 0.8577 - val_loss: 0.8166 - val_accuracy: 0.2139 - val_auc: 0.2440\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8591 - accuracy: 0.8066 - auc: 0.8614 - val_loss: 1.0125 - val_accuracy: 0.2520 - val_auc: 0.3169\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.8317 - accuracy: 0.8121 - auc: 0.8700 - val_loss: 0.4286 - val_accuracy: 0.8574 - val_auc: 0.8900\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8533 - accuracy: 0.7984 - auc: 0.8628 - val_loss: 0.7301 - val_accuracy: 0.5008 - val_auc: 0.4767\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8295 - accuracy: 0.8199 - auc: 0.8608 - val_loss: 0.7187 - val_accuracy: 0.4532 - val_auc: 0.4874\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.8364 - accuracy: 0.8145 - auc: 0.8698 - val_loss: 0.6838 - val_accuracy: 0.4976 - val_auc: 0.5366\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8034 - accuracy: 0.8191 - auc: 0.8799 - val_loss: 0.7808 - val_accuracy: 0.4612 - val_auc: 0.4757\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8071 - accuracy: 0.8301 - auc: 0.8720 - val_loss: 0.4623 - val_accuracy: 0.8716 - val_auc: 0.9405\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8371 - accuracy: 0.8035 - auc: 0.8678 - val_loss: 1.0232 - val_accuracy: 0.2662 - val_auc: 0.3281\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8095 - accuracy: 0.8164 - auc: 0.8720 - val_loss: 0.5364 - val_accuracy: 0.7464 - val_auc: 0.8500\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7969 - accuracy: 0.8234 - auc: 0.8792 - val_loss: 0.4001 - val_accuracy: 0.8510 - val_auc: 0.9060\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7986 - accuracy: 0.8195 - auc: 0.8794 - val_loss: 0.6452 - val_accuracy: 0.5816 - val_auc: 0.6562\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7811 - accuracy: 0.8273 - auc: 0.8920 - val_loss: 0.3294 - val_accuracy: 0.8859 - val_auc: 0.9361\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8242 - accuracy: 0.8234 - auc: 0.8739 - val_loss: 0.4009 - val_accuracy: 0.8574 - val_auc: 0.9150\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8053 - accuracy: 0.8223 - auc: 0.8892 - val_loss: 0.4885 - val_accuracy: 0.8748 - val_auc: 0.9281\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8356 - accuracy: 0.7992 - auc: 0.8793 - val_loss: 0.4368 - val_accuracy: 0.8764 - val_auc: 0.9382\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8404 - accuracy: 0.8102 - auc: 0.8744 - val_loss: 0.5331 - val_accuracy: 0.8764 - val_auc: 0.9279\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7997 - accuracy: 0.8313 - auc: 0.8795 - val_loss: 0.4031 - val_accuracy: 0.8843 - val_auc: 0.9267\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7974 - accuracy: 0.8320 - auc: 0.8786 - val_loss: 0.4232 - val_accuracy: 0.8938 - val_auc: 0.9387\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8038 - accuracy: 0.8258 - auc: 0.8766 - val_loss: 0.4523 - val_accuracy: 0.8098 - val_auc: 0.9061\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7768 - accuracy: 0.8258 - auc: 0.8948 - val_loss: 0.4341 - val_accuracy: 0.8891 - val_auc: 0.9480\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7947 - accuracy: 0.8211 - auc: 0.8779 - val_loss: 0.8582 - val_accuracy: 0.2742 - val_auc: 0.3453\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8150 - accuracy: 0.8223 - auc: 0.8727 - val_loss: 0.3568 - val_accuracy: 0.8811 - val_auc: 0.9266\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7980 - accuracy: 0.8188 - auc: 0.8805 - val_loss: 0.3961 - val_accuracy: 0.8891 - val_auc: 0.9278\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.8061 - accuracy: 0.8203 - auc: 0.8839 - val_loss: 0.4321 - val_accuracy: 0.8811 - val_auc: 0.9284\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8050 - accuracy: 0.8219 - auc: 0.8815 - val_loss: 0.4132 - val_accuracy: 0.8906 - val_auc: 0.9455\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8048 - accuracy: 0.8172 - auc: 0.8827 - val_loss: 0.4887 - val_accuracy: 0.8067 - val_auc: 0.8974\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8024 - accuracy: 0.8250 - auc: 0.8882 - val_loss: 0.4291 - val_accuracy: 0.8257 - val_auc: 0.8902\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7984 - accuracy: 0.8137 - auc: 0.8844 - val_loss: 0.4785 - val_accuracy: 0.8399 - val_auc: 0.9190\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8082 - accuracy: 0.8160 - auc: 0.8848 - val_loss: 0.3996 - val_accuracy: 0.8906 - val_auc: 0.9514\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7838 - accuracy: 0.8188 - auc: 0.8876 - val_loss: 0.5051 - val_accuracy: 0.8019 - val_auc: 0.8966\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7731 - accuracy: 0.8293 - auc: 0.8929 - val_loss: 0.4698 - val_accuracy: 0.8922 - val_auc: 0.9492\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7783 - accuracy: 0.8262 - auc: 0.8900 - val_loss: 0.4320 - val_accuracy: 0.8653 - val_auc: 0.9072\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7984 - accuracy: 0.8262 - auc: 0.8843 - val_loss: 0.3950 - val_accuracy: 0.8986 - val_auc: 0.9422\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8098 - accuracy: 0.8172 - auc: 0.8819 - val_loss: 0.4661 - val_accuracy: 0.8811 - val_auc: 0.9388\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8149 - accuracy: 0.8117 - auc: 0.8914 - val_loss: 0.3915 - val_accuracy: 0.8986 - val_auc: 0.9437\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7954 - accuracy: 0.8109 - auc: 0.8879 - val_loss: 0.4698 - val_accuracy: 0.8716 - val_auc: 0.9156\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7771 - accuracy: 0.8285 - auc: 0.8885 - val_loss: 0.3834 - val_accuracy: 0.8875 - val_auc: 0.9403\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7880 - accuracy: 0.8141 - auc: 0.8925 - val_loss: 0.4048 - val_accuracy: 0.8922 - val_auc: 0.9453\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7889 - accuracy: 0.8238 - auc: 0.8770 - val_loss: 0.4624 - val_accuracy: 0.8970 - val_auc: 0.9444\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7573 - accuracy: 0.8293 - auc: 0.8925 - val_loss: 0.4222 - val_accuracy: 0.8922 - val_auc: 0.9386\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7919 - accuracy: 0.8184 - auc: 0.8922 - val_loss: 0.3513 - val_accuracy: 0.8859 - val_auc: 0.9470\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7796 - accuracy: 0.8309 - auc: 0.8977 - val_loss: 0.3547 - val_accuracy: 0.8716 - val_auc: 0.9306\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7710 - accuracy: 0.8250 - auc: 0.8985 - val_loss: 0.3897 - val_accuracy: 0.8922 - val_auc: 0.9443\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 61s 2s/step - loss: 0.7815 - accuracy: 0.8129 - auc: 0.8952 - val_loss: 0.4081 - val_accuracy: 0.8574 - val_auc: 0.9257\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7595 - accuracy: 0.8297 - auc: 0.8981 - val_loss: 0.4196 - val_accuracy: 0.9049 - val_auc: 0.9397\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7570 - accuracy: 0.8254 - auc: 0.8941 - val_loss: 0.4519 - val_accuracy: 0.8621 - val_auc: 0.9321\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7599 - accuracy: 0.8246 - auc: 0.8996 - val_loss: 0.4548 - val_accuracy: 0.8906 - val_auc: 0.9470\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7360 - accuracy: 0.8289 - auc: 0.9047 - val_loss: 0.4296 - val_accuracy: 0.9097 - val_auc: 0.9444\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7751 - accuracy: 0.8215 - auc: 0.8990 - val_loss: 0.3440 - val_accuracy: 0.8906 - val_auc: 0.9488\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7679 - accuracy: 0.8238 - auc: 0.8925 - val_loss: 0.3739 - val_accuracy: 0.8970 - val_auc: 0.9482\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7728 - accuracy: 0.8145 - auc: 0.8983 - val_loss: 0.3760 - val_accuracy: 0.9033 - val_auc: 0.9493\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7762 - accuracy: 0.8215 - auc: 0.8963 - val_loss: 0.3766 - val_accuracy: 0.8922 - val_auc: 0.9475\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7582 - accuracy: 0.8281 - auc: 0.9002 - val_loss: 0.3543 - val_accuracy: 0.9002 - val_auc: 0.9469\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7688 - accuracy: 0.8254 - auc: 0.8951 - val_loss: 0.3233 - val_accuracy: 0.9049 - val_auc: 0.9469\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7477 - accuracy: 0.8379 - auc: 0.8992 - val_loss: 0.3348 - val_accuracy: 0.9049 - val_auc: 0.9467\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7426 - accuracy: 0.8309 - auc: 0.9032 - val_loss: 0.3641 - val_accuracy: 0.8891 - val_auc: 0.9445\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7615 - accuracy: 0.8234 - auc: 0.9041 - val_loss: 0.3697 - val_accuracy: 0.9097 - val_auc: 0.9484\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7512 - accuracy: 0.8340 - auc: 0.8996 - val_loss: 0.3221 - val_accuracy: 0.8970 - val_auc: 0.9452\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7610 - accuracy: 0.8266 - auc: 0.9090 - val_loss: 0.4265 - val_accuracy: 0.8827 - val_auc: 0.9325\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7567 - accuracy: 0.8266 - auc: 0.9001 - val_loss: 0.4656 - val_accuracy: 0.8384 - val_auc: 0.9046\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 62s 2s/step - loss: 0.7732 - accuracy: 0.8254 - auc: 0.9010 - val_loss: 0.3841 - val_accuracy: 0.9097 - val_auc: 0.9491\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7596 - accuracy: 0.8266 - auc: 0.9071 - val_loss: 0.3845 - val_accuracy: 0.9065 - val_auc: 0.9525\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7758 - accuracy: 0.8195 - auc: 0.9005 - val_loss: 0.3876 - val_accuracy: 0.8986 - val_auc: 0.9488\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7511 - accuracy: 0.8301 - auc: 0.9087 - val_loss: 0.3618 - val_accuracy: 0.9065 - val_auc: 0.9505\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7808 - accuracy: 0.8320 - auc: 0.8961 - val_loss: 0.3617 - val_accuracy: 0.9113 - val_auc: 0.9514\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7350 - accuracy: 0.8340 - auc: 0.9097 - val_loss: 0.3555 - val_accuracy: 0.9033 - val_auc: 0.9527\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7513 - accuracy: 0.8285 - auc: 0.9069 - val_loss: 0.3631 - val_accuracy: 0.9081 - val_auc: 0.9540\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7527 - accuracy: 0.8316 - auc: 0.9071 - val_loss: 0.3799 - val_accuracy: 0.9144 - val_auc: 0.9544\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7741 - accuracy: 0.8227 - auc: 0.9062 - val_loss: 0.3920 - val_accuracy: 0.9097 - val_auc: 0.9519\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7604 - accuracy: 0.8219 - auc: 0.9111 - val_loss: 0.3947 - val_accuracy: 0.9081 - val_auc: 0.9508\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7673 - accuracy: 0.8324 - auc: 0.9065 - val_loss: 0.3775 - val_accuracy: 0.9049 - val_auc: 0.9512\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 62s 2s/step - loss: 0.7537 - accuracy: 0.8277 - auc: 0.9034 - val_loss: 0.3869 - val_accuracy: 0.9065 - val_auc: 0.9501\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7563 - accuracy: 0.8352 - auc: 0.9067 - val_loss: 0.3929 - val_accuracy: 0.9065 - val_auc: 0.9464\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7432 - accuracy: 0.8328 - auc: 0.9104 - val_loss: 0.3782 - val_accuracy: 0.9033 - val_auc: 0.9470\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7502 - accuracy: 0.8250 - auc: 0.9071 - val_loss: 0.3755 - val_accuracy: 0.9049 - val_auc: 0.9488\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7760 - accuracy: 0.8211 - auc: 0.9039 - val_loss: 0.3938 - val_accuracy: 0.9049 - val_auc: 0.9497\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7627 - accuracy: 0.8238 - auc: 0.9044 - val_loss: 0.3898 - val_accuracy: 0.9033 - val_auc: 0.9491\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7409 - accuracy: 0.8250 - auc: 0.9042 - val_loss: 0.3810 - val_accuracy: 0.9049 - val_auc: 0.9497\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7604 - accuracy: 0.8246 - auc: 0.9054 - val_loss: 0.3946 - val_accuracy: 0.9065 - val_auc: 0.9497\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7533 - accuracy: 0.8246 - auc: 0.9106 - val_loss: 0.3883 - val_accuracy: 0.9017 - val_auc: 0.9490\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7554 - accuracy: 0.8188 - auc: 0.9114 - val_loss: 0.3868 - val_accuracy: 0.9065 - val_auc: 0.9499\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7116 - accuracy: 0.8340 - auc: 0.9068 - val_loss: 0.3780 - val_accuracy: 0.9065 - val_auc: 0.9504\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7423 - accuracy: 0.8289 - auc: 0.9098 - val_loss: 0.3857 - val_accuracy: 0.9081 - val_auc: 0.9507\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7500 - accuracy: 0.8352 - auc: 0.9087 - val_loss: 0.3824 - val_accuracy: 0.9081 - val_auc: 0.9498\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7200 - accuracy: 0.8340 - auc: 0.9087 - val_loss: 0.3620 - val_accuracy: 0.9081 - val_auc: 0.9506\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7387 - accuracy: 0.8293 - auc: 0.9107 - val_loss: 0.3646 - val_accuracy: 0.9081 - val_auc: 0.9510\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7409 - accuracy: 0.8332 - auc: 0.9123 - val_loss: 0.3777 - val_accuracy: 0.9081 - val_auc: 0.9496\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7337 - accuracy: 0.8414 - auc: 0.9073 - val_loss: 0.3861 - val_accuracy: 0.9097 - val_auc: 0.9488\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7513 - accuracy: 0.8395 - auc: 0.9073 - val_loss: 0.3920 - val_accuracy: 0.9113 - val_auc: 0.9483\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7399 - accuracy: 0.8297 - auc: 0.9040 - val_loss: 0.3850 - val_accuracy: 0.9113 - val_auc: 0.9495\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 61s 2s/step - loss: 0.7341 - accuracy: 0.8320 - auc: 0.9047 - val_loss: 0.3850 - val_accuracy: 0.9097 - val_auc: 0.9500\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7576 - accuracy: 0.8230 - auc: 0.9068 - val_loss: 0.3827 - val_accuracy: 0.9081 - val_auc: 0.9499\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7532 - accuracy: 0.8344 - auc: 0.9073 - val_loss: 0.3757 - val_accuracy: 0.9081 - val_auc: 0.9503\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7557 - accuracy: 0.8316 - auc: 0.9091 - val_loss: 0.3829 - val_accuracy: 0.9097 - val_auc: 0.9508\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7746 - accuracy: 0.8297 - auc: 0.9061 - val_loss: 0.3913 - val_accuracy: 0.9160 - val_auc: 0.9506\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7406 - accuracy: 0.8258 - auc: 0.9030 - val_loss: 0.3898 - val_accuracy: 0.9097 - val_auc: 0.9513\n",
      "Epoch 1/100\n",
      "40/40 [==============================] - 17s 376ms/step - loss: 1.3047 - accuracy: 0.4930 - auc: 0.4873 - val_loss: 3.1129 - val_accuracy: 0.5008 - val_auc: 0.5216\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 13s 330ms/step - loss: 1.2815 - accuracy: 0.4918 - auc: 0.4941 - val_loss: 0.7838 - val_accuracy: 0.5341 - val_auc: 0.5665\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 1.2502 - accuracy: 0.4805 - auc: 0.4954 - val_loss: 1.9053 - val_accuracy: 0.4992 - val_auc: 0.4577\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 14s 352ms/step - loss: 1.2327 - accuracy: 0.4801 - auc: 0.4956 - val_loss: 0.9836 - val_accuracy: 0.4992 - val_auc: 0.5665\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.2329 - accuracy: 0.4898 - auc: 0.5026 - val_loss: 1.1760 - val_accuracy: 0.4992 - val_auc: 0.5070\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 14s 347ms/step - loss: 1.2428 - accuracy: 0.4746 - auc: 0.4859 - val_loss: 0.7359 - val_accuracy: 0.5055 - val_auc: 0.5844\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 1.2149 - accuracy: 0.4781 - auc: 0.5049 - val_loss: 0.8903 - val_accuracy: 0.5055 - val_auc: 0.6011\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 1.2209 - accuracy: 0.4891 - auc: 0.5107 - val_loss: 0.6645 - val_accuracy: 0.5911 - val_auc: 0.6298\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.2271 - accuracy: 0.4930 - auc: 0.4982 - val_loss: 0.7847 - val_accuracy: 0.4992 - val_auc: 0.5922\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.2190 - accuracy: 0.4777 - auc: 0.5202 - val_loss: 0.7453 - val_accuracy: 0.5008 - val_auc: 0.6011\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.2161 - accuracy: 0.4781 - auc: 0.5267 - val_loss: 0.6985 - val_accuracy: 0.5008 - val_auc: 0.6023\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 1.1953 - accuracy: 0.4852 - auc: 0.5188 - val_loss: 0.8351 - val_accuracy: 0.4992 - val_auc: 0.5790\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 14s 350ms/step - loss: 1.2134 - accuracy: 0.4840 - auc: 0.5171 - val_loss: 0.7835 - val_accuracy: 0.4992 - val_auc: 0.5506\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 1.2177 - accuracy: 0.4824 - auc: 0.4945 - val_loss: 0.7741 - val_accuracy: 0.5087 - val_auc: 0.6005\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.2099 - accuracy: 0.4898 - auc: 0.5356 - val_loss: 0.8470 - val_accuracy: 0.4992 - val_auc: 0.5953\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 14s 339ms/step - loss: 1.2055 - accuracy: 0.4906 - auc: 0.5168 - val_loss: 0.8486 - val_accuracy: 0.4992 - val_auc: 0.5971\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 1.2150 - accuracy: 0.4977 - auc: 0.5225 - val_loss: 0.6584 - val_accuracy: 0.5594 - val_auc: 0.6211\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 14s 351ms/step - loss: 1.2106 - accuracy: 0.4840 - auc: 0.5180 - val_loss: 0.7835 - val_accuracy: 0.5008 - val_auc: 0.5947\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 14s 353ms/step - loss: 1.2178 - accuracy: 0.4961 - auc: 0.5309 - val_loss: 0.7035 - val_accuracy: 0.4992 - val_auc: 0.6121\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 14s 357ms/step - loss: 1.2010 - accuracy: 0.4941 - auc: 0.5325 - val_loss: 0.7353 - val_accuracy: 0.5024 - val_auc: 0.5661\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 1.1846 - accuracy: 0.4895 - auc: 0.5400 - val_loss: 0.6465 - val_accuracy: 0.6022 - val_auc: 0.6662\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 1.2107 - accuracy: 0.5059 - auc: 0.5349 - val_loss: 0.7385 - val_accuracy: 0.5119 - val_auc: 0.5911\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 1.1974 - accuracy: 0.4902 - auc: 0.5319 - val_loss: 0.6701 - val_accuracy: 0.5357 - val_auc: 0.6063\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 1.1750 - accuracy: 0.5039 - auc: 0.5496 - val_loss: 0.7084 - val_accuracy: 0.5008 - val_auc: 0.5920\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 14s 345ms/step - loss: 1.1948 - accuracy: 0.4945 - auc: 0.5311 - val_loss: 0.7552 - val_accuracy: 0.4992 - val_auc: 0.5841\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.1999 - accuracy: 0.4953 - auc: 0.5284 - val_loss: 0.6627 - val_accuracy: 0.5277 - val_auc: 0.5970\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 1.1877 - accuracy: 0.5012 - auc: 0.5392 - val_loss: 0.7734 - val_accuracy: 0.5055 - val_auc: 0.6150\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.1831 - accuracy: 0.4988 - auc: 0.5419 - val_loss: 0.7988 - val_accuracy: 0.5008 - val_auc: 0.5812\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 1.1975 - accuracy: 0.4996 - auc: 0.5410 - val_loss: 0.8351 - val_accuracy: 0.5008 - val_auc: 0.6103\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.1769 - accuracy: 0.4883 - auc: 0.5379 - val_loss: 0.7211 - val_accuracy: 0.5024 - val_auc: 0.5868\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.1888 - accuracy: 0.5020 - auc: 0.5543 - val_loss: 0.6842 - val_accuracy: 0.5420 - val_auc: 0.5860\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 14s 348ms/step - loss: 1.1717 - accuracy: 0.5129 - auc: 0.5463 - val_loss: 0.7686 - val_accuracy: 0.5119 - val_auc: 0.5876\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 1.1779 - accuracy: 0.5027 - auc: 0.5553 - val_loss: 0.7596 - val_accuracy: 0.5087 - val_auc: 0.5582\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 14s 350ms/step - loss: 1.1719 - accuracy: 0.5063 - auc: 0.5541 - val_loss: 0.7311 - val_accuracy: 0.5008 - val_auc: 0.5826\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.1654 - accuracy: 0.5152 - auc: 0.5645 - val_loss: 0.7588 - val_accuracy: 0.5705 - val_auc: 0.6032\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 14s 347ms/step - loss: 1.1788 - accuracy: 0.5191 - auc: 0.5506 - val_loss: 0.6954 - val_accuracy: 0.5547 - val_auc: 0.6119\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.1585 - accuracy: 0.5172 - auc: 0.5611 - val_loss: 0.6962 - val_accuracy: 0.5642 - val_auc: 0.5761\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 1.1310 - accuracy: 0.5207 - auc: 0.5764 - val_loss: 0.6761 - val_accuracy: 0.5674 - val_auc: 0.6235\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 1.1471 - accuracy: 0.5379 - auc: 0.5744 - val_loss: 0.7491 - val_accuracy: 0.4929 - val_auc: 0.5050\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 1.1489 - accuracy: 0.5484 - auc: 0.5816 - val_loss: 0.6630 - val_accuracy: 0.5927 - val_auc: 0.6357\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 14s 349ms/step - loss: 1.1423 - accuracy: 0.5445 - auc: 0.5681 - val_loss: 0.8847 - val_accuracy: 0.5024 - val_auc: 0.5651\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 14s 347ms/step - loss: 1.1441 - accuracy: 0.5410 - auc: 0.5864 - val_loss: 0.7468 - val_accuracy: 0.5151 - val_auc: 0.5541\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.1062 - accuracy: 0.5621 - auc: 0.6047 - val_loss: 0.7430 - val_accuracy: 0.5341 - val_auc: 0.5549\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 1.1059 - accuracy: 0.5805 - auc: 0.6075 - val_loss: 1.0188 - val_accuracy: 0.5040 - val_auc: 0.5190\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.1040 - accuracy: 0.5828 - auc: 0.6109 - val_loss: 0.6868 - val_accuracy: 0.5769 - val_auc: 0.6243\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 1.0818 - accuracy: 0.6203 - auc: 0.6557 - val_loss: 0.7763 - val_accuracy: 0.5261 - val_auc: 0.5855\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 1.0857 - accuracy: 0.6090 - auc: 0.6341 - val_loss: 0.7179 - val_accuracy: 0.5816 - val_auc: 0.6010\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.0557 - accuracy: 0.6434 - auc: 0.6704 - val_loss: 0.7359 - val_accuracy: 0.5895 - val_auc: 0.5988\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 1.0651 - accuracy: 0.6523 - auc: 0.6641 - val_loss: 0.7816 - val_accuracy: 0.5769 - val_auc: 0.6022\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 1.0607 - accuracy: 0.6598 - auc: 0.6821 - val_loss: 0.7655 - val_accuracy: 0.5166 - val_auc: 0.5442\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.0349 - accuracy: 0.6613 - auc: 0.6867 - val_loss: 0.7532 - val_accuracy: 0.5468 - val_auc: 0.5865\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 1.0373 - accuracy: 0.6777 - auc: 0.6972 - val_loss: 0.8678 - val_accuracy: 0.5499 - val_auc: 0.5724\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.0224 - accuracy: 0.6707 - auc: 0.6888 - val_loss: 0.7543 - val_accuracy: 0.5420 - val_auc: 0.5635\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 0.9958 - accuracy: 0.7164 - auc: 0.7080 - val_loss: 0.8116 - val_accuracy: 0.5420 - val_auc: 0.5425\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 0.9472 - accuracy: 0.7316 - auc: 0.7392 - val_loss: 0.7622 - val_accuracy: 0.5769 - val_auc: 0.6072\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 0.9667 - accuracy: 0.7406 - auc: 0.7455 - val_loss: 0.8158 - val_accuracy: 0.5658 - val_auc: 0.5833\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 0.9517 - accuracy: 0.7367 - auc: 0.7444 - val_loss: 0.7970 - val_accuracy: 0.5420 - val_auc: 0.5665\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 0.9412 - accuracy: 0.7426 - auc: 0.7540 - val_loss: 0.8580 - val_accuracy: 0.5357 - val_auc: 0.5800\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 0.9292 - accuracy: 0.7496 - auc: 0.7560 - val_loss: 0.8493 - val_accuracy: 0.5563 - val_auc: 0.5786\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 13s 339ms/step - loss: 0.9238 - accuracy: 0.7684 - auc: 0.7534 - val_loss: 0.8870 - val_accuracy: 0.5277 - val_auc: 0.5541\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 0.9010 - accuracy: 0.7793 - auc: 0.7669 - val_loss: 0.8516 - val_accuracy: 0.5468 - val_auc: 0.5708\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 14s 339ms/step - loss: 0.9110 - accuracy: 0.7770 - auc: 0.7747 - val_loss: 0.8806 - val_accuracy: 0.5531 - val_auc: 0.5605\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 13s 339ms/step - loss: 0.8898 - accuracy: 0.7758 - auc: 0.7721 - val_loss: 0.8627 - val_accuracy: 0.5468 - val_auc: 0.5629\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 14s 348ms/step - loss: 0.8699 - accuracy: 0.7832 - auc: 0.7838 - val_loss: 0.8673 - val_accuracy: 0.5388 - val_auc: 0.5616\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 0.8608 - accuracy: 0.7855 - auc: 0.7791 - val_loss: 0.9056 - val_accuracy: 0.5468 - val_auc: 0.5644\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 0.8821 - accuracy: 0.7910 - auc: 0.7740 - val_loss: 0.8759 - val_accuracy: 0.5705 - val_auc: 0.5700\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 14s 339ms/step - loss: 0.8767 - accuracy: 0.8051 - auc: 0.7925 - val_loss: 0.8848 - val_accuracy: 0.5436 - val_auc: 0.5695\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 14s 346ms/step - loss: 0.8377 - accuracy: 0.8164 - auc: 0.8029 - val_loss: 0.9067 - val_accuracy: 0.5277 - val_auc: 0.5551\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 0.8450 - accuracy: 0.7930 - auc: 0.7865 - val_loss: 0.9163 - val_accuracy: 0.5547 - val_auc: 0.5649\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 0.8843 - accuracy: 0.8031 - auc: 0.7911 - val_loss: 0.9300 - val_accuracy: 0.5293 - val_auc: 0.5517\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 0.8469 - accuracy: 0.8023 - auc: 0.7927 - val_loss: 0.9042 - val_accuracy: 0.5515 - val_auc: 0.5773\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 0.8343 - accuracy: 0.8125 - auc: 0.7968 - val_loss: 0.9087 - val_accuracy: 0.5325 - val_auc: 0.5604\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 14s 347ms/step - loss: 0.8391 - accuracy: 0.8242 - auc: 0.7969 - val_loss: 0.9078 - val_accuracy: 0.5341 - val_auc: 0.5606\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 0.8536 - accuracy: 0.8035 - auc: 0.7965 - val_loss: 0.8936 - val_accuracy: 0.5468 - val_auc: 0.5650\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 14s 352ms/step - loss: 0.8528 - accuracy: 0.8102 - auc: 0.7913 - val_loss: 0.9201 - val_accuracy: 0.5420 - val_auc: 0.5564\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 0.8648 - accuracy: 0.8066 - auc: 0.7947 - val_loss: 0.9379 - val_accuracy: 0.5452 - val_auc: 0.5548\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 14s 350ms/step - loss: 0.8438 - accuracy: 0.8195 - auc: 0.8018 - val_loss: 0.9331 - val_accuracy: 0.5483 - val_auc: 0.5543\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 0.8436 - accuracy: 0.8078 - auc: 0.7910 - val_loss: 0.9139 - val_accuracy: 0.5578 - val_auc: 0.5647\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 0.8161 - accuracy: 0.8215 - auc: 0.8035 - val_loss: 0.9115 - val_accuracy: 0.5404 - val_auc: 0.5643\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 0.8355 - accuracy: 0.8168 - auc: 0.8027 - val_loss: 0.9189 - val_accuracy: 0.5499 - val_auc: 0.5657\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 0.8326 - accuracy: 0.8125 - auc: 0.7968 - val_loss: 0.9089 - val_accuracy: 0.5357 - val_auc: 0.5708\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 13s 328ms/step - loss: 0.8354 - accuracy: 0.8246 - auc: 0.8003 - val_loss: 0.9235 - val_accuracy: 0.5499 - val_auc: 0.5685\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 0.8088 - accuracy: 0.8363 - auc: 0.8085 - val_loss: 0.9315 - val_accuracy: 0.5547 - val_auc: 0.5656\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 0.8481 - accuracy: 0.8289 - auc: 0.7977 - val_loss: 0.9248 - val_accuracy: 0.5436 - val_auc: 0.5655\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 0.8298 - accuracy: 0.8062 - auc: 0.8017 - val_loss: 0.9280 - val_accuracy: 0.5499 - val_auc: 0.5614\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 0.8354 - accuracy: 0.8289 - auc: 0.7927 - val_loss: 0.9270 - val_accuracy: 0.5594 - val_auc: 0.5660\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 14s 339ms/step - loss: 0.8281 - accuracy: 0.8230 - auc: 0.7982 - val_loss: 0.9214 - val_accuracy: 0.5626 - val_auc: 0.5698\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 0.8283 - accuracy: 0.8332 - auc: 0.8106 - val_loss: 0.9280 - val_accuracy: 0.5531 - val_auc: 0.5666\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 0.8177 - accuracy: 0.8109 - auc: 0.7942 - val_loss: 0.9253 - val_accuracy: 0.5468 - val_auc: 0.5652\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 0.8306 - accuracy: 0.8215 - auc: 0.8050 - val_loss: 0.9308 - val_accuracy: 0.5515 - val_auc: 0.5642\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 0.8338 - accuracy: 0.8230 - auc: 0.8049 - val_loss: 0.9440 - val_accuracy: 0.5515 - val_auc: 0.5627\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 0.8407 - accuracy: 0.8402 - auc: 0.8080 - val_loss: 0.9411 - val_accuracy: 0.5483 - val_auc: 0.5631\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - 14s 345ms/step - loss: 0.8479 - accuracy: 0.8137 - auc: 0.7886 - val_loss: 0.9379 - val_accuracy: 0.5499 - val_auc: 0.5641\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 0.8187 - accuracy: 0.8281 - auc: 0.8117 - val_loss: 0.9431 - val_accuracy: 0.5483 - val_auc: 0.5612\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 13s 339ms/step - loss: 0.8430 - accuracy: 0.8344 - auc: 0.8016 - val_loss: 0.9375 - val_accuracy: 0.5452 - val_auc: 0.5609\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 0.8570 - accuracy: 0.8102 - auc: 0.8019 - val_loss: 0.9372 - val_accuracy: 0.5420 - val_auc: 0.5620\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 14s 348ms/step - loss: 0.8353 - accuracy: 0.8277 - auc: 0.7937 - val_loss: 0.9388 - val_accuracy: 0.5483 - val_auc: 0.5619\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 0.8221 - accuracy: 0.8355 - auc: 0.7970 - val_loss: 0.9315 - val_accuracy: 0.5515 - val_auc: 0.5624\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 0.8334 - accuracy: 0.8297 - auc: 0.8045 - val_loss: 0.9341 - val_accuracy: 0.5515 - val_auc: 0.5629\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 0.8208 - accuracy: 0.8254 - auc: 0.8101 - val_loss: 0.9306 - val_accuracy: 0.5468 - val_auc: 0.5638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Challenge model...\n",
      "Running model on Challenge data...\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff51c09cc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff51c0620d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.signal import hilbert\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import librosa\n",
    "import librosa.display\n",
    "import math\n",
    "import sys\n",
    "sys.path.insert(0,'/Data/hmd/hmd_sy/evaluation-2022')\n",
    "sys.path.insert(0,'/Data/hmd/hmd_sy/notebooks')\n",
    "sys.path.insert(0,'utils')\n",
    "from helper_code import *\n",
    "from get_feature import *\n",
    "from models import *\n",
    "from Generator0 import *\n",
    "\n",
    "import datetime\n",
    "from evaluate_model import *\n",
    "from scipy import special\n",
    "import scipy.io as sio\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler,ModelCheckpoint\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[4], 'GPU')\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# data_folder =  '/Data/hmd/physionet.org/files/circor-heart-sound/1.0.3/training_data'\n",
    "train_folder =  '/Data/hmd/data_split/murmur/train/'\n",
    "test_folder = '/Data/hmd/data_split/murmur/test/'\n",
    "\n",
    "import typing\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "## filtering (s1, s2 detect)\n",
    "############################\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order):\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    # Get the filter coefficients \n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "############################\n",
    "## feature_extract_bound_melspec\n",
    "############################\n",
    "\n",
    "def feature_extract_bound_melspec(data, samp_sec=20, sr = 4000, pre_emphasis = 0, hop_length=256, win_length = 512, n_mels = 100):\n",
    "    \n",
    "    if samp_sec:\n",
    "        if len(data) > sample_rate * samp_sec :\n",
    "            n_samp = len(data) // int(sample_rate * samp_sec)\n",
    "            signal = []\n",
    "            for i in range(n_samp) :\n",
    "                signal.append(data[ int(sample_rate * samp_sec)*i:(int(sample_rate * samp_sec)*(i+1))])\n",
    "        else :\n",
    "            n_samp = 1\n",
    "            signal = np.zeros(int(sample_rate*samp_sec,))\n",
    "            for i in range(int(sample_rate * samp_sec) // len(data)) :\n",
    "                signal[(i)*len(data):(i+1)*len(data)] = data\n",
    "            num_last = int(sample_rate * samp_sec) - len(data)*(i+1)\n",
    "            signal[(i+1)*len(data):int(sample_rate * samp_sec)] = data[:num_last]\n",
    "            signal = [signal]\n",
    "    else:\n",
    "        n_samp = 1\n",
    "        signal = [data]\n",
    "\n",
    "    Sig = []\n",
    "    for i in range(n_samp) :\n",
    "        if pre_emphasis :\n",
    "            emphasized_signal = np.append(signal[i][0], signal[i][1:] - pre_emphasis * signal[i][:-1])\n",
    "        else :\n",
    "            emphasized_signal = signal[i]\n",
    "\n",
    "        Sig.append(librosa.power_to_db(librosa.feature.melspectrogram(y=emphasized_signal, sr= sr, n_mels=n_mels, n_fft=win_length, hop_length=hop_length, win_length=win_length)))\n",
    "\n",
    "    return Sig\n",
    "\n",
    "\n",
    "\n",
    "class Generator0():\n",
    "    def __init__(self, X_train, y_train, batch_size=32, beta_param=0.2, mixup = True, lowpass = False, highpass = False, ranfilter2 = False, shuffle=True, datagen=None, chaug = False, cout = False):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = beta_param\n",
    "        self.mixup = mixup\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_num = len(y_train)\n",
    "        self.datagen = datagen\n",
    "\n",
    "        ## ffm \n",
    "        \n",
    "        self.lowpass = lowpass\n",
    "        self.highpass = highpass\n",
    "        self.ranfilter = ranfilter2\n",
    "        self.chaug = chaug\n",
    "        self.cutout = cout        \n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            indexes = self.__get_exploration_order()\n",
    "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
    "\n",
    "            for i in range(itr_num):\n",
    "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
    "                X, y = self.__data_generation(batch_ids)\n",
    "\n",
    "                yield X, y\n",
    "\n",
    "    def __get_exploration_order(self):\n",
    "        indexes = np.arange(self.sample_num)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def __data_generation(self, batch_ids):\n",
    "        \n",
    "        \n",
    "        def get_box(lambda_value, nf, nt):\n",
    "            cut_rat = np.sqrt(1.0 - lambda_value)\n",
    "\n",
    "            cut_w = int(nf * cut_rat)  # rw\n",
    "            cut_h = int(nt * cut_rat)  # rh\n",
    "\n",
    "            cut_x = int(np.random.uniform(low=0, high=nf))  # rx\n",
    "            cut_y = int(np.random.uniform(low=0, high=nt))  # ry\n",
    "\n",
    "            boundaryx1 = np.minimum(np.maximum(cut_x - cut_w // 2, 0), nf) #tf.clip_by_value(cut_x - cut_w // 2, 0, IMG_SIZE_x)\n",
    "            boundaryy1 = np.minimum(np.maximum(cut_y - cut_h // 2, 0), nt) #tf.clip_by_value(cut_y - cut_h // 2, 0, IMG_SIZE_y)\n",
    "            bbx2 = np.minimum(np.maximum(cut_x + cut_w // 2, 0), nf) #tf.clip_by_value(cut_x + cut_w // 2, 0, IMG_SIZE_x)\n",
    "            bby2 = np.minimum(np.maximum(cut_y + cut_h // 2, 0), nt) #tf.clip_by_value(cut_y + cut_h // 2, 0, IMG_SIZE_y)\n",
    "\n",
    "            target_h = bby2 - boundaryy1\n",
    "            if target_h == 0:\n",
    "                target_h += 1\n",
    "\n",
    "            target_w = bbx2 - boundaryx1\n",
    "            if target_w == 0:\n",
    "                target_w += 1\n",
    "\n",
    "            return boundaryx1, boundaryy1, target_h, target_w           \n",
    "        \n",
    "        \n",
    "        if isinstance(self.X_train, list):\n",
    "            X = []\n",
    "            for X_temp in self.X_train:\n",
    "                if len(X_temp.shape) == 4: \n",
    "                    _, h, w, c = X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                elif len(X_temp.shape) == 3:\n",
    "                    _, h, w = X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size, 1, 1)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                elif len(X_temp.shape) == 2:\n",
    "                    _, h = X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size, 1)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                elif len(X_temp.shape) == 1:\n",
    "                    _= X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size,)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                \n",
    "                X1 = X_temp[batch_ids[:self.batch_size]].copy()\n",
    "                X2 = X_temp[batch_ids[self.batch_size:]].copy()\n",
    "                \n",
    "                if self.mixup :\n",
    "                    Xn = X1 * X_l + X2 * (1 - X_l)\n",
    "                else :\n",
    "                    Xn = X1\n",
    "                if len(X_temp.shape) == 4: \n",
    "                    _, h, w, c = X_temp.shape\n",
    "                    if h != 1 :\n",
    "                        if self.lowpass :\n",
    "                            uv, lp = self.lowpass\n",
    "                            dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                            for i in range(self.batch_size) :\n",
    "                                loc1 = np.random.choice(lp, size = 1)[0]\n",
    "                                Xn[i,:loc1,:,:] = 0\n",
    "                        if self.highpass :\n",
    "                            uv, hp = self.highpass\n",
    "                            dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                            for i in range(self.batch_size) :\n",
    "                                loc1 = np.random.choice(hp, size = 1)[0]\n",
    "                                Xn[i,loc1:,:,:] = 0\n",
    "                        if self.ranfilter :                \n",
    "                            raniter, ranf = self.ranfilter\n",
    "                            dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "                            for i in range(self.batch_size) :\n",
    "                                if dec1[i] > 0 :\n",
    "                                    for j in range(dec1[i]) :\n",
    "                                        b1 = np.random.choice(ranf, size = 1)[0]\n",
    "                                        loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "                                        Xn[i, loc1:(loc1 + b1 - 1), :] = 0\n",
    "                        if self.chaug :\n",
    "                            for i in range(self.batch_size) :\n",
    "                                noiselv = np.random.uniform(low= - self.chaug, high= self.chaug)\n",
    "                                Xn[i,:] += noiselv\n",
    "                        if self.cutout :\n",
    "                            lambda1 = np.random.beta(self.cutout, self.cutout, size = self.batch_size)   ## beta_param default : 0.7  STC페이퍼 추천은 0.6~0.8\n",
    "                            for i in range(self.batch_size) :\n",
    "                                boundaryx1, boundaryy1, target_h, target_w = get_box(lambda1[i], h, w)\n",
    "                                Xn[i, boundaryx1:(boundaryx1+target_h), boundaryy1:(boundaryy1+target_w),: ] = 0\n",
    "                \n",
    "#                 if len(X_temp.shape) == 3: \n",
    "                    \n",
    "#                     _, h, w = X_temp.shape\n",
    "                    \n",
    "#                     if h != 1 :\n",
    "                        \n",
    "#                         if self.lowpass :\n",
    "#                             uv, lp = self.lowpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(lp, size = 1)[0]\n",
    "#                                 Xn[i,:loc1,:] = 0\n",
    "#                         if self.highpass :\n",
    "#                             uv, hp = self.highpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(hp, size = 1)[0]\n",
    "#                                 Xn[i,loc1:,:] = 0\n",
    "#                         if self.ranfilter :                \n",
    "#                             raniter, ranf = self.ranfilter\n",
    "#                             dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 if dec1[i] > 0 :\n",
    "#                                     for j in range(dec1[i]) :\n",
    "#                                         b1 = np.random.choice(ranf, size = 1)[0]\n",
    "#                                         loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "#                                         Xn[i, loc1:(loc1 + b1 - 1), :] = 0                    \n",
    "                X.append(Xn)\n",
    "        else:\n",
    "            if len(self.X_train.shape) == 4: \n",
    "                _, h, w, c = self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "            elif len(self.X_train.shape) == 3:\n",
    "                _, h, w = self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size, 1, 1)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "            elif len(self.X_train.shape) == 2:\n",
    "                _, h = self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size, 1)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "            elif len(self.X_train.shape) == 1:\n",
    "                _= self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size,)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "\n",
    "            X1 = self.X_train[batch_ids[:self.batch_size]].copy()\n",
    "            X2 = self.X_train[batch_ids[self.batch_size:]].copy()\n",
    "            if self.mixup :\n",
    "                Xn = X1 * X_l + X2 * (1 - X_l)\n",
    "            else :\n",
    "                Xn = X1\n",
    "\n",
    "            if len(self.X_train.shape) == 4: \n",
    "                _, h, w, c = X_temp.shape\n",
    "                if self.lowpass :\n",
    "                    uv, lp = self.lowpass\n",
    "                    dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                    for i in range(self.batch_size) :\n",
    "                        loc1 = np.random.choice(lp, size = 1)[0]\n",
    "                        Xn[i,:loc1,:,:] = 0\n",
    "                if self.highpass :\n",
    "                    uv, hp = self.highpass\n",
    "                    dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                    for i in range(self.batch_size) :\n",
    "                        loc1 = np.random.choice(hp, size = 1)[0]\n",
    "                        Xn[i,loc1:,:,:] = 0\n",
    "                if self.ranfilter :                \n",
    "                    raniter, ranf = self.ranfilter\n",
    "                    dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "                    for i in range(self.batch_size) :\n",
    "                        if dec1[i] > 0 :\n",
    "                            for j in range(dec1[i]) :\n",
    "                                b1 = np.random.choice(ranf, size = 1)[0]\n",
    "                                loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "                                Xn[i, loc1:(loc1 + b1 - 1), :] = 0\n",
    "                \n",
    "#                 if len(self.X_train.shape) == 3:\n",
    "#                     _, h, w = X_temp.shape\n",
    "                    \n",
    "#                     if h != 1 :\n",
    "#                         if self.lowpass :\n",
    "#                             uv, lp = self.lowpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(lp, size = 1)[0]\n",
    "#                                 Xn[i,:loc1,:] = 0\n",
    "#                         if self.highpass :\n",
    "#                             uv, hp = self.highpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(hp, size = 1)[0]\n",
    "#                                 Xn[i,loc1:,:] = 0\n",
    "#                         if self.ranfilter :                \n",
    "#                             raniter, ranf = self.ranfilter\n",
    "#                             dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 if dec1[i] > 0 :\n",
    "#                                     for j in range(dec1[i]) :\n",
    "#                                         b1 = np.random.choice(ranf, size = 1)[0]\n",
    "#                                         loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "#                                         Xn[i, loc1:(loc1 + b1 - 1), :] = 0\n",
    "                \n",
    "            X.append(Xn)\n",
    "\n",
    "                \n",
    "        if self.datagen:\n",
    "            for i in range(self.batch_size):\n",
    "                X[i] = self.datagen.random_transform(X[i])\n",
    "                X[i] = self.datagen.standardize(X[i])\n",
    "\n",
    "        if isinstance(self.y_train, list):\n",
    "            y = []\n",
    "\n",
    "            for y_train_ in self.y_train:\n",
    "                y1 = y_train_[batch_ids[:self.batch_size]].copy()\n",
    "                y2 = y_train_[batch_ids[self.batch_size:]].copy()\n",
    "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
    "        else:\n",
    "            y1 = self.y_train[batch_ids[:self.batch_size]].copy()\n",
    "            y2 = self.y_train[batch_ids[self.batch_size:]].copy()\n",
    "            y = y1 * y_l + y2 * (1 - y_l)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "def get_LCNN_o_1_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape, \n",
    "                       mel_s1s2_input_shape, use_s1s2 = True, use_mm = True,\n",
    "                       use_mel = True, use_cqt = True, use_stft = True, \n",
    "                       ord1 = True, dp = .5, fc = False, ext = False, ext2 = False):\n",
    "    # Create a towy model.\n",
    "    age = keras.Input(shape=(6,), name = 'age_cat')\n",
    "    sex = keras.Input(shape=(2,), name = 'sex_cat')\n",
    "    hw = keras.Input(shape=(2,), name = 'height_weight')\n",
    "    preg = keras.Input(shape=(1,), name = 'is_preg')\n",
    "    loc = keras.Input(shape=(5,), name = 'loc')\n",
    "    mel1 = keras.Input(shape=(mel_input_shape), name = 'mel')\n",
    "    cqt1 = keras.Input(shape=(cqt_input_shape), name = 'cqt')\n",
    "    stft1 = keras.Input(shape=(stft_input_shape), name = 'stft')\n",
    "    rr1 = keras.Input(shape=(1,), name = 'rr')\n",
    "    # qrs1 = keras.Input(shape=(1,), name = 'qrs')\n",
    "    \n",
    "    mel1_s1s2 = keras.Input(shape=(mel_s1s2_input_shape), name = 'mel_s1s2')\n",
    "        \n",
    "    ## age embeddig\n",
    "    age1 = layers.Dense(2, activation = None)(age)\n",
    "\n",
    "    ## sex embedding\n",
    "    sex1 = layers.Dense(1, activation = None)(sex)\n",
    "\n",
    "    ## hw embedding\n",
    "    hw1 = layers.Dense(1, activation = None)(hw)\n",
    "\n",
    "    ## loc embedding\n",
    "    loc1 = layers.Dense(3, activation = None)(loc)\n",
    "\n",
    "\n",
    "    ## mel embedding\n",
    "    if use_mel :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        max3 = tf.keras.activations.swish(max3)\n",
    "\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "#         batch6 = layers.LeakyReLU()(batch6)\n",
    "\n",
    "\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        batch10 = tf.keras.activations.swish(batch10)\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(max3.shape[1],1), strides=(2,2))(max3)\n",
    "        u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(u)\n",
    "\n",
    "        # u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(max3)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "        batch10 = batch10 + u\n",
    "        ############################\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        batch13 = tf.keras.activations.swish(batch13)\n",
    "\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "        max16 = tf.keras.activations.swish(max16)\n",
    "\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "        batch19 = tf.keras.activations.swish(batch19)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch10.shape[1],1), strides=(2,2))(batch10)\n",
    "        u = Conv2D(filters=64, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=64, kernel_size=1,  padding='same', activation=None)(batch10)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "\n",
    "        batch19 = batch19 + u\n",
    "        ############################\n",
    "\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "#         batch22 = layers.LeakyReLU()(batch22)\n",
    "\n",
    "\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24) \n",
    "        batch25 = tf.keras.activations.swish(batch25)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch19.shape[1],1), strides=(1,1))(batch19)\n",
    "        u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(batch19)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')(u)\n",
    "\n",
    "        batch25 = batch25 + u\n",
    "        ############################\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        mha = layers.MultiHeadAttention(num_heads=8, key_dim=256)(mfm27,mfm27,mfm27)\n",
    "        mel2 = layers.GlobalAveragePooling2D()(mha)\n",
    "        \n",
    "   # mel1_s1s2\n",
    "    if use_s1s2 :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_s1s2)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_s1s2)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        max3 = tf.keras.activations.swish(max3)\n",
    "\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        batch10 = tf.keras.activations.swish(batch10)\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(max3.shape[1],1), strides=(2,2))(max3)\n",
    "        u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(u)\n",
    "\n",
    "        # u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(max3)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "        batch10 = batch10 + u\n",
    "        ############################\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        batch13 = tf.keras.activations.swish(batch13)\n",
    "\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "        max16 = tf.keras.activations.swish(max16)\n",
    "\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "        batch19 = tf.keras.activations.swish(batch19)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch10.shape[1],1), strides=(2,2))(batch10)\n",
    "        u = Conv2D(filters=64, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=64, kernel_size=1,  padding='same', activation=None)(batch10)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "\n",
    "        batch19 = batch19 + u\n",
    "        ############################\n",
    "\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24) \n",
    "        batch25 = tf.keras.activations.swish(batch25)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch19.shape[1],1), strides=(1,1))(batch19)\n",
    "        u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(batch19)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')(u)\n",
    "\n",
    "        batch25 = batch25 + u\n",
    "        ############################\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        mha = layers.MultiHeadAttention(num_heads=8, key_dim=256)(mfm27,mfm27,mfm27)\n",
    "        mel2_s1s2 = layers.GlobalAveragePooling2D()(mha)\n",
    "\n",
    "    if use_cqt :\n",
    "        ## cqt embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "        \n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        cqt2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            cqt2 = Dropout(dp)(cqt2)\n",
    "\n",
    "    if use_stft :\n",
    "        ## stft embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        \n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "        \n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        stft2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            stft2 = Dropout(dp)(stft2)\n",
    "    \n",
    "    \n",
    "    if use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2, stft2])\n",
    "    if not use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([cqt2, stft2])\n",
    "    if use_mel and not use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, stft2])\n",
    "    if use_mel and use_cqt and not use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2])\n",
    "    if not use_mel and not use_cqt and use_stft :  ## stft 만\n",
    "        concat2 = stft2\n",
    "    if use_mel and not use_cqt and not use_stft :  ### mel만\n",
    "        concat2 = mel2\n",
    "    if not use_mel and use_cqt and not use_stft :  ### cqt만\n",
    "        concat2 = cqt2\n",
    "\n",
    "    if ext :\n",
    "        concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg, rr1, mel2_s1s2])\n",
    "        d1 = layers.Dense(3, activation = 'relu')(concat1)\n",
    "        d1 = layers.Dense(2, activation = 'relu')(d1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1])\n",
    "\n",
    "    if ext2 :\n",
    "        concat1 = layers.Concatenate()([rr1,mel2_s1s2])\n",
    "        d1 = layers.Dense(3, activation = 'relu')(concat1)\n",
    "        d1 = layers.Dense(2, activation = 'relu')(d1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1])\n",
    "        \n",
    "    if fc :\n",
    "        concat2 = layers.Dense(10, activation = \"relu\")(concat2)\n",
    "        if dp :\n",
    "            concat2 = Dropout(dp)(concat2)\n",
    "        \n",
    "    if ord1 :\n",
    "        res1 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "    else :\n",
    "        res1 = layers.Dense(3, activation = \"softmax\")(concat2)\n",
    "\n",
    "        \n",
    "    res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "\n",
    "    model = keras.Model(inputs = [age,sex,hw,preg,loc,mel1,cqt1,stft1, rr1, mel1_s1s2] , outputs = res1 )\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy','AUC'])\n",
    "    return(model)\n",
    "\n",
    "\n",
    "def get_LCNN_2_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape, \n",
    "                    mel_s1s2_input_shape, use_s1s2 = True,use_mm = True,\n",
    "                     use_mel = True, use_cqt = True, use_stft = True, \n",
    "                    dp = False, fc = False, ext = False, ext2 = False):\n",
    "        # Create a towy model.\n",
    "    age = keras.Input(shape=(6,), name = 'age_cat')\n",
    "    sex = keras.Input(shape=(2,), name = 'sex_cat')\n",
    "    hw = keras.Input(shape=(2,), name = 'height_weight')\n",
    "    preg = keras.Input(shape=(1,), name = 'is_preg')\n",
    "    loc = keras.Input(shape=(5,), name = 'loc')\n",
    "    mel1 = keras.Input(shape=(mel_input_shape), name = 'mel')\n",
    "    cqt1 = keras.Input(shape=(cqt_input_shape), name = 'cqt')\n",
    "    stft1 = keras.Input(shape=(stft_input_shape), name = 'stft')\n",
    "    rr1 = keras.Input(shape=(1,), name = 'rr')\n",
    "    # qrs1 = keras.Input(shape=(1,), name = 'qrs')\n",
    "    \n",
    "    mel1_s1s2 = keras.Input(shape=(mel_s1s2_input_shape), name = 'mel_s1s2')\n",
    "        \n",
    "        \n",
    "    ## age embeddig\n",
    "    age1 = layers.Dense(2, activation = None)(age)\n",
    "\n",
    "    ## sex embedding\n",
    "    sex1 = layers.Dense(1, activation = None)(sex)\n",
    "\n",
    "    ## hw embedding\n",
    "    hw1 = layers.Dense(1, activation = None)(hw)\n",
    "\n",
    "    ## loc embedding\n",
    "    loc1 = layers.Dense(3, activation = None)(loc)\n",
    "\n",
    "   \n",
    "\n",
    "   ## mel embedding\n",
    "    if use_mel :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        \n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        mel2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            mel2 = Dropout(dp)(mel2)\n",
    "        \n",
    "    if use_s1s2:\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_s1s2)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_s1s2)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        \n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        mel2_s1s2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            mel2_s1s2 = Dropout(dp)(mel2_s1s2)\n",
    "\n",
    "    if use_cqt :\n",
    "        ## cqt embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        \n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        cqt2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            cqt2 = Dropout(dp)(cqt2)\n",
    "\n",
    "    if use_stft :\n",
    "        ## stft embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "        \n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        stft2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            stft2 = Dropout(dp)(stft2)\n",
    "    \n",
    "#    concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg])\n",
    "#    d1 = layers.Dense(2, activation = 'relu')(concat1)\n",
    "    \n",
    "    if use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2, stft2])\n",
    "    if not use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([cqt2, stft2])\n",
    "    if use_mel and not use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, stft2])\n",
    "    if use_mel and use_cqt and not use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2])\n",
    "    if not use_mel and not use_cqt and use_stft :  ## stft 만\n",
    "        concat2 = stft2\n",
    "    if use_mel and not use_cqt and not use_stft :  ### mel만\n",
    "        concat2 = mel2\n",
    "    if not use_mel and use_cqt and not use_stft :  ### cqt만\n",
    "        concat2 = cqt2\n",
    "\n",
    "    if ext :\n",
    "        concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg, rr1,mel2_s1s2])\n",
    "        d1 = layers.Dense(3, activation = 'relu')(concat1)\n",
    "        d1 = layers.Dense(2, activation = 'relu')(d1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1])\n",
    "\n",
    "    if ext2 :\n",
    "        concat1 = layers.Concatenate()([rr1,mel2_s1s2])\n",
    "        d1 = layers.Dense(3, activation = 'relu')(concat1)\n",
    "        d1 = layers.Dense(2, activation = 'relu')(d1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1])\n",
    "        \n",
    "    if fc :\n",
    "        concat2 = layers.Dense(10, activation = 'relu')(concat2)\n",
    "        if dp :\n",
    "            concat2 = Dropout(dp)(concat2)\n",
    "\n",
    "    res1 = layers.Dense(3, activation = \"softmax\")(concat2)\n",
    "    res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "\n",
    "    model = keras.Model(inputs = [age,sex,hw,preg,loc,mel1,cqt1,stft1, rr1, mel1_s1s2] , outputs = res2 )\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy','AUC'])\n",
    "    return(model)\n",
    "\n",
    "\n",
    "def sigmoidal_decay(e, start=0, end=100, lr_start=1e-3, lr_end=1e-5):\n",
    "    if e < start:\n",
    "        return lr_start\n",
    "    elif e > end:\n",
    "        return lr_end\n",
    "\n",
    "    middle = (start + end) / 2\n",
    "    s = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return s(13 * (-e + middle) / np.abs(end - start)) * np.abs(lr_start - lr_end) + lr_end\n",
    "\n",
    "patient_files_trn = find_patient_files(train_folder)\n",
    "patient_files_test = find_patient_files(test_folder)\n",
    "\n",
    "# Run model.\n",
    "def run_model(model_folder, data_folder, output_folder, allow_failures, verbose):\n",
    "    # Load models.\n",
    "    if verbose >= 1:\n",
    "        print('Loading Challenge model...')\n",
    "\n",
    "    model = load_challenge_model(model_folder, verbose) ### Teams: Implement this function!!!\n",
    "\n",
    "    # Find the patient data files.\n",
    "    patient_files = find_patient_files(data_folder)\n",
    "    num_patient_files = len(patient_files)\n",
    "\n",
    "    if num_patient_files==0:\n",
    "        raise Exception('No data was provided.')\n",
    "\n",
    "    # Create a folder for the Challenge outputs if it does not already exist.\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Run the team's model on the Challenge data.\n",
    "    if verbose >= 1:\n",
    "        print('Running model on Challenge data...')\n",
    "\n",
    "#    @tf.function\n",
    "    # Iterate over the patient files.\n",
    "    for i in range(num_patient_files):\n",
    "        if verbose >= 2:\n",
    "            print('    {}/{}...'.format(i+1, num_patient_files))\n",
    "\n",
    "        patient_data = load_patient_data(patient_files[i])\n",
    "        recordings = load_recordings(data_folder, patient_data)\n",
    "\n",
    "        # Allow or disallow the model to fail on parts of the data; helpful for debugging.\n",
    "        try:\n",
    "            classes, labels, probabilities = run_challenge_model(model, patient_data, recordings, verbose) ### Teams: Implement this function!!!\n",
    "        except:\n",
    "            if allow_failures:\n",
    "                if verbose >= 2:\n",
    "                    print('... failed.')\n",
    "                classes, labels, probabilities = list(), list(), list()\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # Save Challenge outputs.\n",
    "        head, tail = os.path.split(patient_files[i])\n",
    "        root, extension = os.path.splitext(tail)\n",
    "        output_file = os.path.join(output_folder, root + '.csv')\n",
    "        patient_id = get_patient_id(patient_data)\n",
    "        save_challenge_outputs(output_file, patient_id, classes, labels, probabilities)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print('Done.')\n",
    "        \n",
    "import pickle as pk\n",
    "def save_challenge_model(model_folder, model1, model2, m_name1, m_name2, param_feature) :\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    filename1 = os.path.join(model_folder, m_name1 + '_model1.hdf5')\n",
    "    filename2 = os.path.join(model_folder, m_name2 + '_model2.hdf5')\n",
    "    model1.save(filename1)\n",
    "    model2.save(filename2)\n",
    "    param_feature['model1'] = m_name1\n",
    "    param_feature['model2'] = m_name2\n",
    "    param_feature['model_fnm1'] = filename1\n",
    "    param_feature['model_fnm2'] = filename2\n",
    "    with open(info_fnm, 'wb') as f:\n",
    "        pk.dump(param_feature, f, pk.HIGHEST_PROTOCOL)\n",
    "    return 1\n",
    "\n",
    "def load_challenge_model(model_folder, verbose):\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    with open(info_fnm, 'rb') as f:\n",
    "        info_m = pk.load(f)\n",
    "#    if info_m['model'] == 'toy' :\n",
    "#        model = get_toy(info_m['mel_shape'])\n",
    "#    filename = os.path.join(model_folder, info_m['model'] + '_model.hdf5')\n",
    "#    model.load_weights(filename)\n",
    "    return info_m\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run your trained model. This function is *required*. You should edit this function to add your code, but do *not* change the\n",
    "# arguments of this function.\n",
    "def run_challenge_model(model, data, recordings, verbose):\n",
    "\n",
    "    murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "    outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "    if model['model1'] == 'lcnn1_dr_rr' :\n",
    "        model1 = get_LCNN_o_1_dr_rr(model['mel_shape'],model['cqt_shape'],model['stft_shape'], \n",
    "                                    model['s1s2_shape'], model['use_s1s2'], model['use_mm'],\n",
    "                                    use_mel = model['use_mel'],\n",
    "                                    use_cqt = model['use_cqt'], use_stft = model['use_stft'], ord1 = model['ord1'], \n",
    "                                    dp = model['dp'], fc = model['fc'], ext = False, ext2 = True)\n",
    "    if model['model2'] == 'lcnn2_dr_rr' :\n",
    "        model2 = get_LCNN_2_dr_rr(model['mel_shape'],model['cqt_shape'],model['stft_shape'], \n",
    "                                  model['s1s2_shape'],model['use_s1s2'], model['use_mm'],\n",
    "                                  use_mel = model['use_mel'],\n",
    "                                  use_cqt = model['use_cqt'], use_stft = model['use_stft'], \n",
    "                                  dp = model['dp'], fc = model['fc'], ext = True, ext2 = False)\n",
    "    model1.load_weights(model['model_fnm1'])\n",
    "    model2.load_weights(model['model_fnm2'])\n",
    "\n",
    "#    classes = model['classes']\n",
    "    # Load features.\n",
    "    features = get_feature_one(data, verbose = 0)\n",
    "\n",
    "    samp_sec = model['samp_sec']\n",
    "    pre_emphasis = model['pre_emphasis']\n",
    "    hop_length = model['hop_length']\n",
    "    win_length = model['win_length']\n",
    "    n_mels = model['n_mels']\n",
    "    filter_scale = model['filter_scale']\n",
    "    n_bins = model['n_bins']\n",
    "    fmin = model['fmin']\n",
    "    use_mel = model['use_mel']\n",
    "    use_cqt = model['use_cqt']\n",
    "    use_stft = model['use_stft']\n",
    "    use_raw = model['use_raw']\n",
    "    trim = model['trim']\n",
    "    use_rr = model['use_rr']\n",
    "    use_mm = model['use_mm']\n",
    "    use_s1s2 = model['use_s1s2']\n",
    "    use_b_detect = True\n",
    "\n",
    "\n",
    "#     use_seq = model['rr_seq']\n",
    "\n",
    "    features['mel1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_mel :\n",
    "            mel1 = feature_extract_melspec(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length,\n",
    "                                           win_length = win_length, n_mels = n_mels, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1) )\n",
    "        features['mel1'].append(mel1)\n",
    "    M, N = features['mel1'][0].shape\n",
    "\n",
    "    if use_mel :\n",
    "        for i in range(len(features['mel1'])) :\n",
    "            features['mel1'][i] = features['mel1'][i].reshape(M,N,1)\n",
    "    features['mel1'] = np.array(features['mel1'])\n",
    "\n",
    "    features['cqt1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_cqt :\n",
    "            mel1 = feature_extract_cqt(recordings[i], samp_sec=samp_sec, pre_emphasis = pre_emphasis, filter_scale = filter_scale,\n",
    "                                        n_bins = n_bins, fmin = fmin, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1))\n",
    "        features['cqt1'].append(mel1)\n",
    "    M, N = features['cqt1'][0].shape\n",
    "    if use_cqt :\n",
    "        for i in range(len(features['cqt1'])) :\n",
    "            features['cqt1'][i] = features['cqt1'][i].reshape(M,N,1)\n",
    "    features['cqt1'] = np.array(features['cqt1'])\n",
    "\n",
    "    features['stft1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_stft :\n",
    "            mel1 = feature_extract_stft(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length,\n",
    "                                        win_length = win_length, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1) )\n",
    "        features['stft1'].append(mel1)\n",
    "    M, N = features['stft1'][0].shape\n",
    "    if use_stft :\n",
    "        for i in range(len(features['stft1'])) :\n",
    "            features['stft1'][i] = features['stft1'][i].reshape(M,N,1)\n",
    "    features['stft1'] = np.array(features['stft1'])\n",
    "\n",
    "    features['raw1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_raw:\n",
    "            recording1 = recordings[i]\n",
    "            if len(recording1) >= maxlen : \n",
    "                recording1 = recording1[:maxlen]\n",
    "            else :\n",
    "                recording1 = np.pad(recording1, (0, maxlen - len(recording1) ), constant_values=(0,0) )\n",
    "        else :\n",
    "            recording1 = np.zeros((1))\n",
    "        features['raw1'].append(recording1)\n",
    "    features['raw1'] = np.array(features['raw1'])\n",
    "    \n",
    "    \n",
    "    features['rr1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_rr :\n",
    "            try:\n",
    "                recording1 = recordings[i]\n",
    "                ____, info = nk.ecg_process(recording1, sampling_rate=4000)\n",
    "                current_rr = np.mean(np.diff(info['ECG_R_Peaks'])/4000)\n",
    "            except:\n",
    "#                print(filename)\n",
    "                current_rr= 0.6414\n",
    "        else :\n",
    "            current_rr = 0\n",
    "        features['rr1'].append(current_rr)\n",
    "    features['rr1'] = np.array(features['rr1'])\n",
    "    \n",
    "    \n",
    "\n",
    "    features['s1s2_detect1'] = []\n",
    "    features['mm_detect1'] = []\n",
    "    features['s1s2_mel'] = []\n",
    "    features['mm_mel'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_b_detect :\n",
    "            recording1 = recordings[i]\n",
    "         # 1. Amplitude normalization\n",
    "            normal_sig = recording1/np.max(np.abs(recording1))\n",
    "\n",
    "            # 2. Filtering\n",
    "            T = len(recording1)/4000 #time interval; Sample Period ;\n",
    "            fs = 4000 #sample rate \n",
    "            cutoff = 150 #sample frequency \n",
    "\n",
    "            nyq = 0.5 * fs \n",
    "            order = 2  # sin wave can be approx represented as quadratic\n",
    "            n = int(T*fs) #total number of samples \n",
    "\n",
    "            low_ft = butter_lowpass_filter(normal_sig, cutoff, fs, order)\n",
    "\n",
    "            # 3. smoothing of signal envelope\n",
    "            duration = 1.0\n",
    "#                 fs = 4000.0\n",
    "            samples = int(fs*duration)\n",
    "            t = np.arange(len(low_ft)) / fs\n",
    "\n",
    "            analytic_signal = hilbert(low_ft)\n",
    "            amplitude_envelope = np.abs(analytic_signal)\n",
    "\n",
    "            # threshld selection\n",
    "            mu = np.sum(amplitude_envelope)/len(amplitude_envelope)\n",
    "            var = np.sum((amplitude_envelope-mu)**2)/len(amplitude_envelope)\n",
    "            t_sh = mu + var +0.05\n",
    "\n",
    "            thres_list = np.argwhere(amplitude_envelope > t_sh)\n",
    "            save = []\n",
    "            for i in thres_list:\n",
    "                j = i[0]\n",
    "                save.append(j)\n",
    "\n",
    "            packet = []\n",
    "            tmp = []\n",
    "            v = save.pop(0)\n",
    "            tmp.append(v)\n",
    "\n",
    "            while(len(save)>0):\n",
    "                vv = save.pop(0)\n",
    "                if v+1 == vv:\n",
    "                    tmp.append(vv)\n",
    "                    v = vv\n",
    "                else:\n",
    "                    packet.append(tmp)\n",
    "                    tmp = []\n",
    "                    tmp.append(vv)\n",
    "                    v = vv\n",
    "\n",
    "            packet.append(tmp)\n",
    "#                 # thresh hold 보다 크지만 연속값이 3개 미만인 리스트 찾아서 삭제\n",
    "#                 packet2 = []\n",
    "#                 for i in range(len(packet)):\n",
    "#                     if len(packet[i]) < 3:\n",
    "#                         j = packet.index(packet[i])\n",
    "#                         packet2.append(j)\n",
    "\n",
    "#                 for i in packet2:\n",
    "#                     del packet[i]\n",
    "\n",
    "            min_list = []\n",
    "            max_list = []\n",
    "            for i in range(len(packet)):\n",
    "                min_find = min(packet[i])\n",
    "                max_find = max(packet[i])\n",
    "                min_list.append(min_find)\n",
    "                max_list.append(max_find)\n",
    "\n",
    "            # s1, s2 boundary가 여러개 그려짐. 추가적인 전처리 필요. 60개보다 커야함\n",
    "            packet_cp = packet.copy()\n",
    "            new_list = []\n",
    "            first_list = []\n",
    "            last_list = []\n",
    "            for i in range(len(max_list)-1):\n",
    "                j = i + 1\n",
    "                # 연속적인 값인 경우 삭제\n",
    "                if abs(max_list[i]-min_list[j]) < 60:\n",
    "                    first = max_list.index(max_list[i])\n",
    "                    last = min_list.index(min_list[j])\n",
    "                    re_join = packet_cp[first]+packet_cp[last]\n",
    "                    new_list.append(re_join)\n",
    "                    first_list.append(first)\n",
    "                    last_list.append(last)\n",
    "\n",
    "            # 연속적인 값 인덱스 찾아서 final_list 만들기\n",
    "            final_list = first_list+last_list\n",
    "            final_list.sort()\n",
    "            set(final_list)\n",
    "\n",
    "            # 위에서 찾은 final_linst에 들어있는 인덱스 위치는 0으로 처리\n",
    "            drop_list = packet_cp.copy()\n",
    "            for i in final_list:\n",
    "                drop_list[i] = 0\n",
    "#             print(len(drop_list))\n",
    "\n",
    "            seq_remake = drop_list+new_list\n",
    "#             print(len(seq_remake))\n",
    "\n",
    "\n",
    "\n",
    "            # 0으로 전처리한 값 삭제\n",
    "            remove_set = [0]\n",
    "\n",
    "            li = [i for i in seq_remake if i not in remove_set]\n",
    "#             print(li)\n",
    "\n",
    "            # 추가 전처리 후 다시 min, max 출력\n",
    "            min_list1 = []\n",
    "            max_list1 = []\n",
    "            for i in range(len(li)):\n",
    "                min_find1 = min(li[i])\n",
    "                max_find1 = max(li[i])\n",
    "                min_list1.append(min_find1)\n",
    "                max_list1.append(max_find1)\n",
    "\n",
    "            min_list1.sort()\n",
    "            max_list1.sort()\n",
    "\n",
    "\n",
    "            # boundary detected s1 and s2\n",
    "            s_detect = amplitude_envelope.copy()\n",
    "            for i in range(len(max_list1)-1):\n",
    "                j = i+1\n",
    "                s_detect[max_list1[i]:min_list1[j]] = 0\n",
    "            s1s2_detect = s_detect.reshape(1, s_detect.shape[0])\n",
    "            s1s2_detect = s1s2_detect.tolist()\n",
    "            s1s2_detect = pad_sequences(s1s2_detect, maxlen = 80000, dtype ='float64', padding = 'post', truncating ='post', value=0.0)\n",
    "\n",
    "            # s1s2_detect 변수에 mel 적용\n",
    "            s1s2_mel = feature_extract_bound_melspec(s_detect)[0]\n",
    "\n",
    "            # boundary detected systolic and diastolic murmurs present in pcg signal\n",
    "            mm_detect = amplitude_envelope.copy()\n",
    "            for i in range(len(max_list1)):\n",
    "                mm_detect[min_list1[i]:max_list1[i]+1] = 0\n",
    "            murmur_detect = mm_detect.reshape(1, mm_detect.shape[0])\n",
    "            murmur_detect = murmur_detect.tolist()\n",
    "            murmur_detect = pad_sequences(murmur_detect, maxlen = 80000, dtype ='float64', padding = 'post', truncating ='post', value=0.0)\n",
    "\n",
    "            # murmur_dect 변수에 mel 적용\n",
    "            mm_mel = feature_extract_bound_melspec(mm_detect)[0]\n",
    "\n",
    "        else :\n",
    "            s1s2_detect = np.zeros((1,1))\n",
    "            murmur_detect = np.zeros((1,1))\n",
    "            s1s2_mel = np.zeros( (1,1,1) )\n",
    "            mm_mel = np.zeros( (1,1,1) )\n",
    "\n",
    "        features['s1s2_detect1'].append(s1s2_detect)\n",
    "        features['mm_detect1'].append(murmur_detect)\n",
    "        features['s1s2_mel'].append(s1s2_mel)\n",
    "        features['mm_mel'].append(mm_mel)\n",
    "\n",
    "    features['s1s2_detect1'] = np.array(features['s1s2_detect1'])\n",
    "    features['mm_detect1'] = np.array(features['mm_detect1'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    M, N = features['s1s2_mel'][0].shape\n",
    "    if use_s1s2:\n",
    "        for i in range(len(features['s1s2_mel'])):\n",
    "            features['s1s2_mel'][i] = features['s1s2_mel'][i].reshape(M,N,1)\n",
    "    features['s1s2_mel'] = np.array(features['s1s2_mel'])\n",
    "    \n",
    "    \n",
    "    M, N = features['mm_mel'][0].shape\n",
    "    if use_mm:\n",
    "        for i in range(len(features['mm_mel'])):\n",
    "            features['mm_mel'][i] = features['mm_mel'][i].reshape(M,N,1)\n",
    "    features['mm_mel'] = np.array(features['mm_mel'])\n",
    "\n",
    "\n",
    "    # Impute missing data.\n",
    "    res1 = model1.predict([features['age'], features['sex'], features['hw'], features['preg'], features['loc'], \n",
    "                           features['mel1'],features['cqt1'], features['stft1'], features['rr1'], \n",
    "                           features['s1s2_mel']])\n",
    "    res2 = model2.predict([features['age'], features['sex'], features['hw'], features['preg'], features['loc'], \n",
    "                           features['mel1'], features['cqt1'], features['stft1'], features['rr1'], \n",
    "                           features['s1s2_mel']])\n",
    "\n",
    "\n",
    "    # Get classifier probabilities.\n",
    "    if model['ord1'] :\n",
    "        idx1 = res1.argmax(axis=0)[0]\n",
    "        murmur_p = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "        murmur_probabilities = np.zeros((3,))\n",
    "        murmur_probabilities[0] = murmur_p[0]\n",
    "        murmur_probabilities[1] = 0\n",
    "        murmur_probabilities[2] = murmur_p[1]\n",
    "        outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "    else :\n",
    "        if model['mm_mean'] :\n",
    "            murmur_probabilities = res1.mean(axis = 0)\n",
    "        else :\n",
    "            idx1 = res1.argmax(axis=0)[0]\n",
    "            murmur_probabilities = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "        outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "\n",
    "    # Get classifier probabilities.\n",
    "#    idx1 = res1.argmax(axis=0)[0]\n",
    "#    murmur_probabilities = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "#    outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "#    idx = np.argmax(prob1)\n",
    "\n",
    "    ## 이부분도 생각 필요.. rule 을 cost를 maximize 하는 기준으로 threshold 탐색 필요할지도..\n",
    "    # Choose label with highest probability.\n",
    "    murmur_labels = np.zeros(len(murmur_classes), dtype=np.int_)\n",
    "    if murmur_probabilities[0] > 0.496 :\n",
    "        idx = 0\n",
    "    else :\n",
    "        idx = 2\n",
    "#    idx = np.argmax(murmur_probabilities)\n",
    "    murmur_labels[idx] = 1\n",
    "\n",
    "    outcome_labels = np.zeros(len(outcome_classes), dtype=np.int_)\n",
    "    if outcome_probabilities[0] > 0.617 :\n",
    "        idx = 0\n",
    "    else :\n",
    "        idx = 1\n",
    "#    idx = np.argmax(outcome_probabilities)\n",
    "    outcome_labels[idx] = 1\n",
    "\n",
    "    # Concatenate classes, labels, and probabilities.\n",
    "    classes = murmur_classes + outcome_classes\n",
    "    labels = np.concatenate((murmur_labels, outcome_labels))\n",
    "    probabilities = np.concatenate((murmur_probabilities, outcome_probabilities))\n",
    "\n",
    "    return classes, labels, probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_folder = 'hyper_1_2'\n",
    "output_folder = '/Data/hmd/hmd_sy/2021_hmd/tmp/out_hyper_1_2'\n",
    "\n",
    "# maxlen = np.random.choice([120000,80000, 50000, 15000])\n",
    "winlen = 512\n",
    "hoplen = 256\n",
    "nmel = 140 #np.random.choice([100, 120, 140])\n",
    "nsec = 50\n",
    "trim = 0 #np.random.choice([0,2000, 4000])\n",
    "use_mel = True\n",
    "use_cqt = False #np.random.choice([True,False])\n",
    "use_stft = False#np.random.choice([True, False])\n",
    "use_rr = True\n",
    "# use_rr_seq = False #True\n",
    "use_raw = False #True\n",
    "\n",
    "use_b_detect = True\n",
    "use_s1s2 = True\n",
    "use_mm = False\n",
    "\n",
    "#################\n",
    "# envelope parameter\n",
    "#################\n",
    "samp_sec = 50\n",
    "sample_rate = 4000\n",
    "pre_emphasis  = 0\n",
    "sr = 4000\n",
    "n_mels = 140\n",
    "\n",
    "# maxlen = 120000\n",
    "win_length = 512\n",
    "hop_length = 256\n",
    "\n",
    "fs = 4000 #sample rate \n",
    "cutoff = 150 #sample frequency \n",
    "nyq = 0.5 * fs \n",
    "\n",
    "\n",
    "\n",
    "params_feature = {'samp_sec': nsec,\n",
    "            #### melspec, stft 피쳐 옵션들  \n",
    "            'pre_emphasis': 0,\n",
    "            'hop_length': hoplen,\n",
    "            'win_length':winlen,\n",
    "            'n_mels': nmel,\n",
    "            #### cqt 피쳐 옵션들  \n",
    "            'filter_scale': 1,\n",
    "            'n_bins': 80,\n",
    "            'fmin': 10,\n",
    "\n",
    "            ### 사용할 피쳐 지정\n",
    "                'trim' : trim, # 앞뒤 얼마나 자를지? 4000 이면 1초\n",
    "                'use_rr' : use_rr,\n",
    "                'use_b_detect': use_b_detect,\n",
    "                'use_raw' : use_raw,\n",
    "                'use_mel' : use_mel,\n",
    "                'use_cqt' : use_cqt,\n",
    "                'use_stft' : use_stft          \n",
    "}\n",
    "\n",
    "\n",
    "mm_weight = 3 #np.random.choice([2,3,4,5])\n",
    "oo_weight = 3 #np.random.choice([2,3,4,5,6])\n",
    "ord1 = True #np.random.choice([True,False])\n",
    "mm_mean = False #np.random.choice([True,False])\n",
    "dp = 0 #np.random.choice([0, .1, .2, .3])\n",
    "fc = False #np.random.choice([True,False])\n",
    "\n",
    "\n",
    "ext = True\n",
    "\n",
    "\n",
    "chaug = 10 #np.random.choice([0, 10])\n",
    "mixup = True #np.random.choice([True,False])\n",
    "cout = .8 #np.random.choice([0, 0.8])\n",
    "wunknown = 1 #np.random.choice([1, 0.7, .5, .2])\n",
    "n1 = 0 #np.random.choice([0,2])\n",
    "if n1 == 0 :\n",
    "    ranfil = False\n",
    "else :\n",
    "    ranfil = [n1, [18,19,20,21,22,23]]\n",
    "    \n",
    "use_mel = params_feature['use_mel']\n",
    "use_cqt = params_feature['use_cqt']\n",
    "use_stft = params_feature['use_stft']\n",
    "nep = 100\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/features_trn_envel.pkl','rb') as f:\n",
    "    features_trn = pickle.load(f)\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/mm_lbs_trn_envel.pkl','rb') as f:\n",
    "    mm_lbs_trn = pickle.load(f)\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/out_lbs_trn_envel.pkl','rb') as f:\n",
    "    out_lbs_trn = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/features_test_envel.pkl','rb') as f:\n",
    "    features_test = pickle.load(f)\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/mm_lbs_test_envel.pkl','rb') as f:\n",
    "    mm_lbs_test = pickle.load(f)\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/out_lbs_test_envel.pkl','rb') as f:\n",
    "    out_lbs_test = pickle.load(f)\n",
    "    \n",
    "# (2532, 140, 782) 에서 (2532, 140, 782, 1)로 변경\n",
    "a, b, c = features_trn['mel1'].shape\n",
    "features_trn['mel1']= features_trn['mel1'].reshape(a,b,c,1)\n",
    "\n",
    "a, b, c = features_trn['s1s2_mel'].shape\n",
    "features_trn['s1s2_mel'] = features_trn['s1s2_mel'].reshape(a,b,c,1)\n",
    "\n",
    "a, b, c = features_trn['mm_mel'].shape\n",
    "features_trn['mm_mel'] = features_trn['mm_mel'].reshape(a,b,c,1)\n",
    "\n",
    "mel_input_shape = features_trn['mel1'][0].shape\n",
    "cqt_input_shape = features_trn['cqt1'][0].shape\n",
    "stft_input_shape = features_trn['stft1'][0].shape\n",
    "\n",
    "mel_s1s2_input_shape = features_trn['s1s2_mel'][0].shape\n",
    "mel_mm_input_shape = features_trn['mm_mel'][0].shape\n",
    "\n",
    "\n",
    "params_feature['ord1'] = ord1\n",
    "params_feature['mm_mean'] = mm_mean\n",
    "params_feature['dp'] = dp\n",
    "params_feature['fc'] = fc\n",
    "params_feature['ext'] = ext\n",
    "params_feature['oo_weight'] = oo_weight\n",
    "params_feature['mm_weight'] = mm_weight\n",
    "params_feature['chaug'] = chaug\n",
    "params_feature['cout'] = cout\n",
    "params_feature['wunknown'] = wunknown\n",
    "params_feature['mixup'] = mixup\n",
    "params_feature['n1'] = n1\n",
    "\n",
    "params_feature['mel_shape'] = mel_input_shape\n",
    "params_feature['cqt_shape'] = cqt_input_shape\n",
    "params_feature['stft_shape'] = stft_input_shape\n",
    "\n",
    "params_feature['s1s2_shape'] = mel_s1s2_input_shape\n",
    "params_feature['mm_shape'] = mel_mm_input_shape\n",
    "\n",
    "params_feature['use_mel'] = use_mel\n",
    "params_feature['use_cqt'] = use_cqt\n",
    "params_feature['use_stft'] = use_stft\n",
    "\n",
    "params_feature['use_rr'] = use_rr\n",
    "params_feature['use_s1s2'] = use_s1s2\n",
    "params_feature['use_mm'] = use_mm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(params_feature)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "model1 = get_LCNN_o_1_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape, \n",
    "                            mel_s1s2_input_shape, use_s1s2 = use_s1s2, use_mm = use_mm,\n",
    "                            use_mel = use_mel, use_cqt = use_cqt, use_stft = use_stft, ord1 = ord1, dp = dp, fc = fc, ext = False, ext2 = True)\n",
    "model2 = get_LCNN_2_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape, \n",
    "                        mel_s1s2_input_shape, use_s1s2 = use_s1s2, use_mm = use_mm,\n",
    "                        use_mel = use_mel, use_cqt = use_cqt, use_stft = use_stft, dp = dp, fc = fc, ext = True, ext2 = False)\n",
    "\n",
    "\n",
    "n_epoch = nep\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "batch_size = 64\n",
    "\n",
    "if mixup :\n",
    "    beta_param = .7\n",
    "else :\n",
    "    beta_param = 0\n",
    "\n",
    "params = {'batch_size': batch_size,\n",
    "        #          'input_shape': (100, 313, 1),\n",
    "        'shuffle': True,\n",
    "        'chaug': chaug,\n",
    "        'beta_param': beta_param,\n",
    "        'cout': cout\n",
    "#              'mixup': mixup,\n",
    "        #          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "        #          'highpass': [.5, [78,79,80,81,82,83,84,85]]\n",
    "#              'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "        #           'dropblock' : [30, 100]\n",
    "        #'device' : device\n",
    "}\n",
    "\n",
    "if mixup :\n",
    "    params['mixup'] = mixup\n",
    "    params['ranfilter2'] = ranfil\n",
    "else :\n",
    "    params['cutout'] = cout\n",
    "\n",
    "params_no_shuffle = {'batch_size': batch_size,\n",
    "                    #          'input_shape': (100, 313, 1),\n",
    "                    'shuffle': False,\n",
    "                    'beta_param': 0.7,\n",
    "                    'mixup': False\n",
    "                    #'device': device\n",
    "}\n",
    "\n",
    "if ord1 :\n",
    "    class_weight = {0: mm_weight, 1: 1.}\n",
    "else :\n",
    "    class_weight = {0: mm_weight, 1: wunknown, 2:1.}\n",
    "\n",
    "\n",
    "if mixup :\n",
    "        TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                                features_trn['preg'], features_trn['loc'], features_trn['mel1'],\n",
    "                                features_trn['cqt1'],features_trn['stft1'],features_trn['rr1'], \n",
    "                                features_trn['s1s2_mel']], \n",
    "                        mm_lbs_trn,  ## our Y\n",
    "                            **params)()\n",
    "        model1.fit(TrainDGen_1,\n",
    "            validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                                features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                                features_test['cqt1'], features_test['stft1'],features_test['rr1'],\n",
    "                                features_test['s1s2_mel']], \n",
    "                                mm_lbs_test), \n",
    "            callbacks=[lr],\n",
    "            steps_per_epoch=np.ceil(len(mm_lbs_trn)/64),\n",
    "            class_weight=class_weight, \n",
    "            epochs = n_epoch)\n",
    "\n",
    "else :\n",
    "    TrainGen = DataGenerator([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                            features_trn['preg'], features_trn['loc'], features_trn['mel1'],\n",
    "                            features_trn['cqt1'],features_trn['stft1'],features_trn['rr1'], \n",
    "                            features_trn['s1s2_mel']], \n",
    "                mm_lbs_trn,  ## our Y\n",
    "                **params)\n",
    "    model1.fit(TrainGen,\n",
    "        validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                            features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                            features_test['cqt1'], features_test['stft1'],features_test['rr1'], \n",
    "                            features_test['s1s2_mel']], \n",
    "                            mm_lbs_test), \n",
    "        callbacks=[lr],\n",
    "        #        steps_per_epoch=np.ceil(len(mm_lbs_trn)/64),\n",
    "        class_weight=class_weight, \n",
    "        epochs = n_epoch)\n",
    "    \n",
    "n_epoch = nep\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "batch_size = 64\n",
    "params = {'batch_size': batch_size,\n",
    "        #          'input_shape': (100, 313, 1),\n",
    "        'shuffle': True,\n",
    "        'chaug': chaug,\n",
    "        'beta_param': beta_param,\n",
    "        'cout': cout,\n",
    "#              'mixup': True,\n",
    "        #          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "#            'highpass': [.5, [78,79,80,81,82,83,84,85]],\n",
    "#              'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "        #           'dropblock' : [30, 100]\n",
    "        #'device' : device\n",
    "}\n",
    "\n",
    "\n",
    "if mixup :\n",
    "    params['mixup'] = mixup\n",
    "    params['ranfilter2'] = ranfil\n",
    "else :\n",
    "    params['cutout'] = cout\n",
    "\n",
    "\n",
    "params_no_shuffle = {'batch_size': batch_size,\n",
    "                    #          'input_shape': (100, 313, 1),\n",
    "                    'shuffle': False,\n",
    "                    'beta_param': 0.7,\n",
    "                    'mixup': False\n",
    "                    #'device': device\n",
    "}\n",
    "\n",
    "class_weight = {0: oo_weight, 1: 1.}\n",
    "\n",
    "\n",
    "\n",
    "if mixup :\n",
    "    TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                                features_trn['preg'], features_trn['loc'], features_trn['mel1'],\n",
    "                                features_trn['cqt1'],features_trn['stft1'],features_trn['rr1'], \n",
    "                                features_trn['s1s2_mel']], \n",
    "                                out_lbs_trn,  ## our Y\n",
    "                    **params)()\n",
    "\n",
    "    model2.fit(TrainDGen_1,\n",
    "    validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                                features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                                features_test['cqt1'], features_test['stft1'],features_test['rr1'],\n",
    "                                features_test['s1s2_mel']], \n",
    "                                out_lbs_test), \n",
    "        callbacks=[lr],\n",
    "        steps_per_epoch=np.ceil(len(out_lbs_trn)/64),\n",
    "        class_weight=class_weight, \n",
    "        epochs = n_epoch)\n",
    "else :\n",
    "    TrainGen = DataGenerator([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                            features_trn['preg'], features_trn['loc'], features_trn['mel1'],\n",
    "                            features_trn['cqt1'],features_trn['stft1'],features_trn['rr1'], \n",
    "                            features_trn['s1s2_mel']], \n",
    "                            out_lbs_trn,  ## our Y\n",
    "                **params)\n",
    "    model2.fit(TrainGen,\n",
    "        validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                            features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                            features_test['cqt1'], features_test['stft1'],features_test['rr1'],\n",
    "                            features_test['s1s2_mel']], \n",
    "                            out_lbs_test), \n",
    "        callbacks=[lr],\n",
    "        class_weight=class_weight, \n",
    "        epochs = n_epoch)\n",
    "    \n",
    "\n",
    "\n",
    "# params_feature['mel_shape'] = mel_input_shape\n",
    "# params_feature['cqt_shape'] = cqt_input_shape\n",
    "# params_feature['stft_shape'] = stft_input_shape\n",
    "\n",
    "# params_feature['use_mel'] = use_mel\n",
    "# params_feature['use_cqt'] = use_cqt\n",
    "# params_feature['use_stft'] = use_stft\n",
    "\n",
    "save_challenge_model(model_folder, model1, model2, m_name1 = 'lcnn1_dr_rr', m_name2 = 'lcnn2_dr_rr', param_feature = params_feature)\n",
    "\n",
    "run_model(model_folder, test_folder, output_folder, allow_failures = True, verbose = 1)\n",
    "\n",
    "murmur_scores, outcome_scores = evaluate_model(test_folder, output_folder)\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "','.join(classes),\n",
    "','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "params_feature['mm_weighted_accuracy'] = weighted_accuracy\n",
    "\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "','.join(classes),\n",
    "','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "+ '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "\n",
    "params_feature['out_cost'] = cost\n",
    "\n",
    "label_folder = test_folder\n",
    "murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "# Load and parse label and model output files.\n",
    "label_files, output_files = find_challenge_files(label_folder, output_folder)\n",
    "murmur_labels = load_murmurs(label_files, murmur_classes)\n",
    "murmur_binary_outputs, murmur_scalar_outputs = load_classifier_outputs(output_files, murmur_classes)\n",
    "outcome_labels = load_outcomes(label_files, outcome_classes)\n",
    "outcome_binary_outputs, outcome_scalar_outputs = load_classifier_outputs(output_files, outcome_classes)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e16eebce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.571,0.853,0.795,18601.128\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.581,0.581,0.547,15094.137\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.805,0.000,0.909\n",
      "Accuracy,0.868,0.000,0.935\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.565,0.596\n",
      "Accuracy,0.531,0.634\n",
      "\n",
      "0.40697253249702653\n",
      "0.5930274657866094\n",
      "0.5730042433520384\n",
      "0.4269957578307047\n"
     ]
    }
   ],
   "source": [
    "murmur_scores, outcome_scores = evaluate_model(test_folder, output_folder)\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "','.join(classes),\n",
    "','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "params_feature['mm_weighted_accuracy'] = weighted_accuracy\n",
    "\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "','.join(classes),\n",
    "','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "+ '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "print(output_string)\n",
    "\n",
    "\n",
    "\n",
    "label_folder = test_folder\n",
    "murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "# Load and parse label and model output files.\n",
    "label_files, output_files = find_challenge_files(label_folder, output_folder)\n",
    "murmur_labels = load_murmurs(label_files, murmur_classes)\n",
    "murmur_binary_outputs, murmur_scalar_outputs = load_classifier_outputs(output_files, murmur_classes)\n",
    "outcome_labels = load_outcomes(label_files, outcome_classes)\n",
    "outcome_binary_outputs, outcome_scalar_outputs = load_classifier_outputs(output_files, outcome_classes)\n",
    "\n",
    "\n",
    "print(np.mean(murmur_scalar_outputs[:,0]))\n",
    "print(np.mean(murmur_scalar_outputs[:,2]))\n",
    "print(np.mean(outcome_scalar_outputs[:,0]))\n",
    "print(np.mean(outcome_scalar_outputs[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c39efd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold:  0.01\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.111,0.199,0.512,15140.890\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.337,0.508,0.832,15131.464\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.332,0.000,0.000\n",
      "Accuracy,1.000,0.000,0.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.674,0.000\n",
      "Accuracy,0.990,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.05\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.111,0.199,0.512,15140.890\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.337,0.508,0.832,15131.464\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.332,0.000,0.000\n",
      "Accuracy,1.000,0.000,0.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.674,0.000\n",
      "Accuracy,0.990,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.1\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.111,0.199,0.512,15140.890\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.349,0.513,0.834,14916.196\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.332,0.000,0.000\n",
      "Accuracy,1.000,0.000,0.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.676,0.021\n",
      "Accuracy,0.990,0.011\n",
      "\n",
      "-------------\n",
      "threshold:  0.15\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.116,0.204,0.515,15131.464\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.355,0.508,0.818,14710.354\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.333,0.000,0.014\n",
      "Accuracy,1.000,0.000,0.007\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.669,0.041\n",
      "Accuracy,0.969,0.022\n",
      "\n",
      "-------------\n",
      "threshold:  0.2\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.136,0.225,0.526,14500.929\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.352,0.503,0.810,14718.457\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.339,0.000,0.069\n",
      "Accuracy,1.000,0.000,0.036\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.664,0.040\n",
      "Accuracy,0.959,0.022\n",
      "\n",
      "-------------\n",
      "threshold:  0.25\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.242,0.351,0.590,12495.812\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.371,0.508,0.804,14344.120\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.382,0.000,0.343\n",
      "Accuracy,1.000,0.000,0.209\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.664,0.078\n",
      "Accuracy,0.949,0.043\n",
      "\n",
      "-------------\n",
      "threshold:  0.3\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.334,0.487,0.660,12850.995\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.416,0.524,0.796,13513.935\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.447,0.000,0.556\n",
      "Accuracy,1.000,0.000,0.396\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.667,0.165\n",
      "Accuracy,0.929,0.097\n",
      "\n",
      "-------------\n",
      "threshold:  0.35\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.430,0.644,0.741,13474.125\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.457,0.534,0.772,12821.512\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.555,0.000,0.736\n",
      "Accuracy,1.000,0.000,0.612\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.662,0.252\n",
      "Accuracy,0.888,0.161\n",
      "\n",
      "-------------\n",
      "threshold:  0.4\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.503,0.759,0.790,15770.716\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.491,0.539,0.732,12484.703\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.673,0.000,0.837\n",
      "Accuracy,0.974,0.000,0.777\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.648,0.333\n",
      "Accuracy,0.827,0.237\n",
      "\n",
      "-------------\n",
      "threshold:  0.45\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.550,0.822,0.811,17783.522\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.508,0.534,0.676,12792.378\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.766,0.000,0.883\n",
      "Accuracy,0.947,0.000,0.871\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.621,0.395\n",
      "Accuracy,0.745,0.312\n",
      "\n",
      "-------------\n",
      "threshold:  0.5\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.571,0.853,0.795,18601.128\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.533,0.545,0.638,13143.953\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.805,0.000,0.909\n",
      "Accuracy,0.868,0.000,0.935\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.606,0.460\n",
      "Accuracy,0.684,0.398\n",
      "\n",
      "-------------\n",
      "threshold:  0.55\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.567,0.853,0.763,19225.984\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.542,0.545,0.583,14129.232\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.789,0.000,0.911\n",
      "Accuracy,0.789,0.000,0.957\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.576,0.508\n",
      "Accuracy,0.602,0.484\n",
      "\n",
      "-------------\n",
      "threshold:  0.6\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.561,0.848,0.739,19853.149\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.576,0.576,0.552,14934.478\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.778,0.000,0.905\n",
      "Accuracy,0.737,0.000,0.964\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.567,0.585\n",
      "Accuracy,0.541,0.613\n",
      "\n",
      "-------------\n",
      "threshold:  0.65\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.548,0.843,0.693,20898.997\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.574,0.581,0.485,16789.276\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.738,0.000,0.904\n",
      "Accuracy,0.632,0.000,0.986\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.518,0.630\n",
      "Accuracy,0.439,0.731\n",
      "\n",
      "-------------\n",
      "threshold:  0.7\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.529,0.832,0.655,21107.390\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.579,0.602,0.437,18395.931\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.689,0.000,0.899\n",
      "Accuracy,0.553,0.000,0.993\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.479,0.678\n",
      "Accuracy,0.357,0.860\n",
      "\n",
      "-------------\n",
      "threshold:  0.75\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.451,0.791,0.536,23194.698\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.552,0.592,0.386,19853.627\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.480,0.000,0.874\n",
      "Accuracy,0.316,0.000,1.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.418,0.685\n",
      "Accuracy,0.286,0.914\n",
      "\n",
      "-------------\n",
      "threshold:  0.8\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.779,0.633,0.316,0.738,0.402,25274.571\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.606,0.610,0.522,0.576,0.346,20899.372\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.967,0.500,0.868\n",
      "AUPRC,0.892,0.073,0.933\n",
      "F-measure,0.100,0.000,0.848\n",
      "Accuracy,0.053,0.000,1.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.606,0.606\n",
      "AUPRC,0.670,0.549\n",
      "F-measure,0.362,0.682\n",
      "Accuracy,0.235,0.935\n",
      "\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "# 8/14 15:59\n",
    "for th1 in [0.01, 0.05, 0.1, 0.15,0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8] :\n",
    "    murmur_binary_outputs[:,0] = murmur_scalar_outputs[:,0] > th1\n",
    "    murmur_binary_outputs[:,2] = murmur_scalar_outputs[:,2] > 1 - th1\n",
    "    outcome_binary_outputs[:,0] = outcome_scalar_outputs[:,0] > th1\n",
    "    outcome_binary_outputs[:,1] = outcome_scalar_outputs[:,1] > 1 - th1\n",
    "    # For each patient, set the 'Present' or 'Abnormal' class to positive if no class is positive or if multiple classes are positive.\n",
    "    murmur_labels = enforce_positives(murmur_labels, murmur_classes, 'Present')\n",
    "    murmur_binary_outputs = enforce_positives(murmur_binary_outputs, murmur_classes, 'Present')\n",
    "    outcome_labels = enforce_positives(outcome_labels, outcome_classes, 'Abnormal')\n",
    "    outcome_binary_outputs = enforce_positives(outcome_binary_outputs, outcome_classes, 'Abnormal')\n",
    "    # Evaluate the murmur model by comparing the labels and model outputs.\n",
    "    murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes = compute_auc(murmur_labels, murmur_scalar_outputs)\n",
    "    murmur_f_measure, murmur_f_measure_classes = compute_f_measure(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_accuracy, murmur_accuracy_classes = compute_accuracy(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_weighted_accuracy = compute_weighted_accuracy(murmur_labels, murmur_binary_outputs, murmur_classes) # This is the murmur scoring metric.\n",
    "    murmur_cost = compute_cost(outcome_labels, murmur_binary_outputs, outcome_classes, murmur_classes) # Use *outcomes* to score *murmurs* for the Challenge cost metric, but this is not the actual murmur scoring metric.\n",
    "    murmur_scores = (murmur_classes, murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes, \\\n",
    "                 murmur_f_measure, murmur_f_measure_classes, murmur_accuracy, murmur_accuracy_classes, murmur_weighted_accuracy, murmur_cost)\n",
    "\n",
    "    # Evaluate the outcome model by comparing the labels and model outputs.\n",
    "    outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes = compute_auc(outcome_labels, outcome_scalar_outputs)\n",
    "    outcome_f_measure, outcome_f_measure_classes = compute_f_measure(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_accuracy, outcome_accuracy_classes = compute_accuracy(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_weighted_accuracy = compute_weighted_accuracy(outcome_labels, outcome_binary_outputs, outcome_classes)\n",
    "    outcome_cost = compute_cost(outcome_labels, outcome_binary_outputs, outcome_classes, outcome_classes) # This is the clinical outcomes scoring metric.\n",
    "    outcome_scores = (outcome_classes, outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes, \\\n",
    "                  outcome_f_measure, outcome_f_measure_classes, outcome_accuracy, outcome_accuracy_classes, outcome_weighted_accuracy, outcome_cost)\n",
    "\n",
    "\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "    murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "    outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "                + '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "    print(\"threshold: \", th1)\n",
    "    print(output_string)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e91d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6cd94f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'samp_sec': 50, 'pre_emphasis': 0, 'hop_length': 256, 'win_length': 512, 'n_mels': 140, 'filter_scale': 1, 'n_bins': 80, 'fmin': 10, 'trim': 0, 'use_rr': True, 'use_b_detect': True, 'use_raw': False, 'use_mel': True, 'use_cqt': False, 'use_stft': False, 'ord1': True, 'mm_mean': False, 'dp': 0, 'fc': False, 'ext': True, 'oo_weight': 3, 'mm_weight': 3, 'chaug': 10, 'cout': 0.8, 'wunknown': 1, 'mixup': True, 'n1': 0, 'mel_shape': (140, 782, 1), 'cqt_shape': (1, 1, 1), 'stft_shape': (1, 1, 1), 's1s2_shape': (100, 313, 1), 'mm_shape': (100, 313, 1), 'use_s1s2': False, 'use_mm': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 13:21:51.201268: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-24 13:21:52.380314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 44177 MB memory:  -> device: 4, name: Quadro RTX 8000, pci bus id: 0000:3e:00.0, compute capability: 7.5\n",
      "2023-05-24 13:21:54.558493: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 13:22:00.858290: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 0s - loss: 1.4301 - accuracy: 0.7004 - auc: 0.7296"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 13:23:03.379693: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 276327520 exceeds 10% of free system memory.\n",
      "2023-05-24 13:23:03.664445: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 276327520 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 77s 2s/step - loss: 1.4301 - accuracy: 0.7004 - auc: 0.7296 - val_loss: 1.1044 - val_accuracy: 0.7892 - val_auc: 0.8222\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.9450 - accuracy: 0.7473 - auc: 0.8080 - val_loss: 0.7069 - val_accuracy: 0.4105 - val_auc: 0.4772\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.8732 - accuracy: 0.8105 - auc: 0.8500 - val_loss: 0.9524 - val_accuracy: 0.2266 - val_auc: 0.2379\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.8779 - accuracy: 0.8094 - auc: 0.8678 - val_loss: 1.0829 - val_accuracy: 0.2124 - val_auc: 0.2515\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.8910 - accuracy: 0.7961 - auc: 0.8486 - val_loss: 0.4967 - val_accuracy: 0.8241 - val_auc: 0.8900\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8818 - accuracy: 0.7977 - auc: 0.8606 - val_loss: 0.4459 - val_accuracy: 0.8114 - val_auc: 0.8888\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.8630 - accuracy: 0.7887 - auc: 0.8656 - val_loss: 0.4598 - val_accuracy: 0.7956 - val_auc: 0.8920\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.8448 - accuracy: 0.8074 - auc: 0.8650 - val_loss: 0.4320 - val_accuracy: 0.8526 - val_auc: 0.9130\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 60s 1s/step - loss: 0.8283 - accuracy: 0.8203 - auc: 0.8706 - val_loss: 0.5109 - val_accuracy: 0.8399 - val_auc: 0.9097\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8329 - accuracy: 0.8180 - auc: 0.8728 - val_loss: 0.6571 - val_accuracy: 0.5848 - val_auc: 0.6170\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8469 - accuracy: 0.8152 - auc: 0.8693 - val_loss: 0.6162 - val_accuracy: 0.6323 - val_auc: 0.7064\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7984 - accuracy: 0.8277 - auc: 0.8747 - val_loss: 0.4922 - val_accuracy: 0.8605 - val_auc: 0.8988\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.8196 - accuracy: 0.8152 - auc: 0.8805 - val_loss: 0.5769 - val_accuracy: 0.8225 - val_auc: 0.9083\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7937 - accuracy: 0.8254 - auc: 0.8843 - val_loss: 0.5562 - val_accuracy: 0.7068 - val_auc: 0.7998\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.8044 - accuracy: 0.8289 - auc: 0.8738 - val_loss: 0.3472 - val_accuracy: 0.8732 - val_auc: 0.9305\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8079 - accuracy: 0.8230 - auc: 0.8814 - val_loss: 1.4708 - val_accuracy: 0.7971 - val_auc: 0.8396\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.8221 - accuracy: 0.8184 - auc: 0.8771 - val_loss: 0.4866 - val_accuracy: 0.7940 - val_auc: 0.8846\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8067 - accuracy: 0.8254 - auc: 0.8785 - val_loss: 0.4005 - val_accuracy: 0.8732 - val_auc: 0.9245\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7900 - accuracy: 0.8336 - auc: 0.8788 - val_loss: 0.5294 - val_accuracy: 0.7464 - val_auc: 0.8402\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.8159 - accuracy: 0.8105 - auc: 0.8878 - val_loss: 0.7883 - val_accuracy: 0.3059 - val_auc: 0.3612\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.8091 - accuracy: 0.8145 - auc: 0.8878 - val_loss: 0.3725 - val_accuracy: 0.8859 - val_auc: 0.9251\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7904 - accuracy: 0.8199 - auc: 0.8865 - val_loss: 0.4004 - val_accuracy: 0.8954 - val_auc: 0.9502\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7805 - accuracy: 0.8250 - auc: 0.8822 - val_loss: 0.5124 - val_accuracy: 0.8510 - val_auc: 0.9253\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8079 - accuracy: 0.8188 - auc: 0.8774 - val_loss: 1.2265 - val_accuracy: 0.3328 - val_auc: 0.3468\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.8134 - accuracy: 0.8191 - auc: 0.8842 - val_loss: 0.4526 - val_accuracy: 0.8637 - val_auc: 0.9280\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7888 - accuracy: 0.8242 - auc: 0.8934 - val_loss: 0.5432 - val_accuracy: 0.7750 - val_auc: 0.8575\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8186 - accuracy: 0.8059 - auc: 0.8791 - val_loss: 0.8505 - val_accuracy: 0.3074 - val_auc: 0.3737\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.8065 - accuracy: 0.8223 - auc: 0.8890 - val_loss: 0.5111 - val_accuracy: 0.8764 - val_auc: 0.9110\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8133 - accuracy: 0.8172 - auc: 0.8836 - val_loss: 0.4980 - val_accuracy: 0.8352 - val_auc: 0.9000\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7995 - accuracy: 0.8199 - auc: 0.8812 - val_loss: 0.5218 - val_accuracy: 0.8669 - val_auc: 0.9312\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7918 - accuracy: 0.8129 - auc: 0.8921 - val_loss: 0.3777 - val_accuracy: 0.8811 - val_auc: 0.9218\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7965 - accuracy: 0.8191 - auc: 0.8867 - val_loss: 0.4126 - val_accuracy: 0.8700 - val_auc: 0.9285\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7778 - accuracy: 0.8246 - auc: 0.8999 - val_loss: 0.3372 - val_accuracy: 0.8970 - val_auc: 0.9435\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.8098 - accuracy: 0.8086 - auc: 0.8916 - val_loss: 0.5732 - val_accuracy: 0.8685 - val_auc: 0.9192\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7937 - accuracy: 0.8273 - auc: 0.8946 - val_loss: 0.3884 - val_accuracy: 0.8891 - val_auc: 0.9500\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7883 - accuracy: 0.8168 - auc: 0.8890 - val_loss: 0.3677 - val_accuracy: 0.9033 - val_auc: 0.9437\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7721 - accuracy: 0.8219 - auc: 0.8904 - val_loss: 0.5876 - val_accuracy: 0.7036 - val_auc: 0.7507\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7940 - accuracy: 0.8203 - auc: 0.8953 - val_loss: 0.5995 - val_accuracy: 0.7369 - val_auc: 0.8054\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7745 - accuracy: 0.8340 - auc: 0.8862 - val_loss: 0.4386 - val_accuracy: 0.8954 - val_auc: 0.9517\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7815 - accuracy: 0.8223 - auc: 0.8867 - val_loss: 0.3935 - val_accuracy: 0.8716 - val_auc: 0.9374\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7655 - accuracy: 0.8289 - auc: 0.8978 - val_loss: 0.4003 - val_accuracy: 0.8796 - val_auc: 0.9302\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7734 - accuracy: 0.8215 - auc: 0.8924 - val_loss: 0.3628 - val_accuracy: 0.9033 - val_auc: 0.9471\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7731 - accuracy: 0.8262 - auc: 0.8902 - val_loss: 0.3560 - val_accuracy: 0.9002 - val_auc: 0.9510\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7613 - accuracy: 0.8316 - auc: 0.9017 - val_loss: 0.4201 - val_accuracy: 0.8653 - val_auc: 0.9315\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7713 - accuracy: 0.8250 - auc: 0.9017 - val_loss: 0.3825 - val_accuracy: 0.8922 - val_auc: 0.9533\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.8091 - accuracy: 0.8082 - auc: 0.8887 - val_loss: 0.4954 - val_accuracy: 0.9017 - val_auc: 0.9512\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7613 - accuracy: 0.8324 - auc: 0.8966 - val_loss: 0.3834 - val_accuracy: 0.8685 - val_auc: 0.9321\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 61s 2s/step - loss: 0.7697 - accuracy: 0.8195 - auc: 0.8973 - val_loss: 0.5989 - val_accuracy: 0.6577 - val_auc: 0.7280\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7947 - accuracy: 0.8062 - auc: 0.8848 - val_loss: 0.4349 - val_accuracy: 0.8938 - val_auc: 0.9475\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7777 - accuracy: 0.8328 - auc: 0.8945 - val_loss: 0.3805 - val_accuracy: 0.9033 - val_auc: 0.9504\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7812 - accuracy: 0.8266 - auc: 0.8976 - val_loss: 0.3693 - val_accuracy: 0.9017 - val_auc: 0.9541\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7766 - accuracy: 0.8156 - auc: 0.8915 - val_loss: 0.3951 - val_accuracy: 0.9017 - val_auc: 0.9515\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7822 - accuracy: 0.8102 - auc: 0.9019 - val_loss: 0.3910 - val_accuracy: 0.9017 - val_auc: 0.9529\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7831 - accuracy: 0.8297 - auc: 0.8986 - val_loss: 0.4288 - val_accuracy: 0.8875 - val_auc: 0.9426\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7991 - accuracy: 0.8195 - auc: 0.8939 - val_loss: 0.3754 - val_accuracy: 0.9033 - val_auc: 0.9469\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7742 - accuracy: 0.8246 - auc: 0.8955 - val_loss: 0.3842 - val_accuracy: 0.8827 - val_auc: 0.9399\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7598 - accuracy: 0.8359 - auc: 0.8957 - val_loss: 0.4381 - val_accuracy: 0.8938 - val_auc: 0.9458\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7950 - accuracy: 0.8215 - auc: 0.8909 - val_loss: 0.3777 - val_accuracy: 0.9002 - val_auc: 0.9545\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7503 - accuracy: 0.8223 - auc: 0.8979 - val_loss: 0.3569 - val_accuracy: 0.9002 - val_auc: 0.9474\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7634 - accuracy: 0.8309 - auc: 0.9076 - val_loss: 0.3742 - val_accuracy: 0.8906 - val_auc: 0.9371\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7770 - accuracy: 0.8238 - auc: 0.8928 - val_loss: 0.4234 - val_accuracy: 0.9002 - val_auc: 0.9457\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7710 - accuracy: 0.8246 - auc: 0.9032 - val_loss: 0.3663 - val_accuracy: 0.8986 - val_auc: 0.9533\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7654 - accuracy: 0.8258 - auc: 0.9075 - val_loss: 0.3531 - val_accuracy: 0.9081 - val_auc: 0.9524\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7765 - accuracy: 0.8215 - auc: 0.9021 - val_loss: 0.4671 - val_accuracy: 0.8827 - val_auc: 0.9455\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7311 - accuracy: 0.8328 - auc: 0.8976 - val_loss: 0.3870 - val_accuracy: 0.9002 - val_auc: 0.9520\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7576 - accuracy: 0.8363 - auc: 0.8913 - val_loss: 0.3608 - val_accuracy: 0.9017 - val_auc: 0.9539\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7558 - accuracy: 0.8258 - auc: 0.9080 - val_loss: 0.4167 - val_accuracy: 0.9049 - val_auc: 0.9513\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7738 - accuracy: 0.8242 - auc: 0.9064 - val_loss: 0.3829 - val_accuracy: 0.8986 - val_auc: 0.9516\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7520 - accuracy: 0.8363 - auc: 0.9051 - val_loss: 0.3598 - val_accuracy: 0.9017 - val_auc: 0.9532\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7625 - accuracy: 0.8168 - auc: 0.9018 - val_loss: 0.4102 - val_accuracy: 0.9097 - val_auc: 0.9521\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7655 - accuracy: 0.8246 - auc: 0.9105 - val_loss: 0.3990 - val_accuracy: 0.9049 - val_auc: 0.9506\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7756 - accuracy: 0.8156 - auc: 0.9026 - val_loss: 0.4322 - val_accuracy: 0.8986 - val_auc: 0.9462\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7471 - accuracy: 0.8223 - auc: 0.9031 - val_loss: 0.3787 - val_accuracy: 0.9128 - val_auc: 0.9533\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7360 - accuracy: 0.8273 - auc: 0.9095 - val_loss: 0.3872 - val_accuracy: 0.9113 - val_auc: 0.9527\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7527 - accuracy: 0.8313 - auc: 0.9040 - val_loss: 0.3943 - val_accuracy: 0.9081 - val_auc: 0.9527\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7504 - accuracy: 0.8395 - auc: 0.9033 - val_loss: 0.3857 - val_accuracy: 0.9033 - val_auc: 0.9525\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7577 - accuracy: 0.8324 - auc: 0.9076 - val_loss: 0.3761 - val_accuracy: 0.9065 - val_auc: 0.9526\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7503 - accuracy: 0.8281 - auc: 0.9050 - val_loss: 0.3574 - val_accuracy: 0.9049 - val_auc: 0.9522\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7389 - accuracy: 0.8367 - auc: 0.9100 - val_loss: 0.3655 - val_accuracy: 0.9033 - val_auc: 0.9536\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7673 - accuracy: 0.8141 - auc: 0.9048 - val_loss: 0.3966 - val_accuracy: 0.9097 - val_auc: 0.9531\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7244 - accuracy: 0.8262 - auc: 0.9066 - val_loss: 0.3690 - val_accuracy: 0.9002 - val_auc: 0.9518\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7412 - accuracy: 0.8297 - auc: 0.9079 - val_loss: 0.3713 - val_accuracy: 0.9033 - val_auc: 0.9520\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7630 - accuracy: 0.8254 - auc: 0.9023 - val_loss: 0.3778 - val_accuracy: 0.9065 - val_auc: 0.9510\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7598 - accuracy: 0.8301 - auc: 0.9016 - val_loss: 0.4011 - val_accuracy: 0.9049 - val_auc: 0.9503\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7494 - accuracy: 0.8254 - auc: 0.9026 - val_loss: 0.3949 - val_accuracy: 0.9097 - val_auc: 0.9518\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7406 - accuracy: 0.8391 - auc: 0.9088 - val_loss: 0.3867 - val_accuracy: 0.9097 - val_auc: 0.9527\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7536 - accuracy: 0.8281 - auc: 0.9094 - val_loss: 0.3778 - val_accuracy: 0.9017 - val_auc: 0.9520\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7555 - accuracy: 0.8355 - auc: 0.9102 - val_loss: 0.3760 - val_accuracy: 0.9065 - val_auc: 0.9525\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7576 - accuracy: 0.8414 - auc: 0.9031 - val_loss: 0.3810 - val_accuracy: 0.9033 - val_auc: 0.9527\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7611 - accuracy: 0.8238 - auc: 0.9022 - val_loss: 0.3797 - val_accuracy: 0.9065 - val_auc: 0.9528\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7362 - accuracy: 0.8230 - auc: 0.9147 - val_loss: 0.3759 - val_accuracy: 0.9065 - val_auc: 0.9519\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7527 - accuracy: 0.8313 - auc: 0.8965 - val_loss: 0.3863 - val_accuracy: 0.9081 - val_auc: 0.9524\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - 60s 2s/step - loss: 0.7680 - accuracy: 0.8223 - auc: 0.9057 - val_loss: 0.3904 - val_accuracy: 0.9097 - val_auc: 0.9526\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7529 - accuracy: 0.8234 - auc: 0.9044 - val_loss: 0.3906 - val_accuracy: 0.9097 - val_auc: 0.9525\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 60s 2s/step - loss: 0.7526 - accuracy: 0.8340 - auc: 0.9020 - val_loss: 0.3855 - val_accuracy: 0.9113 - val_auc: 0.9530\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7624 - accuracy: 0.8270 - auc: 0.9015 - val_loss: 0.3927 - val_accuracy: 0.9065 - val_auc: 0.9525\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7390 - accuracy: 0.8289 - auc: 0.9075 - val_loss: 0.3822 - val_accuracy: 0.9065 - val_auc: 0.9527\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7485 - accuracy: 0.8289 - auc: 0.9079 - val_loss: 0.3808 - val_accuracy: 0.9065 - val_auc: 0.9523\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7482 - accuracy: 0.8301 - auc: 0.9071 - val_loss: 0.3859 - val_accuracy: 0.9097 - val_auc: 0.9527\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 61s 2s/step - loss: 0.7570 - accuracy: 0.8258 - auc: 0.9026 - val_loss: 0.3906 - val_accuracy: 0.9097 - val_auc: 0.9525\n",
      "Epoch 1/100\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.2335 - accuracy: 0.4758 - auc: 0.4889"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 15:03:20.469461: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 276327520 exceeds 10% of free system memory.\n",
      "2023-05-24 15:03:20.670277: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 276327520 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 18s 383ms/step - loss: 1.2335 - accuracy: 0.4758 - auc: 0.4889 - val_loss: 0.6909 - val_accuracy: 0.5293 - val_auc: 0.6021\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 1.2371 - accuracy: 0.4875 - auc: 0.4993 - val_loss: 1.6016 - val_accuracy: 0.4992 - val_auc: 0.4698\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 1.2415 - accuracy: 0.4895 - auc: 0.4950 - val_loss: 0.9712 - val_accuracy: 0.4992 - val_auc: 0.5002\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.2272 - accuracy: 0.4914 - auc: 0.5036 - val_loss: 0.8306 - val_accuracy: 0.4992 - val_auc: 0.6110\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 1.2150 - accuracy: 0.4770 - auc: 0.5194 - val_loss: 0.6973 - val_accuracy: 0.5214 - val_auc: 0.5940\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 1.2196 - accuracy: 0.4762 - auc: 0.5085 - val_loss: 0.7032 - val_accuracy: 0.4992 - val_auc: 0.6020\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 1.2166 - accuracy: 0.4809 - auc: 0.5164 - val_loss: 0.7412 - val_accuracy: 0.4992 - val_auc: 0.5972\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 13s 330ms/step - loss: 1.2159 - accuracy: 0.4816 - auc: 0.5208 - val_loss: 0.8029 - val_accuracy: 0.4992 - val_auc: 0.6166\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.2306 - accuracy: 0.4918 - auc: 0.5139 - val_loss: 1.0313 - val_accuracy: 0.4992 - val_auc: 0.5850\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 1.2191 - accuracy: 0.4785 - auc: 0.5085 - val_loss: 0.6545 - val_accuracy: 0.6086 - val_auc: 0.6644\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 14s 348ms/step - loss: 1.2186 - accuracy: 0.4715 - auc: 0.5059 - val_loss: 0.9276 - val_accuracy: 0.4992 - val_auc: 0.5513\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 1.2077 - accuracy: 0.4703 - auc: 0.5261 - val_loss: 0.6522 - val_accuracy: 0.5182 - val_auc: 0.6087\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 14s 346ms/step - loss: 1.2148 - accuracy: 0.4836 - auc: 0.5292 - val_loss: 0.6855 - val_accuracy: 0.4881 - val_auc: 0.5164\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 14s 339ms/step - loss: 1.2095 - accuracy: 0.4684 - auc: 0.5271 - val_loss: 0.6992 - val_accuracy: 0.5071 - val_auc: 0.5549\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 1.2082 - accuracy: 0.4812 - auc: 0.5210 - val_loss: 1.1022 - val_accuracy: 0.4992 - val_auc: 0.6115\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 1.2117 - accuracy: 0.4840 - auc: 0.5308 - val_loss: 0.7459 - val_accuracy: 0.4945 - val_auc: 0.5999\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 13s 339ms/step - loss: 1.2089 - accuracy: 0.4816 - auc: 0.5305 - val_loss: 0.7448 - val_accuracy: 0.4992 - val_auc: 0.5903\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 1.2120 - accuracy: 0.4785 - auc: 0.5321 - val_loss: 0.7296 - val_accuracy: 0.5008 - val_auc: 0.5902\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 13s 339ms/step - loss: 1.2043 - accuracy: 0.4820 - auc: 0.5366 - val_loss: 0.6429 - val_accuracy: 0.5911 - val_auc: 0.6560\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 13s 339ms/step - loss: 1.2112 - accuracy: 0.4895 - auc: 0.5272 - val_loss: 0.7269 - val_accuracy: 0.5024 - val_auc: 0.5241\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 14s 347ms/step - loss: 1.2156 - accuracy: 0.4934 - auc: 0.5369 - val_loss: 0.7549 - val_accuracy: 0.5024 - val_auc: 0.5632\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 1.2044 - accuracy: 0.4691 - auc: 0.5191 - val_loss: 0.6877 - val_accuracy: 0.5309 - val_auc: 0.5620\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 13s 329ms/step - loss: 1.2013 - accuracy: 0.4758 - auc: 0.5291 - val_loss: 0.7952 - val_accuracy: 0.5008 - val_auc: 0.5977\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 1.2054 - accuracy: 0.4801 - auc: 0.5347 - val_loss: 0.7307 - val_accuracy: 0.4992 - val_auc: 0.6056\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.2140 - accuracy: 0.4766 - auc: 0.5319 - val_loss: 0.6914 - val_accuracy: 0.4992 - val_auc: 0.6024\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 14s 347ms/step - loss: 1.2020 - accuracy: 0.4797 - auc: 0.5439 - val_loss: 0.6164 - val_accuracy: 0.6624 - val_auc: 0.7314\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 1.2022 - accuracy: 0.4918 - auc: 0.5485 - val_loss: 0.8323 - val_accuracy: 0.4992 - val_auc: 0.5940\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 1.2113 - accuracy: 0.4832 - auc: 0.5410 - val_loss: 0.6778 - val_accuracy: 0.5071 - val_auc: 0.6141\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 1.2166 - accuracy: 0.4785 - auc: 0.5225 - val_loss: 0.8269 - val_accuracy: 0.4865 - val_auc: 0.5078\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.2017 - accuracy: 0.4676 - auc: 0.5319 - val_loss: 0.7838 - val_accuracy: 0.5008 - val_auc: 0.5932\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 1.2001 - accuracy: 0.4867 - auc: 0.5471 - val_loss: 0.6282 - val_accuracy: 0.6418 - val_auc: 0.7170\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 14s 346ms/step - loss: 1.2105 - accuracy: 0.4863 - auc: 0.5378 - val_loss: 0.7699 - val_accuracy: 0.4992 - val_auc: 0.6085\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 1.2004 - accuracy: 0.4875 - auc: 0.5478 - val_loss: 0.7139 - val_accuracy: 0.5087 - val_auc: 0.6087\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 1.2089 - accuracy: 0.4730 - auc: 0.5287 - val_loss: 0.7300 - val_accuracy: 0.5008 - val_auc: 0.5990\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.1986 - accuracy: 0.4836 - auc: 0.5497 - val_loss: 0.6193 - val_accuracy: 0.6656 - val_auc: 0.7218\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 1.2026 - accuracy: 0.4836 - auc: 0.5413 - val_loss: 0.6705 - val_accuracy: 0.5198 - val_auc: 0.6002\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 14s 347ms/step - loss: 1.2026 - accuracy: 0.4824 - auc: 0.5462 - val_loss: 0.6492 - val_accuracy: 0.5420 - val_auc: 0.6200\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 1.1981 - accuracy: 0.4906 - auc: 0.5489 - val_loss: 0.6848 - val_accuracy: 0.5040 - val_auc: 0.6068\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 14s 339ms/step - loss: 1.1970 - accuracy: 0.4898 - auc: 0.5466 - val_loss: 0.6923 - val_accuracy: 0.5055 - val_auc: 0.6044\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.2126 - accuracy: 0.4906 - auc: 0.5493 - val_loss: 0.6644 - val_accuracy: 0.5119 - val_auc: 0.6094\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 1.1841 - accuracy: 0.4703 - auc: 0.5355 - val_loss: 0.7196 - val_accuracy: 0.5024 - val_auc: 0.5932\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 13s 329ms/step - loss: 1.2024 - accuracy: 0.4887 - auc: 0.5454 - val_loss: 0.7663 - val_accuracy: 0.4992 - val_auc: 0.5912\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 1.1924 - accuracy: 0.4820 - auc: 0.5480 - val_loss: 0.6610 - val_accuracy: 0.5293 - val_auc: 0.6161\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 13s 339ms/step - loss: 1.2136 - accuracy: 0.4953 - auc: 0.5493 - val_loss: 0.7342 - val_accuracy: 0.5071 - val_auc: 0.5824\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 14s 346ms/step - loss: 1.2087 - accuracy: 0.4836 - auc: 0.5514 - val_loss: 0.7205 - val_accuracy: 0.4992 - val_auc: 0.6079\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 13s 327ms/step - loss: 1.2093 - accuracy: 0.4957 - auc: 0.5509 - val_loss: 0.7608 - val_accuracy: 0.4992 - val_auc: 0.5749\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 13s 332ms/step - loss: 1.2112 - accuracy: 0.4887 - auc: 0.5504 - val_loss: 0.6851 - val_accuracy: 0.5087 - val_auc: 0.5978\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 1.1763 - accuracy: 0.4691 - auc: 0.5532 - val_loss: 0.6398 - val_accuracy: 0.6371 - val_auc: 0.6978\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.1902 - accuracy: 0.4844 - auc: 0.5558 - val_loss: 0.6309 - val_accuracy: 0.6022 - val_auc: 0.6690\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 1.1947 - accuracy: 0.4961 - auc: 0.5612 - val_loss: 0.7202 - val_accuracy: 0.5008 - val_auc: 0.6017\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 1.1882 - accuracy: 0.4688 - auc: 0.5460 - val_loss: 0.6725 - val_accuracy: 0.5357 - val_auc: 0.6090\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 1.2005 - accuracy: 0.4895 - auc: 0.5497 - val_loss: 0.6419 - val_accuracy: 0.5737 - val_auc: 0.6400\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 1.1941 - accuracy: 0.4824 - auc: 0.5596 - val_loss: 0.7221 - val_accuracy: 0.5071 - val_auc: 0.5834\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 13s 330ms/step - loss: 1.1846 - accuracy: 0.4820 - auc: 0.5520 - val_loss: 0.7579 - val_accuracy: 0.5024 - val_auc: 0.5887\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 1.2016 - accuracy: 0.4855 - auc: 0.5442 - val_loss: 0.7300 - val_accuracy: 0.5024 - val_auc: 0.5971\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 1.1969 - accuracy: 0.5016 - auc: 0.5607 - val_loss: 0.6729 - val_accuracy: 0.5309 - val_auc: 0.5927\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 1.1951 - accuracy: 0.4855 - auc: 0.5610 - val_loss: 0.6331 - val_accuracy: 0.6086 - val_auc: 0.6661\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 1.1882 - accuracy: 0.4930 - auc: 0.5654 - val_loss: 0.6514 - val_accuracy: 0.5674 - val_auc: 0.6320\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 1.1831 - accuracy: 0.4871 - auc: 0.5661 - val_loss: 0.6727 - val_accuracy: 0.5452 - val_auc: 0.6134\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.1731 - accuracy: 0.4699 - auc: 0.5533 - val_loss: 0.6326 - val_accuracy: 0.6466 - val_auc: 0.7142\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 14s 347ms/step - loss: 1.1800 - accuracy: 0.4789 - auc: 0.5625 - val_loss: 0.6748 - val_accuracy: 0.5309 - val_auc: 0.6028\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 1.1947 - accuracy: 0.4934 - auc: 0.5692 - val_loss: 0.6912 - val_accuracy: 0.5071 - val_auc: 0.5985\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 13s 330ms/step - loss: 1.1814 - accuracy: 0.4941 - auc: 0.5689 - val_loss: 0.7701 - val_accuracy: 0.5040 - val_auc: 0.5874\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 1.1796 - accuracy: 0.4879 - auc: 0.5686 - val_loss: 0.6866 - val_accuracy: 0.5182 - val_auc: 0.5943\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 13s 329ms/step - loss: 1.1805 - accuracy: 0.4867 - auc: 0.5694 - val_loss: 0.6336 - val_accuracy: 0.6054 - val_auc: 0.6618\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 1.1744 - accuracy: 0.4770 - auc: 0.5648 - val_loss: 0.6870 - val_accuracy: 0.5246 - val_auc: 0.5900\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 1.1806 - accuracy: 0.4785 - auc: 0.5684 - val_loss: 0.6448 - val_accuracy: 0.5895 - val_auc: 0.6384\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 1.1934 - accuracy: 0.5016 - auc: 0.5651 - val_loss: 0.7342 - val_accuracy: 0.5103 - val_auc: 0.5862\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 1.1758 - accuracy: 0.4781 - auc: 0.5723 - val_loss: 0.6920 - val_accuracy: 0.5214 - val_auc: 0.5923\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 1.1722 - accuracy: 0.4918 - auc: 0.5704 - val_loss: 0.6665 - val_accuracy: 0.5436 - val_auc: 0.6096\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 1.1794 - accuracy: 0.4895 - auc: 0.5667 - val_loss: 0.6891 - val_accuracy: 0.5261 - val_auc: 0.5941\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.1758 - accuracy: 0.4781 - auc: 0.5605 - val_loss: 0.7175 - val_accuracy: 0.5119 - val_auc: 0.5807\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 1.1878 - accuracy: 0.5012 - auc: 0.5715 - val_loss: 0.6760 - val_accuracy: 0.5341 - val_auc: 0.6022\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 1.1860 - accuracy: 0.4836 - auc: 0.5632 - val_loss: 0.7160 - val_accuracy: 0.5119 - val_auc: 0.5841\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 1.1745 - accuracy: 0.4887 - auc: 0.5818 - val_loss: 0.7073 - val_accuracy: 0.5119 - val_auc: 0.5855\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 1.1771 - accuracy: 0.5000 - auc: 0.5740 - val_loss: 0.6932 - val_accuracy: 0.5230 - val_auc: 0.5915\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 13s 338ms/step - loss: 1.1790 - accuracy: 0.4949 - auc: 0.5756 - val_loss: 0.6844 - val_accuracy: 0.5277 - val_auc: 0.5999\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 13s 339ms/step - loss: 1.1807 - accuracy: 0.4836 - auc: 0.5604 - val_loss: 0.6849 - val_accuracy: 0.5214 - val_auc: 0.6016\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 13s 331ms/step - loss: 1.1866 - accuracy: 0.4824 - auc: 0.5588 - val_loss: 0.7147 - val_accuracy: 0.5103 - val_auc: 0.5804\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.1803 - accuracy: 0.4918 - auc: 0.5738 - val_loss: 0.7231 - val_accuracy: 0.5040 - val_auc: 0.5771\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 14s 339ms/step - loss: 1.1733 - accuracy: 0.4902 - auc: 0.5786 - val_loss: 0.7311 - val_accuracy: 0.5024 - val_auc: 0.5714\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 13s 332ms/step - loss: 1.1784 - accuracy: 0.4895 - auc: 0.5785 - val_loss: 0.6964 - val_accuracy: 0.5198 - val_auc: 0.5927\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 1.1858 - accuracy: 0.4750 - auc: 0.5644 - val_loss: 0.6976 - val_accuracy: 0.5214 - val_auc: 0.5888\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.1713 - accuracy: 0.4801 - auc: 0.5740 - val_loss: 0.7167 - val_accuracy: 0.5119 - val_auc: 0.5838\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 14s 342ms/step - loss: 1.1679 - accuracy: 0.4902 - auc: 0.5781 - val_loss: 0.7190 - val_accuracy: 0.5135 - val_auc: 0.5856\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 1.1745 - accuracy: 0.4941 - auc: 0.5695 - val_loss: 0.7162 - val_accuracy: 0.5135 - val_auc: 0.5877\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 14s 346ms/step - loss: 1.1815 - accuracy: 0.4863 - auc: 0.5674 - val_loss: 0.7051 - val_accuracy: 0.5214 - val_auc: 0.5889\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 13s 337ms/step - loss: 1.1743 - accuracy: 0.4945 - auc: 0.5781 - val_loss: 0.7255 - val_accuracy: 0.5119 - val_auc: 0.5812\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 1.1772 - accuracy: 0.4980 - auc: 0.5806 - val_loss: 0.7159 - val_accuracy: 0.5182 - val_auc: 0.5857\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 1.1764 - accuracy: 0.4934 - auc: 0.5632 - val_loss: 0.7106 - val_accuracy: 0.5230 - val_auc: 0.5842\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 14s 339ms/step - loss: 1.1948 - accuracy: 0.5121 - auc: 0.5684 - val_loss: 0.7071 - val_accuracy: 0.5230 - val_auc: 0.5890\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 1.1739 - accuracy: 0.5051 - auc: 0.5737 - val_loss: 0.7013 - val_accuracy: 0.5230 - val_auc: 0.5911\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 13s 338ms/step - loss: 1.1684 - accuracy: 0.4773 - auc: 0.5688 - val_loss: 0.7003 - val_accuracy: 0.5230 - val_auc: 0.5894\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 1.1752 - accuracy: 0.4879 - auc: 0.5660 - val_loss: 0.6969 - val_accuracy: 0.5182 - val_auc: 0.5885\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 14s 343ms/step - loss: 1.1722 - accuracy: 0.4805 - auc: 0.5736 - val_loss: 0.6981 - val_accuracy: 0.5214 - val_auc: 0.5898\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 14s 354ms/step - loss: 1.1927 - accuracy: 0.4965 - auc: 0.5694 - val_loss: 0.6979 - val_accuracy: 0.5198 - val_auc: 0.5908\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 14s 344ms/step - loss: 1.1771 - accuracy: 0.4953 - auc: 0.5734 - val_loss: 0.6906 - val_accuracy: 0.5277 - val_auc: 0.5938\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 1.1760 - accuracy: 0.4914 - auc: 0.5738 - val_loss: 0.7041 - val_accuracy: 0.5230 - val_auc: 0.5857\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 13s 336ms/step - loss: 1.1713 - accuracy: 0.4918 - auc: 0.5771 - val_loss: 0.7043 - val_accuracy: 0.5230 - val_auc: 0.5859\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 14s 340ms/step - loss: 1.1817 - accuracy: 0.4949 - auc: 0.5734 - val_loss: 0.7102 - val_accuracy: 0.5230 - val_auc: 0.5840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Challenge model...\n",
      "Running model on Challenge data...\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f20042a2af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f1fe46c2dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Done.\n",
      "0.40947464927640886\n",
      "0.5905253531421042\n",
      "0.6724696927045652\n",
      "0.32753030932386507\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.signal import hilbert\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import librosa\n",
    "import librosa.display\n",
    "import math\n",
    "import sys\n",
    "sys.path.insert(0,'/Data/hmd/hmd_sy/evaluation-2022')\n",
    "sys.path.insert(0,'/Data/hmd/hmd_sy/notebooks')\n",
    "sys.path.insert(0,'utils')\n",
    "from helper_code import *\n",
    "from get_feature import *\n",
    "from models import *\n",
    "from Generator0 import *\n",
    "\n",
    "import datetime\n",
    "from evaluate_model import *\n",
    "from scipy import special\n",
    "import scipy.io as sio\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler,ModelCheckpoint\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[4], 'GPU')\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# data_folder =  '/Data/hmd/physionet.org/files/circor-heart-sound/1.0.3/training_data'\n",
    "train_folder =  '/Data/hmd/data_split/murmur/train/'\n",
    "test_folder = '/Data/hmd/data_split/murmur/test/'\n",
    "\n",
    "import typing\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "## filtering (s1, s2 detect)\n",
    "############################\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order):\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    # Get the filter coefficients \n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "############################\n",
    "## feature_extract_bound_melspec\n",
    "############################\n",
    "\n",
    "def feature_extract_bound_melspec(data, samp_sec=20, sr = 4000, pre_emphasis = 0, hop_length=256, win_length = 512, n_mels = 100):\n",
    "    \n",
    "    if samp_sec:\n",
    "        if len(data) > sample_rate * samp_sec :\n",
    "            n_samp = len(data) // int(sample_rate * samp_sec)\n",
    "            signal = []\n",
    "            for i in range(n_samp) :\n",
    "                signal.append(data[ int(sample_rate * samp_sec)*i:(int(sample_rate * samp_sec)*(i+1))])\n",
    "        else :\n",
    "            n_samp = 1\n",
    "            signal = np.zeros(int(sample_rate*samp_sec,))\n",
    "            for i in range(int(sample_rate * samp_sec) // len(data)) :\n",
    "                signal[(i)*len(data):(i+1)*len(data)] = data\n",
    "            num_last = int(sample_rate * samp_sec) - len(data)*(i+1)\n",
    "            signal[(i+1)*len(data):int(sample_rate * samp_sec)] = data[:num_last]\n",
    "            signal = [signal]\n",
    "    else:\n",
    "        n_samp = 1\n",
    "        signal = [data]\n",
    "\n",
    "    Sig = []\n",
    "    for i in range(n_samp) :\n",
    "        if pre_emphasis :\n",
    "            emphasized_signal = np.append(signal[i][0], signal[i][1:] - pre_emphasis * signal[i][:-1])\n",
    "        else :\n",
    "            emphasized_signal = signal[i]\n",
    "\n",
    "        Sig.append(librosa.power_to_db(librosa.feature.melspectrogram(y=emphasized_signal, sr= sr, n_mels=n_mels, n_fft=win_length, hop_length=hop_length, win_length=win_length)))\n",
    "\n",
    "    return Sig\n",
    "\n",
    "\n",
    "\n",
    "class Generator0():\n",
    "    def __init__(self, X_train, y_train, batch_size=32, beta_param=0.2, mixup = True, lowpass = False, highpass = False, ranfilter2 = False, shuffle=True, datagen=None, chaug = False, cout = False):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = beta_param\n",
    "        self.mixup = mixup\n",
    "        self.shuffle = shuffle\n",
    "        self.sample_num = len(y_train)\n",
    "        self.datagen = datagen\n",
    "\n",
    "        ## ffm \n",
    "        \n",
    "        self.lowpass = lowpass\n",
    "        self.highpass = highpass\n",
    "        self.ranfilter = ranfilter2\n",
    "        self.chaug = chaug\n",
    "        self.cutout = cout        \n",
    "\n",
    "\n",
    "    def __call__(self):\n",
    "        while True:\n",
    "            indexes = self.__get_exploration_order()\n",
    "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
    "\n",
    "            for i in range(itr_num):\n",
    "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
    "                X, y = self.__data_generation(batch_ids)\n",
    "\n",
    "                yield X, y\n",
    "\n",
    "    def __get_exploration_order(self):\n",
    "        indexes = np.arange(self.sample_num)\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def __data_generation(self, batch_ids):\n",
    "        \n",
    "        \n",
    "        def get_box(lambda_value, nf, nt):\n",
    "            cut_rat = np.sqrt(1.0 - lambda_value)\n",
    "\n",
    "            cut_w = int(nf * cut_rat)  # rw\n",
    "            cut_h = int(nt * cut_rat)  # rh\n",
    "\n",
    "            cut_x = int(np.random.uniform(low=0, high=nf))  # rx\n",
    "            cut_y = int(np.random.uniform(low=0, high=nt))  # ry\n",
    "\n",
    "            boundaryx1 = np.minimum(np.maximum(cut_x - cut_w // 2, 0), nf) #tf.clip_by_value(cut_x - cut_w // 2, 0, IMG_SIZE_x)\n",
    "            boundaryy1 = np.minimum(np.maximum(cut_y - cut_h // 2, 0), nt) #tf.clip_by_value(cut_y - cut_h // 2, 0, IMG_SIZE_y)\n",
    "            bbx2 = np.minimum(np.maximum(cut_x + cut_w // 2, 0), nf) #tf.clip_by_value(cut_x + cut_w // 2, 0, IMG_SIZE_x)\n",
    "            bby2 = np.minimum(np.maximum(cut_y + cut_h // 2, 0), nt) #tf.clip_by_value(cut_y + cut_h // 2, 0, IMG_SIZE_y)\n",
    "\n",
    "            target_h = bby2 - boundaryy1\n",
    "            if target_h == 0:\n",
    "                target_h += 1\n",
    "\n",
    "            target_w = bbx2 - boundaryx1\n",
    "            if target_w == 0:\n",
    "                target_w += 1\n",
    "\n",
    "            return boundaryx1, boundaryy1, target_h, target_w           \n",
    "        \n",
    "        \n",
    "        if isinstance(self.X_train, list):\n",
    "            X = []\n",
    "            for X_temp in self.X_train:\n",
    "                if len(X_temp.shape) == 4: \n",
    "                    _, h, w, c = X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                elif len(X_temp.shape) == 3:\n",
    "                    _, h, w = X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size, 1, 1)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                elif len(X_temp.shape) == 2:\n",
    "                    _, h = X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size, 1)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                elif len(X_temp.shape) == 1:\n",
    "                    _= X_temp.shape\n",
    "                    l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                    X_l = l.reshape(self.batch_size,)\n",
    "                    y_l = l.reshape(self.batch_size, 1)\n",
    "                \n",
    "                X1 = X_temp[batch_ids[:self.batch_size]].copy()\n",
    "                X2 = X_temp[batch_ids[self.batch_size:]].copy()\n",
    "                \n",
    "                if self.mixup :\n",
    "                    Xn = X1 * X_l + X2 * (1 - X_l)\n",
    "                else :\n",
    "                    Xn = X1\n",
    "                if len(X_temp.shape) == 4: \n",
    "                    _, h, w, c = X_temp.shape\n",
    "                    if h != 1 :\n",
    "                        if self.lowpass :\n",
    "                            uv, lp = self.lowpass\n",
    "                            dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                            for i in range(self.batch_size) :\n",
    "                                loc1 = np.random.choice(lp, size = 1)[0]\n",
    "                                Xn[i,:loc1,:,:] = 0\n",
    "                        if self.highpass :\n",
    "                            uv, hp = self.highpass\n",
    "                            dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                            for i in range(self.batch_size) :\n",
    "                                loc1 = np.random.choice(hp, size = 1)[0]\n",
    "                                Xn[i,loc1:,:,:] = 0\n",
    "                        if self.ranfilter :                \n",
    "                            raniter, ranf = self.ranfilter\n",
    "                            dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "                            for i in range(self.batch_size) :\n",
    "                                if dec1[i] > 0 :\n",
    "                                    for j in range(dec1[i]) :\n",
    "                                        b1 = np.random.choice(ranf, size = 1)[0]\n",
    "                                        loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "                                        Xn[i, loc1:(loc1 + b1 - 1), :] = 0\n",
    "                        if self.chaug :\n",
    "                            for i in range(self.batch_size) :\n",
    "                                noiselv = np.random.uniform(low= - self.chaug, high= self.chaug)\n",
    "                                Xn[i,:] += noiselv\n",
    "                        if self.cutout :\n",
    "                            lambda1 = np.random.beta(self.cutout, self.cutout, size = self.batch_size)   ## beta_param default : 0.7  STC페이퍼 추천은 0.6~0.8\n",
    "                            for i in range(self.batch_size) :\n",
    "                                boundaryx1, boundaryy1, target_h, target_w = get_box(lambda1[i], h, w)\n",
    "                                Xn[i, boundaryx1:(boundaryx1+target_h), boundaryy1:(boundaryy1+target_w),: ] = 0\n",
    "                \n",
    "#                 if len(X_temp.shape) == 3: \n",
    "                    \n",
    "#                     _, h, w = X_temp.shape\n",
    "                    \n",
    "#                     if h != 1 :\n",
    "                        \n",
    "#                         if self.lowpass :\n",
    "#                             uv, lp = self.lowpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(lp, size = 1)[0]\n",
    "#                                 Xn[i,:loc1,:] = 0\n",
    "#                         if self.highpass :\n",
    "#                             uv, hp = self.highpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(hp, size = 1)[0]\n",
    "#                                 Xn[i,loc1:,:] = 0\n",
    "#                         if self.ranfilter :                \n",
    "#                             raniter, ranf = self.ranfilter\n",
    "#                             dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 if dec1[i] > 0 :\n",
    "#                                     for j in range(dec1[i]) :\n",
    "#                                         b1 = np.random.choice(ranf, size = 1)[0]\n",
    "#                                         loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "#                                         Xn[i, loc1:(loc1 + b1 - 1), :] = 0                    \n",
    "                X.append(Xn)\n",
    "        else:\n",
    "            if len(self.X_train.shape) == 4: \n",
    "                _, h, w, c = self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "            elif len(self.X_train.shape) == 3:\n",
    "                _, h, w = self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size, 1, 1)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "            elif len(self.X_train.shape) == 2:\n",
    "                _, h = self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size, 1)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "            elif len(self.X_train.shape) == 1:\n",
    "                _= self.X_train.shape\n",
    "                l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
    "                X_l = l.reshape(self.batch_size,)\n",
    "                y_l = l.reshape(self.batch_size, 1)\n",
    "\n",
    "            X1 = self.X_train[batch_ids[:self.batch_size]].copy()\n",
    "            X2 = self.X_train[batch_ids[self.batch_size:]].copy()\n",
    "            if self.mixup :\n",
    "                Xn = X1 * X_l + X2 * (1 - X_l)\n",
    "            else :\n",
    "                Xn = X1\n",
    "\n",
    "            if len(self.X_train.shape) == 4: \n",
    "                _, h, w, c = X_temp.shape\n",
    "                if self.lowpass :\n",
    "                    uv, lp = self.lowpass\n",
    "                    dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                    for i in range(self.batch_size) :\n",
    "                        loc1 = np.random.choice(lp, size = 1)[0]\n",
    "                        Xn[i,:loc1,:,:] = 0\n",
    "                if self.highpass :\n",
    "                    uv, hp = self.highpass\n",
    "                    dec1 = np.random.choice(2, size = self.batch_size)\n",
    "                    for i in range(self.batch_size) :\n",
    "                        loc1 = np.random.choice(hp, size = 1)[0]\n",
    "                        Xn[i,loc1:,:,:] = 0\n",
    "                if self.ranfilter :                \n",
    "                    raniter, ranf = self.ranfilter\n",
    "                    dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "                    for i in range(self.batch_size) :\n",
    "                        if dec1[i] > 0 :\n",
    "                            for j in range(dec1[i]) :\n",
    "                                b1 = np.random.choice(ranf, size = 1)[0]\n",
    "                                loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "                                Xn[i, loc1:(loc1 + b1 - 1), :] = 0\n",
    "                \n",
    "#                 if len(self.X_train.shape) == 3:\n",
    "#                     _, h, w = X_temp.shape\n",
    "                    \n",
    "#                     if h != 1 :\n",
    "#                         if self.lowpass :\n",
    "#                             uv, lp = self.lowpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(lp, size = 1)[0]\n",
    "#                                 Xn[i,:loc1,:] = 0\n",
    "#                         if self.highpass :\n",
    "#                             uv, hp = self.highpass\n",
    "#                             dec1 = np.random.choice(2, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 loc1 = np.random.choice(hp, size = 1)[0]\n",
    "#                                 Xn[i,loc1:,:] = 0\n",
    "#                         if self.ranfilter :                \n",
    "#                             raniter, ranf = self.ranfilter\n",
    "#                             dec1 = np.random.choice(raniter, size = self.batch_size)\n",
    "#                             for i in range(self.batch_size) :\n",
    "#                                 if dec1[i] > 0 :\n",
    "#                                     for j in range(dec1[i]) :\n",
    "#                                         b1 = np.random.choice(ranf, size = 1)[0]\n",
    "#                                         loc1 = np.random.choice(h - b1, size = 1)[0]\n",
    "#                                         Xn[i, loc1:(loc1 + b1 - 1), :] = 0\n",
    "                \n",
    "            X.append(Xn)\n",
    "\n",
    "                \n",
    "        if self.datagen:\n",
    "            for i in range(self.batch_size):\n",
    "                X[i] = self.datagen.random_transform(X[i])\n",
    "                X[i] = self.datagen.standardize(X[i])\n",
    "\n",
    "        if isinstance(self.y_train, list):\n",
    "            y = []\n",
    "\n",
    "            for y_train_ in self.y_train:\n",
    "                y1 = y_train_[batch_ids[:self.batch_size]].copy()\n",
    "                y2 = y_train_[batch_ids[self.batch_size:]].copy()\n",
    "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
    "        else:\n",
    "            y1 = self.y_train[batch_ids[:self.batch_size]].copy()\n",
    "            y2 = self.y_train[batch_ids[self.batch_size:]].copy()\n",
    "            y = y1 * y_l + y2 * (1 - y_l)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "def get_LCNN_o_1_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape, \n",
    "                       mel_mm_input_shape, use_s1s2 = True,use_mm = True,\n",
    "                       use_mel = True, use_cqt = True, use_stft = True, \n",
    "                       ord1 = True, dp = .5, fc = False, ext = False, ext2 = False):\n",
    "    # Create a towy model.\n",
    "    age = keras.Input(shape=(6,), name = 'age_cat')\n",
    "    sex = keras.Input(shape=(2,), name = 'sex_cat')\n",
    "    hw = keras.Input(shape=(2,), name = 'height_weight')\n",
    "    preg = keras.Input(shape=(1,), name = 'is_preg')\n",
    "    loc = keras.Input(shape=(5,), name = 'loc')\n",
    "    mel1 = keras.Input(shape=(mel_input_shape), name = 'mel')\n",
    "    cqt1 = keras.Input(shape=(cqt_input_shape), name = 'cqt')\n",
    "    stft1 = keras.Input(shape=(stft_input_shape), name = 'stft')\n",
    "    rr1 = keras.Input(shape=(1,), name = 'rr')\n",
    "\n",
    "    \n",
    "#     mel1_s1s2 = keras.Input(shape=(mel_s1s2_input_shape), name = 'mel_s1s2')\n",
    "    mel1_mm = keras.Input(shape=(mel_mm_input_shape), name = 'mel_mm')\n",
    "        \n",
    "    ## age embeddig\n",
    "    age1 = layers.Dense(2, activation = None)(age)\n",
    "\n",
    "    ## sex embedding\n",
    "    sex1 = layers.Dense(1, activation = None)(sex)\n",
    "\n",
    "    ## hw embedding\n",
    "    hw1 = layers.Dense(1, activation = None)(hw)\n",
    "\n",
    "    ## loc embedding\n",
    "    loc1 = layers.Dense(3, activation = None)(loc)\n",
    "\n",
    "\n",
    "    ## mel embedding\n",
    "    if use_mel :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        max3 = tf.keras.activations.swish(max3)\n",
    "\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "#         batch6 = layers.LeakyReLU()(batch6)\n",
    "\n",
    "\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        batch10 = tf.keras.activations.swish(batch10)\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(max3.shape[1],1), strides=(2,2))(max3)\n",
    "        u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(u)\n",
    "\n",
    "        # u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(max3)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "        batch10 = batch10 + u\n",
    "        ############################\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        batch13 = tf.keras.activations.swish(batch13)\n",
    "\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "        max16 = tf.keras.activations.swish(max16)\n",
    "\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "        batch19 = tf.keras.activations.swish(batch19)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch10.shape[1],1), strides=(2,2))(batch10)\n",
    "        u = Conv2D(filters=64, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=64, kernel_size=1,  padding='same', activation=None)(batch10)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "\n",
    "        batch19 = batch19 + u\n",
    "        ############################\n",
    "\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "#         batch22 = layers.LeakyReLU()(batch22)\n",
    "\n",
    "\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24) \n",
    "        batch25 = tf.keras.activations.swish(batch25)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch19.shape[1],1), strides=(1,1))(batch19)\n",
    "        u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(batch19)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')(u)\n",
    "\n",
    "        batch25 = batch25 + u\n",
    "        ############################\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        mha = layers.MultiHeadAttention(num_heads=8, key_dim=256)(mfm27,mfm27,mfm27)\n",
    "        mel2 = layers.GlobalAveragePooling2D()(mha)\n",
    "        \n",
    "   # mel1_s1s2\n",
    "    if use_s1s2 :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_s1s2)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_s1s2)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        max3 = tf.keras.activations.swish(max3)\n",
    "\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        batch10 = tf.keras.activations.swish(batch10)\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(max3.shape[1],1), strides=(2,2))(max3)\n",
    "        u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(u)\n",
    "\n",
    "        # u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(max3)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "        batch10 = batch10 + u\n",
    "        ############################\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        batch13 = tf.keras.activations.swish(batch13)\n",
    "\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "        max16 = tf.keras.activations.swish(max16)\n",
    "\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "        batch19 = tf.keras.activations.swish(batch19)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch10.shape[1],1), strides=(2,2))(batch10)\n",
    "        u = Conv2D(filters=64, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=64, kernel_size=1,  padding='same', activation=None)(batch10)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "\n",
    "        batch19 = batch19 + u\n",
    "        ############################\n",
    "\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24) \n",
    "        batch25 = tf.keras.activations.swish(batch25)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch19.shape[1],1), strides=(1,1))(batch19)\n",
    "        u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(batch19)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')(u)\n",
    "\n",
    "        batch25 = batch25 + u\n",
    "        ############################\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        mha = layers.MultiHeadAttention(num_heads=8, key_dim=256)(mfm27,mfm27,mfm27)\n",
    "        mel2_s1s2 = layers.GlobalAveragePooling2D()(mha)\n",
    "        \n",
    "       \n",
    "   # mel1_s1s2\n",
    "    if use_mm :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_mm)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_mm)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        max3 = tf.keras.activations.swish(max3)\n",
    "\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        batch10 = tf.keras.activations.swish(batch10)\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(max3.shape[1],1), strides=(2,2))(max3)\n",
    "        u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(u)\n",
    "\n",
    "        # u = Conv2D(filters=48, kernel_size=3, padding='same', activation=None)(max3)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "        batch10 = batch10 + u\n",
    "        ############################\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        batch13 = tf.keras.activations.swish(batch13)\n",
    "\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "        max16 = tf.keras.activations.swish(max16)\n",
    "\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "        batch19 = tf.keras.activations.swish(batch19)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch10.shape[1],1), strides=(2,2))(batch10)\n",
    "        u = Conv2D(filters=64, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=64, kernel_size=1,  padding='same', activation=None)(batch10)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(u)\n",
    "\n",
    "        batch19 = batch19 + u\n",
    "        ############################\n",
    "\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24) \n",
    "        batch25 = tf.keras.activations.swish(batch25)\n",
    "\n",
    "\n",
    "        ############################\n",
    "        u = AveragePooling2D(pool_size=(batch19.shape[1],1), strides=(1,1))(batch19)\n",
    "        u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(u)\n",
    "        \n",
    "        # u = Conv2D(filters=32, kernel_size=1, padding='same', activation=None)(batch19)\n",
    "        # u = MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')(u)\n",
    "\n",
    "        batch25 = batch25 + u\n",
    "        ############################\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        mha = layers.MultiHeadAttention(num_heads=8, key_dim=256)(mfm27,mfm27,mfm27)\n",
    "        mel2_mm = layers.GlobalAveragePooling2D()(mha)\n",
    "\n",
    "    if use_cqt :\n",
    "        ## cqt embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "        \n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        cqt2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            cqt2 = Dropout(dp)(cqt2)\n",
    "\n",
    "    if use_stft :\n",
    "        ## stft embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        \n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "        \n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        stft2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            stft2 = Dropout(dp)(stft2)\n",
    "    \n",
    "    \n",
    "    if use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2, stft2])\n",
    "    if not use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([cqt2, stft2])\n",
    "    if use_mel and not use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, stft2])\n",
    "    if use_mel and use_cqt and not use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2])\n",
    "    if not use_mel and not use_cqt and use_stft :  ## stft 만\n",
    "        concat2 = stft2\n",
    "    if use_mel and not use_cqt and not use_stft :  ### mel만\n",
    "        concat2 = mel2\n",
    "    if not use_mel and use_cqt and not use_stft :  ### cqt만\n",
    "        concat2 = cqt2\n",
    "\n",
    "    if ext :\n",
    "        concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg, rr1, mel2_mm])\n",
    "        d1 = layers.Dense(3, activation = 'relu')(concat1)\n",
    "        d1 = layers.Dense(2, activation = 'relu')(d1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1])\n",
    "\n",
    "    if ext2 :\n",
    "        concat1 = layers.Concatenate()([rr1,mel2_mm])\n",
    "        d1 = layers.Dense(3, activation = 'relu')(concat1)\n",
    "        d1 = layers.Dense(2, activation = 'relu')(d1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1])\n",
    "        \n",
    "    if fc :\n",
    "        concat2 = layers.Dense(10, activation = \"relu\")(concat2)\n",
    "        if dp :\n",
    "            concat2 = Dropout(dp)(concat2)\n",
    "        \n",
    "    if ord1 :\n",
    "        res1 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "    else :\n",
    "        res1 = layers.Dense(3, activation = \"softmax\")(concat2)\n",
    "\n",
    "        \n",
    "    res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "\n",
    "    model = keras.Model(inputs = [age,sex,hw,preg,loc,mel1,cqt1,stft1, rr1, mel1_mm] , outputs = res1 )\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy','AUC'])\n",
    "    return(model)\n",
    "\n",
    "\n",
    "def get_LCNN_2_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape, \n",
    "                    mel_mm_input_shape, use_s1s2 = True,use_mm = True,\n",
    "                     use_mel = True, use_cqt = True, use_stft = True, \n",
    "                    dp = False, fc = False, ext = False, ext2 = False):\n",
    "        # Create a towy model.\n",
    "    age = keras.Input(shape=(6,), name = 'age_cat')\n",
    "    sex = keras.Input(shape=(2,), name = 'sex_cat')\n",
    "    hw = keras.Input(shape=(2,), name = 'height_weight')\n",
    "    preg = keras.Input(shape=(1,), name = 'is_preg')\n",
    "    loc = keras.Input(shape=(5,), name = 'loc')\n",
    "    mel1 = keras.Input(shape=(mel_input_shape), name = 'mel')\n",
    "    cqt1 = keras.Input(shape=(cqt_input_shape), name = 'cqt')\n",
    "    stft1 = keras.Input(shape=(stft_input_shape), name = 'stft')\n",
    "    rr1 = keras.Input(shape=(1,), name = 'rr')\n",
    "    \n",
    "#     mel1_s1s2 = keras.Input(shape=(mel_s1s2_input_shape), name = 'mel_s1s2')\n",
    "    mel1_mm = keras.Input(shape=(mel_mm_input_shape), name = 'mel_mm')\n",
    "        \n",
    "        \n",
    "    ## age embeddig\n",
    "    age1 = layers.Dense(2, activation = None)(age)\n",
    "\n",
    "    ## sex embedding\n",
    "    sex1 = layers.Dense(1, activation = None)(sex)\n",
    "\n",
    "    ## hw embedding\n",
    "    hw1 = layers.Dense(1, activation = None)(hw)\n",
    "\n",
    "    ## loc embedding\n",
    "    loc1 = layers.Dense(3, activation = None)(loc)\n",
    "\n",
    "   \n",
    "\n",
    "   ## mel embedding\n",
    "    if use_mel :\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        \n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        mel2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            mel2 = Dropout(dp)(mel2)\n",
    "        \n",
    "    if use_s1s2:\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_s1s2)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_s1s2)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        \n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        mel2_s1s2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            mel2_s1s2 = Dropout(dp)(mel2_s1s2)\n",
    "            \n",
    "    if use_mm:\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_mm)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(mel1_mm)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "        \n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "        \n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "        \n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        mel2_mm = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            mel2_mm = Dropout(dp)(mel2_mm)\n",
    "\n",
    "    if use_cqt :\n",
    "        ## cqt embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(cqt1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "\n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "        \n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "        \n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "\n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        cqt2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            cqt2 = Dropout(dp)(cqt2)\n",
    "\n",
    "    if use_stft :\n",
    "        ## stft embedding\n",
    "        conv1_1 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        conv1_2 = Conv2D(filters = 32, kernel_size =5, strides=(1, 1), padding='same', activation=None)(stft1)\n",
    "        mfm2 = tensorflow.keras.layers.maximum([conv1_1, conv1_2])\n",
    "        max3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm2)\n",
    "        \n",
    "        conv4_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        conv4_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max3)\n",
    "        mfm5 = tensorflow.keras.layers.maximum([conv4_1, conv4_2])\n",
    "        batch6 = BatchNormalization(axis=3, scale=False)(mfm5)\n",
    "\n",
    "        conv7_1 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        conv7_2 = Conv2D(filters = 48, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch6)\n",
    "        mfm8 = tensorflow.keras.layers.maximum([conv7_1, conv7_2])\n",
    "\n",
    "        max9 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm8)\n",
    "        batch10 = BatchNormalization(axis=3, scale=False)(max9)\n",
    "\n",
    "        conv11_1 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        conv11_2 = Conv2D(filters = 48, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch10)\n",
    "        mfm12 = tensorflow.keras.layers.maximum([conv11_1, conv11_2])\n",
    "        batch13 = BatchNormalization(axis=3, scale=False)(mfm12)\n",
    "\n",
    "        conv14_1 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        conv14_2 = Conv2D(filters = 64, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch13)\n",
    "        mfm15 = tensorflow.keras.layers.maximum([conv14_1, conv14_2])\n",
    "\n",
    "        max16 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same')(mfm15)\n",
    "\n",
    "        conv17_1 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        conv17_2 = Conv2D(filters = 64, kernel_size =1, strides=(1, 1), padding='same', activation=None)(max16)\n",
    "        mfm18 = tensorflow.keras.layers.maximum([conv17_1, conv17_2])\n",
    "        batch19 = BatchNormalization(axis=3, scale=False)(mfm18)\n",
    "\n",
    "        conv20_1 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        conv20_2 = Conv2D(filters = 32, kernel_size =3, strides=(1, 1), padding='same', activation=None)(batch19)\n",
    "        mfm21 = tensorflow.keras.layers.maximum([conv20_1, conv20_2])\n",
    "        batch22 = BatchNormalization(axis=3, scale=False)(mfm21)\n",
    "\n",
    "        conv23_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        conv23_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch22)\n",
    "        mfm24 = tensorflow.keras.layers.maximum([conv23_1, conv23_2])\n",
    "        batch25 = BatchNormalization(axis=3, scale=False)(mfm24)\n",
    "        \n",
    "        conv26_1 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        conv26_2 = Conv2D(filters = 32, kernel_size =1, strides=(1, 1), padding='same', activation=None)(batch25)\n",
    "        mfm27 = tensorflow.keras.layers.maximum([conv26_1, conv26_2])\n",
    "\n",
    "        max28 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(mfm27)\n",
    "        stft2 = layers.GlobalAveragePooling2D()(max28)\n",
    "        if dp :\n",
    "            stft2 = Dropout(dp)(stft2)\n",
    "    \n",
    "#    concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg])\n",
    "#    d1 = layers.Dense(2, activation = 'relu')(concat1)\n",
    "    \n",
    "    if use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2, stft2])\n",
    "    if not use_mel and use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([cqt2, stft2])\n",
    "    if use_mel and not use_cqt and use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, stft2])\n",
    "    if use_mel and use_cqt and not use_stft :\n",
    "        concat2 = layers.Concatenate()([mel2, cqt2])\n",
    "    if not use_mel and not use_cqt and use_stft :  ## stft 만\n",
    "        concat2 = stft2\n",
    "    if use_mel and not use_cqt and not use_stft :  ### mel만\n",
    "        concat2 = mel2\n",
    "    if not use_mel and use_cqt and not use_stft :  ### cqt만\n",
    "        concat2 = cqt2\n",
    "\n",
    "    if ext :\n",
    "        concat1 = layers.Concatenate()([age1, sex1, hw1, loc1, preg, rr1,mel2_mm])\n",
    "        d1 = layers.Dense(3, activation = 'relu')(concat1)\n",
    "        d1 = layers.Dense(2, activation = 'relu')(d1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1])\n",
    "\n",
    "    if ext2 :\n",
    "        concat1 = layers.Concatenate()([rr1,mel2_mm])\n",
    "        d1 = layers.Dense(3, activation = 'relu')(concat1)\n",
    "        d1 = layers.Dense(2, activation = 'relu')(d1)\n",
    "        concat2 = layers.Concatenate()([concat2, d1])\n",
    "        \n",
    "    if fc :\n",
    "        concat2 = layers.Dense(10, activation = 'relu')(concat2)\n",
    "        if dp :\n",
    "            concat2 = Dropout(dp)(concat2)\n",
    "\n",
    "    res1 = layers.Dense(3, activation = \"softmax\")(concat2)\n",
    "    res2 = layers.Dense(2, activation = \"softmax\")(concat2)\n",
    "\n",
    "    model = keras.Model(inputs = [age,sex,hw,preg,loc,mel1,cqt1,stft1, rr1, mel1_mm] , outputs = res2 )\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['accuracy','AUC'])\n",
    "    return(model)\n",
    "\n",
    "\n",
    "def sigmoidal_decay(e, start=0, end=100, lr_start=1e-3, lr_end=1e-5):\n",
    "    if e < start:\n",
    "        return lr_start\n",
    "    elif e > end:\n",
    "        return lr_end\n",
    "\n",
    "    middle = (start + end) / 2\n",
    "    s = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return s(13 * (-e + middle) / np.abs(end - start)) * np.abs(lr_start - lr_end) + lr_end\n",
    "\n",
    "patient_files_trn = find_patient_files(train_folder)\n",
    "patient_files_test = find_patient_files(test_folder)\n",
    "\n",
    "# Run model.\n",
    "def run_model(model_folder, data_folder, output_folder, allow_failures, verbose):\n",
    "    # Load models.\n",
    "    if verbose >= 1:\n",
    "        print('Loading Challenge model...')\n",
    "\n",
    "    model = load_challenge_model(model_folder, verbose) ### Teams: Implement this function!!!\n",
    "\n",
    "    # Find the patient data files.\n",
    "    patient_files = find_patient_files(data_folder)\n",
    "    num_patient_files = len(patient_files)\n",
    "\n",
    "    if num_patient_files==0:\n",
    "        raise Exception('No data was provided.')\n",
    "\n",
    "    # Create a folder for the Challenge outputs if it does not already exist.\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Run the team's model on the Challenge data.\n",
    "    if verbose >= 1:\n",
    "        print('Running model on Challenge data...')\n",
    "\n",
    "#    @tf.function\n",
    "    # Iterate over the patient files.\n",
    "    for i in range(num_patient_files):\n",
    "        if verbose >= 2:\n",
    "            print('    {}/{}...'.format(i+1, num_patient_files))\n",
    "\n",
    "        patient_data = load_patient_data(patient_files[i])\n",
    "        recordings = load_recordings(data_folder, patient_data)\n",
    "\n",
    "        # Allow or disallow the model to fail on parts of the data; helpful for debugging.\n",
    "        try:\n",
    "            classes, labels, probabilities = run_challenge_model(model, patient_data, recordings, verbose) ### Teams: Implement this function!!!\n",
    "        except:\n",
    "            if allow_failures:\n",
    "                if verbose >= 2:\n",
    "                    print('... failed.')\n",
    "                classes, labels, probabilities = list(), list(), list()\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # Save Challenge outputs.\n",
    "        head, tail = os.path.split(patient_files[i])\n",
    "        root, extension = os.path.splitext(tail)\n",
    "        output_file = os.path.join(output_folder, root + '.csv')\n",
    "        patient_id = get_patient_id(patient_data)\n",
    "        save_challenge_outputs(output_file, patient_id, classes, labels, probabilities)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print('Done.')\n",
    "        \n",
    "import pickle as pk\n",
    "def save_challenge_model(model_folder, model1, model2, m_name1, m_name2, param_feature) :\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    filename1 = os.path.join(model_folder, m_name1 + '_model1.hdf5')\n",
    "    filename2 = os.path.join(model_folder, m_name2 + '_model2.hdf5')\n",
    "    model1.save(filename1)\n",
    "    model2.save(filename2)\n",
    "    param_feature['model1'] = m_name1\n",
    "    param_feature['model2'] = m_name2\n",
    "    param_feature['model_fnm1'] = filename1\n",
    "    param_feature['model_fnm2'] = filename2\n",
    "    with open(info_fnm, 'wb') as f:\n",
    "        pk.dump(param_feature, f, pk.HIGHEST_PROTOCOL)\n",
    "    return 1\n",
    "\n",
    "def load_challenge_model(model_folder, verbose):\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    with open(info_fnm, 'rb') as f:\n",
    "        info_m = pk.load(f)\n",
    "#    if info_m['model'] == 'toy' :\n",
    "#        model = get_toy(info_m['mel_shape'])\n",
    "#    filename = os.path.join(model_folder, info_m['model'] + '_model.hdf5')\n",
    "#    model.load_weights(filename)\n",
    "    return info_m\n",
    "\n",
    "\n",
    "\n",
    "# Run your trained model. This function is *required*. You should edit this function to add your code, but do *not* change the\n",
    "# arguments of this function.\n",
    "def run_challenge_model(model, data, recordings, verbose):\n",
    "\n",
    "    murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "    outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "    \n",
    "    if model['model1'] == 'lcnn1_dr_rr' :\n",
    "        model1 = get_LCNN_o_1_dr_rr(model['mel_shape'],model['cqt_shape'],model['stft_shape'], \n",
    "                                    model['mm_shape'],model['use_s1s2'], model['use_mm'],\n",
    "                                    use_mel = model['use_mel'],use_cqt = model['use_cqt'], use_stft = model['use_stft'],\n",
    "                                    ord1 = model['ord1'], \n",
    "                                    dp = model['dp'], fc = model['fc'], ext = False, ext2 = True)\n",
    "    if model['model2'] == 'lcnn2_dr_rr' :\n",
    "        model2 = get_LCNN_2_dr_rr(model['mel_shape'],model['cqt_shape'],model['stft_shape'], \n",
    "                                  model['mm_shape'],model['use_s1s2'], model['use_mm'],\n",
    "                                  use_mel = model['use_mel'],use_cqt = model['use_cqt'], use_stft = model['use_stft'], \n",
    "                                  dp = model['dp'], fc = model['fc'], ext = True, ext2 = False)\n",
    "    model1.load_weights(model['model_fnm1'])\n",
    "    model2.load_weights(model['model_fnm2'])\n",
    "\n",
    "#    classes = model['classes']\n",
    "    # Load features.\n",
    "    features = get_feature_one(data, verbose = 0)\n",
    "\n",
    "    samp_sec = model['samp_sec']\n",
    "    pre_emphasis = model['pre_emphasis']\n",
    "    hop_length = model['hop_length']\n",
    "    win_length = model['win_length']\n",
    "    n_mels = model['n_mels']\n",
    "    filter_scale = model['filter_scale']\n",
    "    n_bins = model['n_bins']\n",
    "    fmin = model['fmin']\n",
    "    use_mel = model['use_mel']\n",
    "    use_cqt = model['use_cqt']\n",
    "    use_stft = model['use_stft']\n",
    "    use_raw = model['use_raw']\n",
    "    trim = model['trim']\n",
    "    use_rr = model['use_rr']\n",
    "    use_mm = model['use_mm']\n",
    "    use_s1s2 = model['use_s1s2']\n",
    "    use_b_detect = True\n",
    "\n",
    "    features['mel1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_mel :\n",
    "            mel1 = feature_extract_melspec(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length,\n",
    "                                           win_length = win_length, n_mels = n_mels, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1) )\n",
    "        features['mel1'].append(mel1)\n",
    "    M, N = features['mel1'][0].shape\n",
    "\n",
    "    if use_mel :\n",
    "        for i in range(len(features['mel1'])) :\n",
    "            features['mel1'][i] = features['mel1'][i].reshape(M,N,1)\n",
    "    features['mel1'] = np.array(features['mel1'])\n",
    "\n",
    "    features['cqt1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_cqt :\n",
    "            mel1 = feature_extract_cqt(recordings[i], samp_sec=samp_sec, pre_emphasis = pre_emphasis, filter_scale = filter_scale,\n",
    "                                        n_bins = n_bins, fmin = fmin, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1))\n",
    "        features['cqt1'].append(mel1)\n",
    "    M, N = features['cqt1'][0].shape\n",
    "    if use_cqt :\n",
    "        for i in range(len(features['cqt1'])) :\n",
    "            features['cqt1'][i] = features['cqt1'][i].reshape(M,N,1)\n",
    "    features['cqt1'] = np.array(features['cqt1'])\n",
    "\n",
    "    features['stft1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_stft :\n",
    "            mel1 = feature_extract_stft(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length,\n",
    "                                        win_length = win_length, trim = trim)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1) )\n",
    "        features['stft1'].append(mel1)\n",
    "    M, N = features['stft1'][0].shape\n",
    "    if use_stft :\n",
    "        for i in range(len(features['stft1'])) :\n",
    "            features['stft1'][i] = features['stft1'][i].reshape(M,N,1)\n",
    "    features['stft1'] = np.array(features['stft1'])\n",
    "\n",
    "    features['raw1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_raw:\n",
    "            recording1 = recordings[i]\n",
    "            if len(recording1) >= maxlen : \n",
    "                recording1 = recording1[:maxlen]\n",
    "            else :\n",
    "                recording1 = np.pad(recording1, (0, maxlen - len(recording1) ), constant_values=(0,0) )\n",
    "        else :\n",
    "            recording1 = np.zeros((1))\n",
    "        features['raw1'].append(recording1)\n",
    "    features['raw1'] = np.array(features['raw1'])\n",
    "    \n",
    "    \n",
    "    features['rr1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_rr :\n",
    "            try:\n",
    "                recording1 = recordings[i]\n",
    "                ____, info = nk.ecg_process(recording1, sampling_rate=4000)\n",
    "                current_rr = np.mean(np.diff(info['ECG_R_Peaks'])/4000)\n",
    "            except:\n",
    "#                print(filename)\n",
    "                current_rr= 0.6414\n",
    "        else :\n",
    "            current_rr = 0\n",
    "        features['rr1'].append(current_rr)\n",
    "    features['rr1'] = np.array(features['rr1'])\n",
    "    \n",
    "    \n",
    "    features['s1s2_detect1'] = []\n",
    "    features['mm_detect1'] = []\n",
    "    features['s1s2_mel'] = []\n",
    "    features['mm_mel'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_b_detect :\n",
    "            recording1 = recordings[i]\n",
    "         # 1. Amplitude normalization\n",
    "            normal_sig = recording1/np.max(np.abs(recording1))\n",
    "\n",
    "            # 2. Filtering\n",
    "            T = len(recording1)/4000 #time interval; Sample Period ;\n",
    "            fs = 4000 #sample rate \n",
    "            cutoff = 150 #sample frequency \n",
    "\n",
    "            nyq = 0.5 * fs \n",
    "            order = 2  # sin wave can be approx represented as quadratic\n",
    "            n = int(T*fs) #total number of samples \n",
    "\n",
    "            low_ft = butter_lowpass_filter(normal_sig, cutoff, fs, order)\n",
    "\n",
    "            # 3. smoothing of signal envelope\n",
    "            duration = 1.0\n",
    "#                 fs = 4000.0\n",
    "            samples = int(fs*duration)\n",
    "            t = np.arange(len(low_ft)) / fs\n",
    "\n",
    "            analytic_signal = hilbert(low_ft)\n",
    "            amplitude_envelope = np.abs(analytic_signal)\n",
    "\n",
    "            # threshld selection\n",
    "            mu = np.sum(amplitude_envelope)/len(amplitude_envelope)\n",
    "            var = np.sum((amplitude_envelope-mu)**2)/len(amplitude_envelope)\n",
    "            t_sh = mu + var +0.05\n",
    "\n",
    "            thres_list = np.argwhere(amplitude_envelope > t_sh)\n",
    "            save = []\n",
    "            for i in thres_list:\n",
    "                j = i[0]\n",
    "                save.append(j)\n",
    "\n",
    "            packet = []\n",
    "            tmp = []\n",
    "            v = save.pop(0)\n",
    "            tmp.append(v)\n",
    "\n",
    "            while(len(save)>0):\n",
    "                vv = save.pop(0)\n",
    "                if v+1 == vv:\n",
    "                    tmp.append(vv)\n",
    "                    v = vv\n",
    "                else:\n",
    "                    packet.append(tmp)\n",
    "                    tmp = []\n",
    "                    tmp.append(vv)\n",
    "                    v = vv\n",
    "\n",
    "            packet.append(tmp)\n",
    "#                 # thresh hold 보다 크지만 연속값이 3개 미만인 리스트 찾아서 삭제\n",
    "#                 packet2 = []\n",
    "#                 for i in range(len(packet)):\n",
    "#                     if len(packet[i]) < 3:\n",
    "#                         j = packet.index(packet[i])\n",
    "#                         packet2.append(j)\n",
    "\n",
    "#                 for i in packet2:\n",
    "#                     del packet[i]\n",
    "\n",
    "            min_list = []\n",
    "            max_list = []\n",
    "            for i in range(len(packet)):\n",
    "                min_find = min(packet[i])\n",
    "                max_find = max(packet[i])\n",
    "                min_list.append(min_find)\n",
    "                max_list.append(max_find)\n",
    "\n",
    "            # s1, s2 boundary가 여러개 그려짐. 추가적인 전처리 필요. 60개보다 커야함\n",
    "            packet_cp = packet.copy()\n",
    "            new_list = []\n",
    "            first_list = []\n",
    "            last_list = []\n",
    "            for i in range(len(max_list)-1):\n",
    "                j = i + 1\n",
    "                # 연속적인 값인 경우 삭제\n",
    "                if abs(max_list[i]-min_list[j]) < 60:\n",
    "                    first = max_list.index(max_list[i])\n",
    "                    last = min_list.index(min_list[j])\n",
    "                    re_join = packet_cp[first]+packet_cp[last]\n",
    "                    new_list.append(re_join)\n",
    "                    first_list.append(first)\n",
    "                    last_list.append(last)\n",
    "\n",
    "            # 연속적인 값 인덱스 찾아서 final_list 만들기\n",
    "            final_list = first_list+last_list\n",
    "            final_list.sort()\n",
    "            set(final_list)\n",
    "\n",
    "            # 위에서 찾은 final_linst에 들어있는 인덱스 위치는 0으로 처리\n",
    "            drop_list = packet_cp.copy()\n",
    "            for i in final_list:\n",
    "                drop_list[i] = 0\n",
    "#             print(len(drop_list))\n",
    "\n",
    "            seq_remake = drop_list+new_list\n",
    "#             print(len(seq_remake))\n",
    "\n",
    "\n",
    "\n",
    "            # 0으로 전처리한 값 삭제\n",
    "            remove_set = [0]\n",
    "\n",
    "            li = [i for i in seq_remake if i not in remove_set]\n",
    "#             print(li)\n",
    "\n",
    "            # 추가 전처리 후 다시 min, max 출력\n",
    "            min_list1 = []\n",
    "            max_list1 = []\n",
    "            for i in range(len(li)):\n",
    "                min_find1 = min(li[i])\n",
    "                max_find1 = max(li[i])\n",
    "                min_list1.append(min_find1)\n",
    "                max_list1.append(max_find1)\n",
    "\n",
    "            min_list1.sort()\n",
    "            max_list1.sort()\n",
    "\n",
    "\n",
    "            # boundary detected s1 and s2\n",
    "            s_detect = amplitude_envelope.copy()\n",
    "            for i in range(len(max_list1)-1):\n",
    "                j = i+1\n",
    "                s_detect[max_list1[i]:min_list1[j]] = 0\n",
    "            s1s2_detect = s_detect.reshape(1, s_detect.shape[0])\n",
    "            s1s2_detect = s1s2_detect.tolist()\n",
    "            s1s2_detect = pad_sequences(s1s2_detect, maxlen = 80000, dtype ='float64', padding = 'post', truncating ='post', value=0.0)\n",
    "\n",
    "            # s1s2_detect 변수에 mel 적용\n",
    "            s1s2_mel = feature_extract_bound_melspec(s_detect)[0]\n",
    "\n",
    "            # boundary detected systolic and diastolic murmurs present in pcg signal\n",
    "            mm_detect = amplitude_envelope.copy()\n",
    "            for i in range(len(max_list1)):\n",
    "                mm_detect[min_list1[i]:max_list1[i]+1] = 0\n",
    "            murmur_detect = mm_detect.reshape(1, mm_detect.shape[0])\n",
    "            murmur_detect = murmur_detect.tolist()\n",
    "            murmur_detect = pad_sequences(murmur_detect, maxlen = 80000, dtype ='float64', padding = 'post', truncating ='post', value=0.0)\n",
    "\n",
    "            # murmur_dect 변수에 mel 적용\n",
    "            mm_mel = feature_extract_bound_melspec(mm_detect)[0]\n",
    "\n",
    "        else :\n",
    "            s1s2_detect = np.zeros((1,1))\n",
    "            murmur_detect = np.zeros((1,1))\n",
    "            s1s2_mel = np.zeros( (1,1,1) )\n",
    "            mm_mel = np.zeros( (1,1,1) )\n",
    "\n",
    "        features['s1s2_detect1'].append(s1s2_detect)\n",
    "        features['mm_detect1'].append(murmur_detect)\n",
    "        features['s1s2_mel'].append(s1s2_mel)\n",
    "        features['mm_mel'].append(mm_mel)\n",
    "\n",
    "    features['s1s2_detect1'] = np.array(features['s1s2_detect1'])\n",
    "    features['mm_detect1'] = np.array(features['mm_detect1'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    M, N = features['s1s2_mel'][0].shape\n",
    "    if use_s1s2:\n",
    "        for i in range(len(features['s1s2_mel'])):\n",
    "            features['s1s2_mel'][i] = features['s1s2_mel'][i].reshape(M,N,1)\n",
    "    features['s1s2_mel'] = np.array(features['s1s2_mel'])\n",
    "    \n",
    "    \n",
    "    M, N = features['mm_mel'][0].shape\n",
    "    if use_mm:\n",
    "        for i in range(len(features['mm_mel'])):\n",
    "            features['mm_mel'][i] = features['mm_mel'][i].reshape(M,N,1)\n",
    "    features['mm_mel'] = np.array(features['mm_mel'])\n",
    "\n",
    "\n",
    "    # Impute missing data.\n",
    "    res1 = model1.predict([features['age'], features['sex'], features['hw'], features['preg'], features['loc'], \n",
    "                           features['mel1'],features['cqt1'], features['stft1'], features['rr1'], \n",
    "                           features['mm_mel']])\n",
    "    res2 = model2.predict([features['age'], features['sex'], features['hw'], features['preg'], features['loc'], \n",
    "                           features['mel1'], features['cqt1'], features['stft1'], features['rr1'], \n",
    "                           features['mm_mel']])\n",
    "\n",
    "    # Get classifier probabilities.\n",
    "    if model['ord1'] :\n",
    "        idx1 = res1.argmax(axis=0)[0]\n",
    "        murmur_p = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "        murmur_probabilities = np.zeros((3,))\n",
    "        murmur_probabilities[0] = murmur_p[0]\n",
    "        murmur_probabilities[1] = 0\n",
    "        murmur_probabilities[2] = murmur_p[1]\n",
    "        outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "    else :\n",
    "        if model['mm_mean'] :\n",
    "            murmur_probabilities = res1.mean(axis = 0)\n",
    "        else :\n",
    "            idx1 = res1.argmax(axis=0)[0]\n",
    "            murmur_probabilities = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "        outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "\n",
    "\n",
    "\n",
    "    ## 이부분도 생각 필요.. rule 을 cost를 maximize 하는 기준으로 threshold 탐색 필요할지도..\n",
    "    # Choose label with highest probability.\n",
    "    murmur_labels = np.zeros(len(murmur_classes), dtype=np.int_)\n",
    "    if murmur_probabilities[0] > 0.496 :\n",
    "        idx = 0\n",
    "    else :\n",
    "        idx = 2\n",
    "#    idx = np.argmax(murmur_probabilities)\n",
    "    murmur_labels[idx] = 1\n",
    "\n",
    "    outcome_labels = np.zeros(len(outcome_classes), dtype=np.int_)\n",
    "    if outcome_probabilities[0] > 0.617 :\n",
    "        idx = 0\n",
    "    else :\n",
    "        idx = 1\n",
    "#    idx = np.argmax(outcome_probabilities)\n",
    "    outcome_labels[idx] = 1\n",
    "\n",
    "    # Concatenate classes, labels, and probabilities.\n",
    "    classes = murmur_classes + outcome_classes\n",
    "    labels = np.concatenate((murmur_labels, outcome_labels))\n",
    "    probabilities = np.concatenate((murmur_probabilities, outcome_probabilities))\n",
    "\n",
    "    return classes, labels, probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_folder = 'hyper_1_3'\n",
    "output_folder = '/Data/hmd/hmd_sy/2021_hmd/tmp/out_hyper_1_3'\n",
    "\n",
    "# maxlen = np.random.choice([120000,80000, 50000, 15000])\n",
    "winlen = 512\n",
    "hoplen = 256\n",
    "nmel = 140 #np.random.choice([100, 120, 140])\n",
    "nsec = 50\n",
    "trim = 0 #np.random.choice([0,2000, 4000])\n",
    "use_mel = True\n",
    "use_cqt = False #np.random.choice([True,False])\n",
    "use_stft = False#np.random.choice([True, False])\n",
    "use_rr = True\n",
    "# use_rr_seq = False #True\n",
    "use_raw = False #True\n",
    "\n",
    "use_b_detect = True\n",
    "use_s1s2 = False\n",
    "use_mm = True\n",
    "\n",
    "#################\n",
    "# envelope parameter\n",
    "#################\n",
    "samp_sec = 50\n",
    "sample_rate = 4000\n",
    "pre_emphasis  = 0\n",
    "sr = 4000\n",
    "n_mels = 140\n",
    "\n",
    "# maxlen = 120000\n",
    "win_length = 512\n",
    "hop_length = 256\n",
    "\n",
    "fs = 4000 #sample rate \n",
    "cutoff = 150 #sample frequency \n",
    "nyq = 0.5 * fs \n",
    "\n",
    "\n",
    "\n",
    "params_feature = {'samp_sec': nsec,\n",
    "            #### melspec, stft 피쳐 옵션들  \n",
    "            'pre_emphasis': 0,\n",
    "            'hop_length': hoplen,\n",
    "            'win_length':winlen,\n",
    "            'n_mels': nmel,\n",
    "            #### cqt 피쳐 옵션들  \n",
    "            'filter_scale': 1,\n",
    "            'n_bins': 80,\n",
    "            'fmin': 10,\n",
    "\n",
    "            ### 사용할 피쳐 지정\n",
    "                'trim' : trim, # 앞뒤 얼마나 자를지? 4000 이면 1초\n",
    "                'use_rr' : use_rr,\n",
    "                'use_b_detect': use_b_detect,\n",
    "                'use_raw' : use_raw,\n",
    "                'use_mel' : use_mel,\n",
    "                'use_cqt' : use_cqt,\n",
    "                'use_stft' : use_stft          \n",
    "}\n",
    "\n",
    "\n",
    "mm_weight = 3 #np.random.choice([2,3,4,5])\n",
    "oo_weight = 3 #np.random.choice([2,3,4,5,6])\n",
    "ord1 = True #np.random.choice([True,False])\n",
    "mm_mean = False #np.random.choice([True,False])\n",
    "dp = 0 #np.random.choice([0, .1, .2, .3])\n",
    "fc = False #np.random.choice([True,False])\n",
    "\n",
    "\n",
    "ext = True\n",
    "\n",
    "\n",
    "chaug = 10 #np.random.choice([0, 10])\n",
    "mixup = True #np.random.choice([True,False])\n",
    "cout = .8 #np.random.choice([0, 0.8])\n",
    "wunknown = 1 #np.random.choice([1, 0.7, .5, .2])\n",
    "n1 = 0 #np.random.choice([0,2])\n",
    "if n1 == 0 :\n",
    "    ranfil = False\n",
    "else :\n",
    "    ranfil = [n1, [18,19,20,21,22,23]]\n",
    "    \n",
    "use_mel = params_feature['use_mel']\n",
    "use_cqt = params_feature['use_cqt']\n",
    "use_stft = params_feature['use_stft']\n",
    "nep = 100\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/features_trn_envel.pkl','rb') as f:\n",
    "    features_trn = pickle.load(f)\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/mm_lbs_trn_envel.pkl','rb') as f:\n",
    "    mm_lbs_trn = pickle.load(f)\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/out_lbs_trn_envel.pkl','rb') as f:\n",
    "    out_lbs_trn = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/features_test_envel.pkl','rb') as f:\n",
    "    features_test = pickle.load(f)\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/mm_lbs_test_envel.pkl','rb') as f:\n",
    "    mm_lbs_test = pickle.load(f)\n",
    "\n",
    "with open('/Data/hmd/hmd_sy/2021_hmd/out_lbs_test_envel.pkl','rb') as f:\n",
    "    out_lbs_test = pickle.load(f)\n",
    "    \n",
    "# (2532, 140, 782) 에서 (2532, 140, 782, 1)로 변경\n",
    "a, b, c = features_trn['mel1'].shape\n",
    "features_trn['mel1']= features_trn['mel1'].reshape(a,b,c,1)\n",
    "\n",
    "a, b, c = features_trn['s1s2_mel'].shape\n",
    "features_trn['s1s2_mel'] = features_trn['s1s2_mel'].reshape(a,b,c,1)\n",
    "\n",
    "a, b, c = features_trn['mm_mel'].shape\n",
    "features_trn['mm_mel'] = features_trn['mm_mel'].reshape(a,b,c,1)\n",
    "\n",
    "mel_input_shape = features_trn['mel1'][0].shape\n",
    "cqt_input_shape = features_trn['cqt1'][0].shape\n",
    "stft_input_shape = features_trn['stft1'][0].shape\n",
    "\n",
    "mel_s1s2_input_shape = features_trn['s1s2_mel'][0].shape\n",
    "mel_mm_input_shape = features_trn['mm_mel'][0].shape\n",
    "\n",
    "\n",
    "params_feature['ord1'] = ord1\n",
    "params_feature['mm_mean'] = mm_mean\n",
    "params_feature['dp'] = dp\n",
    "params_feature['fc'] = fc\n",
    "params_feature['ext'] = ext\n",
    "params_feature['oo_weight'] = oo_weight\n",
    "params_feature['mm_weight'] = mm_weight\n",
    "params_feature['chaug'] = chaug\n",
    "params_feature['cout'] = cout\n",
    "params_feature['wunknown'] = wunknown\n",
    "params_feature['mixup'] = mixup\n",
    "params_feature['n1'] = n1\n",
    "\n",
    "params_feature['mel_shape'] = mel_input_shape\n",
    "params_feature['cqt_shape'] = cqt_input_shape\n",
    "params_feature['stft_shape'] = stft_input_shape\n",
    "\n",
    "params_feature['s1s2_shape'] = mel_s1s2_input_shape\n",
    "params_feature['mm_shape'] = mel_mm_input_shape\n",
    "\n",
    "params_feature['use_mel'] = use_mel\n",
    "params_feature['use_cqt'] = use_cqt\n",
    "params_feature['use_stft'] = use_stft\n",
    "\n",
    "params_feature['use_rr'] = use_rr\n",
    "params_feature['use_s1s2'] = use_s1s2\n",
    "params_feature['use_mm'] = use_mm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(params_feature)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "model1 = get_LCNN_o_1_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape, \n",
    "                            mel_mm_input_shape, use_s1s2 = use_s1s2, use_mm = use_mm,\n",
    "                            use_mel = use_mel, use_cqt = use_cqt, use_stft = use_stft, ord1 = ord1, dp = dp, fc = fc, ext = False, ext2 = True)\n",
    "model2 = get_LCNN_2_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape, \n",
    "                        mel_mm_input_shape, use_s1s2 = use_s1s2, use_mm = use_mm,\n",
    "                        use_mel = use_mel, use_cqt = use_cqt, use_stft = use_stft, dp = dp, fc = fc, ext = True, ext2 = False)\n",
    "\n",
    "n_epoch = nep\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "batch_size = 64\n",
    "\n",
    "if mixup :\n",
    "    beta_param = .7\n",
    "else :\n",
    "    beta_param = 0\n",
    "\n",
    "params = {'batch_size': batch_size,\n",
    "        #          'input_shape': (100, 313, 1),\n",
    "        'shuffle': True,\n",
    "        'chaug': chaug,\n",
    "        'beta_param': beta_param,\n",
    "        'cout': cout\n",
    "#              'mixup': mixup,\n",
    "        #          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "        #          'highpass': [.5, [78,79,80,81,82,83,84,85]]\n",
    "#              'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "        #           'dropblock' : [30, 100]\n",
    "        #'device' : device\n",
    "}\n",
    "\n",
    "if mixup :\n",
    "    params['mixup'] = mixup\n",
    "    params['ranfilter2'] = ranfil\n",
    "else :\n",
    "    params['cutout'] = cout\n",
    "\n",
    "params_no_shuffle = {'batch_size': batch_size,\n",
    "                    #          'input_shape': (100, 313, 1),\n",
    "                    'shuffle': False,\n",
    "                    'beta_param': 0.7,\n",
    "                    'mixup': False\n",
    "                    #'device': device\n",
    "}\n",
    "\n",
    "if ord1 :\n",
    "    class_weight = {0: mm_weight, 1: 1.}\n",
    "else :\n",
    "    class_weight = {0: mm_weight, 1: wunknown, 2:1.}\n",
    "\n",
    "\n",
    "if mixup :\n",
    "        TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                                features_trn['preg'], features_trn['loc'], features_trn['mel1'],\n",
    "                                features_trn['cqt1'],features_trn['stft1'],features_trn['rr1'], \n",
    "                                features_trn['mm_mel']], \n",
    "                        mm_lbs_trn,  ## our Y\n",
    "                            **params)()\n",
    "        model1.fit(TrainDGen_1,\n",
    "            validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                                features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                                features_test['cqt1'], features_test['stft1'],features_test['rr1'],\n",
    "                                features_test['mm_mel']], \n",
    "                                mm_lbs_test), \n",
    "            callbacks=[lr],\n",
    "            steps_per_epoch=np.ceil(len(mm_lbs_trn)/64),\n",
    "            class_weight=class_weight, \n",
    "            epochs = n_epoch)\n",
    "\n",
    "else :\n",
    "    TrainGen = DataGenerator([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                            features_trn['preg'], features_trn['loc'], features_trn['mel1'],\n",
    "                            features_trn['cqt1'],features_trn['stft1'],features_trn['rr1'], \n",
    "                            features_trn['mm_mel']], \n",
    "                mm_lbs_trn,  ## our Y\n",
    "                **params)\n",
    "    model1.fit(TrainGen,\n",
    "        validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                            features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                            features_test['cqt1'], features_test['stft1'],features_test['rr1'], \n",
    "                            features_test['mm_mel']], \n",
    "                            mm_lbs_test), \n",
    "        callbacks=[lr],\n",
    "        #        steps_per_epoch=np.ceil(len(mm_lbs_trn)/64),\n",
    "        class_weight=class_weight, \n",
    "        epochs = n_epoch)\n",
    "    \n",
    "n_epoch = nep\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "batch_size = 64\n",
    "params = {'batch_size': batch_size,\n",
    "        #          'input_shape': (100, 313, 1),\n",
    "        'shuffle': True,\n",
    "        'chaug': chaug,\n",
    "        'beta_param': beta_param,\n",
    "        'cout': cout,\n",
    "#              'mixup': True,\n",
    "        #          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "#            'highpass': [.5, [78,79,80,81,82,83,84,85]],\n",
    "#              'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "        #           'dropblock' : [30, 100]\n",
    "        #'device' : device\n",
    "}\n",
    "\n",
    "\n",
    "if mixup :\n",
    "    params['mixup'] = mixup\n",
    "    params['ranfilter2'] = ranfil\n",
    "else :\n",
    "    params['cutout'] = cout\n",
    "\n",
    "\n",
    "params_no_shuffle = {'batch_size': batch_size,\n",
    "                    #          'input_shape': (100, 313, 1),\n",
    "                    'shuffle': False,\n",
    "                    'beta_param': 0.7,\n",
    "                    'mixup': False\n",
    "                    #'device': device\n",
    "}\n",
    "\n",
    "class_weight = {0: oo_weight, 1: 1.}\n",
    "\n",
    "\n",
    "\n",
    "if mixup :\n",
    "    TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                                features_trn['preg'], features_trn['loc'], features_trn['mel1'],\n",
    "                                features_trn['cqt1'],features_trn['stft1'],features_trn['rr1'], \n",
    "                                features_trn['mm_mel']], \n",
    "                                out_lbs_trn,  ## our Y\n",
    "                    **params)()\n",
    "\n",
    "    model2.fit(TrainDGen_1,\n",
    "    validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                                features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                                features_test['cqt1'], features_test['stft1'],features_test['rr1'],\n",
    "                                features_test['mm_mel']], \n",
    "                                out_lbs_test), \n",
    "        callbacks=[lr],\n",
    "        steps_per_epoch=np.ceil(len(out_lbs_trn)/64),\n",
    "        class_weight=class_weight, \n",
    "        epochs = n_epoch)\n",
    "else :\n",
    "    TrainGen = DataGenerator([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                            features_trn['preg'], features_trn['loc'], features_trn['mel1'],\n",
    "                            features_trn['cqt1'],features_trn['stft1'],features_trn['rr1'], \n",
    "                            features_trn['mm_mel']], \n",
    "                            out_lbs_trn,  ## our Y\n",
    "                **params)\n",
    "    model2.fit(TrainGen,\n",
    "        validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                            features_test['preg'], features_test['loc'], features_test['mel1'], \n",
    "                            features_test['cqt1'], features_test['stft1'],features_test['rr1'],\n",
    "                            features_test['mm_mel']], \n",
    "                            out_lbs_test), \n",
    "        callbacks=[lr],\n",
    "        class_weight=class_weight, \n",
    "        epochs = n_epoch)\n",
    "    \n",
    "\n",
    "\n",
    "# params_feature['mel_shape'] = mel_input_shape\n",
    "# params_feature['cqt_shape'] = cqt_input_shape\n",
    "# params_feature['stft_shape'] = stft_input_shape\n",
    "\n",
    "# params_feature['use_mel'] = use_mel\n",
    "# params_feature['use_cqt'] = use_cqt\n",
    "# params_feature['use_stft'] = use_stft\n",
    "\n",
    "save_challenge_model(model_folder, model1, model2, m_name1 = 'lcnn1_dr_rr', m_name2 = 'lcnn2_dr_rr', param_feature = params_feature)\n",
    "\n",
    "run_model(model_folder, test_folder, output_folder, allow_failures = True, verbose = 1)\n",
    "\n",
    "murmur_scores, outcome_scores = evaluate_model(test_folder, output_folder)\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "','.join(classes),\n",
    "','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "params_feature['mm_weighted_accuracy'] = weighted_accuracy\n",
    "\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "','.join(classes),\n",
    "','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "+ '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "\n",
    "params_feature['out_cost'] = cost\n",
    "\n",
    "label_folder = test_folder\n",
    "murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "# Load and parse label and model output files.\n",
    "label_files, output_files = find_challenge_files(label_folder, output_folder)\n",
    "murmur_labels = load_murmurs(label_files, murmur_classes)\n",
    "murmur_binary_outputs, murmur_scalar_outputs = load_classifier_outputs(output_files, murmur_classes)\n",
    "outcome_labels = load_outcomes(label_files, outcome_classes)\n",
    "outcome_binary_outputs, outcome_scalar_outputs = load_classifier_outputs(output_files, outcome_classes)\n",
    "\n",
    "\n",
    "print(np.mean(murmur_scalar_outputs[:,0]))\n",
    "print(np.mean(murmur_scalar_outputs[:,2]))\n",
    "print(np.mean(outcome_scalar_outputs[:,0]))\n",
    "print(np.mean(outcome_scalar_outputs[:,1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "049a3d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold:  0.01\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.776,0.629,0.111,0.199,0.512,15140.890\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.699,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.965,0.500,0.864\n",
      "AUPRC,0.882,0.073,0.932\n",
      "F-measure,0.332,0.000,0.000\n",
      "Accuracy,1.000,0.000,0.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.745,0.654\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.05\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.776,0.629,0.111,0.199,0.512,15140.890\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.699,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.965,0.500,0.864\n",
      "AUPRC,0.882,0.073,0.932\n",
      "F-measure,0.332,0.000,0.000\n",
      "Accuracy,1.000,0.000,0.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.745,0.654\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.1\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.776,0.629,0.111,0.199,0.512,15140.890\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.699,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.965,0.500,0.864\n",
      "AUPRC,0.882,0.073,0.932\n",
      "F-measure,0.332,0.000,0.000\n",
      "Accuracy,1.000,0.000,0.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.745,0.654\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.15\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.776,0.629,0.121,0.209,0.518,15125.621\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.699,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.965,0.500,0.864\n",
      "AUPRC,0.882,0.073,0.932\n",
      "F-measure,0.335,0.000,0.028\n",
      "Accuracy,1.000,0.000,0.014\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.745,0.654\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.2\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.776,0.629,0.141,0.230,0.528,14509.033\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.699,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.965,0.500,0.864\n",
      "AUPRC,0.882,0.073,0.932\n",
      "F-measure,0.341,0.000,0.083\n",
      "Accuracy,1.000,0.000,0.043\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.745,0.654\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.25\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.776,0.629,0.205,0.304,0.566,13286.399\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.699,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.965,0.500,0.864\n",
      "AUPRC,0.882,0.073,0.932\n",
      "F-measure,0.364,0.000,0.252\n",
      "Accuracy,1.000,0.000,0.144\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.745,0.654\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.3\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.776,0.629,0.331,0.482,0.658,11523.887\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.699,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.965,0.500,0.864\n",
      "AUPRC,0.882,0.073,0.932\n",
      "F-measure,0.442,0.000,0.551\n",
      "Accuracy,1.000,0.000,0.388\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.745,0.654\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.35\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.776,0.629,0.431,0.644,0.741,13448.877\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.699,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.965,0.500,0.864\n",
      "AUPRC,0.882,0.073,0.932\n",
      "F-measure,0.559,0.000,0.733\n",
      "Accuracy,1.000,0.000,0.612\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.745,0.654\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.4\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.776,0.629,0.515,0.775,0.809,15770.716\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.699,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.965,0.500,0.864\n",
      "AUPRC,0.882,0.073,0.932\n",
      "F-measure,0.691,0.000,0.853\n",
      "Accuracy,1.000,0.000,0.791\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.745,0.654\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.45\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.776,0.629,0.543,0.817,0.798,17574.098\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.699,0.339,0.513,0.840,15140.890\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.965,0.500,0.864\n",
      "AUPRC,0.882,0.073,0.932\n",
      "F-measure,0.745,0.000,0.883\n",
      "Accuracy,0.921,0.000,0.871\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.745,0.654\n",
      "F-measure,0.678,0.000\n",
      "Accuracy,1.000,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.5\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.776,0.629,0.564,0.843,0.801,18186.507\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.699,0.337,0.508,0.832,15131.464\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.965,0.500,0.864\n",
      "AUPRC,0.882,0.073,0.932\n",
      "F-measure,0.791,0.000,0.901\n",
      "Accuracy,0.895,0.000,0.914\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.745,0.654\n",
      "F-measure,0.674,0.000\n",
      "Accuracy,0.990,0.000\n",
      "\n",
      "-------------\n",
      "threshold:  0.55\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.776,0.629,0.553,0.838,0.744,19435.408\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.699,0.448,0.555,0.834,13017.439\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.965,0.500,0.864\n",
      "AUPRC,0.882,0.073,0.932\n",
      "F-measure,0.763,0.000,0.897\n",
      "Accuracy,0.763,0.000,0.942\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.745,0.654\n",
      "F-measure,0.691,0.206\n",
      "Accuracy,0.969,0.118\n",
      "\n",
      "-------------\n",
      "threshold:  0.6\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.776,0.629,0.540,0.832,0.698,20480.865\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.699,0.553,0.597,0.792,11336.284\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.965,0.500,0.864\n",
      "AUPRC,0.882,0.073,0.932\n",
      "F-measure,0.725,0.000,0.896\n",
      "Accuracy,0.658,0.000,0.964\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.745,0.654\n",
      "F-measure,0.693,0.412\n",
      "Accuracy,0.888,0.290\n",
      "\n",
      "-------------\n",
      "threshold:  0.65\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.776,0.629,0.538,0.832,0.677,21317.630\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.699,0.611,0.613,0.647,12839.628\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.965,0.500,0.864\n",
      "AUPRC,0.882,0.073,0.932\n",
      "F-measure,0.719,0.000,0.895\n",
      "Accuracy,0.605,0.000,0.978\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.745,0.654\n",
      "F-measure,0.637,0.584\n",
      "Accuracy,0.663,0.559\n",
      "\n",
      "-------------\n",
      "threshold:  0.7\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.776,0.629,0.519,0.822,0.639,21735.316\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.699,0.646,0.654,0.544,15719.628\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.965,0.500,0.864\n",
      "AUPRC,0.882,0.073,0.932\n",
      "F-measure,0.667,0.000,0.890\n",
      "Accuracy,0.526,0.000,0.986\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.745,0.654\n",
      "F-measure,0.593,0.700\n",
      "Accuracy,0.490,0.828\n",
      "\n",
      "-------------\n",
      "threshold:  0.75\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.776,0.629,0.462,0.796,0.550,22986.242\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.699,0.570,0.607,0.405,19435.071\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.965,0.500,0.864\n",
      "AUPRC,0.882,0.073,0.932\n",
      "F-measure,0.510,0.000,0.877\n",
      "Accuracy,0.342,0.000,1.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.745,0.654\n",
      "F-measure,0.444,0.696\n",
      "Accuracy,0.306,0.925\n",
      "\n",
      "-------------\n",
      "threshold:  0.8\n",
      "#Murmur scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.776,0.629,0.316,0.738,0.402,25274.571\n",
      "\n",
      "#Outcome scores\n",
      "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
      "0.693,0.699,0.505,0.581,0.314,21942.863\n",
      "\n",
      "#Murmur scores (per class)\n",
      "Classes,Present,Unknown,Absent\n",
      "AUROC,0.965,0.500,0.864\n",
      "AUPRC,0.882,0.073,0.932\n",
      "F-measure,0.100,0.000,0.848\n",
      "Accuracy,0.053,0.000,1.000\n",
      "\n",
      "#Outcome scores (per class)\n",
      "Classes,Abnormal,Normal\n",
      "AUROC,0.693,0.693\n",
      "AUPRC,0.745,0.654\n",
      "F-measure,0.310,0.699\n",
      "Accuracy,0.184,1.000\n",
      "\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "# 8/14 15:59\n",
    "for th1 in [0.01, 0.05, 0.1, 0.15,0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8] :\n",
    "    murmur_binary_outputs[:,0] = murmur_scalar_outputs[:,0] > th1\n",
    "    murmur_binary_outputs[:,2] = murmur_scalar_outputs[:,2] > 1 - th1\n",
    "    outcome_binary_outputs[:,0] = outcome_scalar_outputs[:,0] > th1\n",
    "    outcome_binary_outputs[:,1] = outcome_scalar_outputs[:,1] > 1 - th1\n",
    "    # For each patient, set the 'Present' or 'Abnormal' class to positive if no class is positive or if multiple classes are positive.\n",
    "    murmur_labels = enforce_positives(murmur_labels, murmur_classes, 'Present')\n",
    "    murmur_binary_outputs = enforce_positives(murmur_binary_outputs, murmur_classes, 'Present')\n",
    "    outcome_labels = enforce_positives(outcome_labels, outcome_classes, 'Abnormal')\n",
    "    outcome_binary_outputs = enforce_positives(outcome_binary_outputs, outcome_classes, 'Abnormal')\n",
    "    # Evaluate the murmur model by comparing the labels and model outputs.\n",
    "    murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes = compute_auc(murmur_labels, murmur_scalar_outputs)\n",
    "    murmur_f_measure, murmur_f_measure_classes = compute_f_measure(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_accuracy, murmur_accuracy_classes = compute_accuracy(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_weighted_accuracy = compute_weighted_accuracy(murmur_labels, murmur_binary_outputs, murmur_classes) # This is the murmur scoring metric.\n",
    "    murmur_cost = compute_cost(outcome_labels, murmur_binary_outputs, outcome_classes, murmur_classes) # Use *outcomes* to score *murmurs* for the Challenge cost metric, but this is not the actual murmur scoring metric.\n",
    "    murmur_scores = (murmur_classes, murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes, \\\n",
    "                 murmur_f_measure, murmur_f_measure_classes, murmur_accuracy, murmur_accuracy_classes, murmur_weighted_accuracy, murmur_cost)\n",
    "\n",
    "    # Evaluate the outcome model by comparing the labels and model outputs.\n",
    "    outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes = compute_auc(outcome_labels, outcome_scalar_outputs)\n",
    "    outcome_f_measure, outcome_f_measure_classes = compute_f_measure(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_accuracy, outcome_accuracy_classes = compute_accuracy(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_weighted_accuracy = compute_weighted_accuracy(outcome_labels, outcome_binary_outputs, outcome_classes)\n",
    "    outcome_cost = compute_cost(outcome_labels, outcome_binary_outputs, outcome_classes, outcome_classes) # This is the clinical outcomes scoring metric.\n",
    "    outcome_scores = (outcome_classes, outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes, \\\n",
    "                  outcome_f_measure, outcome_f_measure_classes, outcome_accuracy, outcome_accuracy_classes, outcome_weighted_accuracy, outcome_cost)\n",
    "\n",
    "\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "    murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "    outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "                + '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "    print(\"threshold: \", th1)\n",
    "    print(output_string)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0904f389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_code import *\n",
    "import numpy as np, scipy as sp, scipy.stats, os, sys, joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from models import *\n",
    "from get_feature import *\n",
    "from Generator0 import *\n",
    "import pickle as pk\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import librosa\n",
    "import librosa.display\n",
    "import math\n",
    "import sys\n",
    "sys.path.insert(0,'lucashnegri-peakutils-51a679cd8428')\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "import peakutils\n",
    "from scipy import special\n",
    "import scipy.io as sio\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import peakutils\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 64/64 [00:13<00:00,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melspec:  120 313\n",
      "cqt:  1 1\n",
      "stft:  1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "    train_folder =  '/home/jk21/Downloads/Data/data/murmur/train'\n",
    "    test_folder = '/home/jk21/Downloads/Data/data/murmur/test'\n",
    "    patient_files_trn = find_patient_files(train_folder)\n",
    "    patient_files_test = find_patient_files(test_folder)\n",
    "    \n",
    "    per_sec=4000\n",
    "    winlen = 512\n",
    "    hoplen = 256\n",
    "    nmel = 120 \n",
    "    nsec = 20 \n",
    "    trim = 1 \n",
    "    use_mel=True\n",
    "    use_cqt = False \n",
    "    use_stft = False\n",
    "    interval_seq = np.random.choice([True,False])\n",
    "    ord1 = True #np.random.choice([True,False])\n",
    "    maxlen1 = 120000\n",
    "    min_dist = np.random.choice(list(range(10,2000 ,10)))\n",
    "    max_interval_len = np.random.choice(list(range(100,300 ,10)))\n",
    "    \n",
    "    params_feature = {'samp_sec': nsec,\n",
    "                  #### melspec, stft 피쳐 옵션들  \n",
    "                  'pre_emphasis': 0,\n",
    "                  'hop_length': hoplen,\n",
    "                  'win_length': winlen,\n",
    "                  'n_mels': nmel,\n",
    "                  #### cqt 피쳐 옵션들  \n",
    "                  'filter_scale': 1,\n",
    "                  'n_bins': 80,\n",
    "                  'fmin': 10,\n",
    "                  'maxlen1': maxlen1,\n",
    "                  'min_dist':min_dist,\n",
    "                  'max_interval_len' : max_interval_len,\n",
    "                  'trim' :1,\n",
    "                  'use_mel' : use_mel,\n",
    "                  'use_cqt' : use_cqt,\n",
    "                  'use_stft' : use_stft,\n",
    "                  'use_interval_seq' : interval_seq,\n",
    "                  'per_sec' : per_sec}\n",
    "    \n",
    "    model_folder = 'lcnn2'\n",
    "    \n",
    "    def sigmoidal_decay(e, start=0, end=100, lr_start=1e-3, lr_end=1e-5):\n",
    "        \n",
    "        if e < start:\n",
    "            return lr_start\n",
    "        elif e > end:\n",
    "            return lr_end\n",
    "\n",
    "        middle = (start + end) / 2\n",
    "        s = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "        return s(13 * (-e + middle) / np.abs(end - start)) * np.abs(lr_start - lr_end) + lr_end\n",
    "\n",
    "    if ord1 :\n",
    "        \n",
    "        features_trn, mel_input_shape, cqt_input_shape, stft_input_shape,interval_input_shape = get_features_3lb_all_ord_rr(train_folder, patient_files_trn[:64], **params_feature)\n",
    "    else :\n",
    "        features_trn, mel_input_shape, cqt_input_shape, stft_input_shape,interval_input_shape = get_features_3lb_all(train_folder, patient_files_trn, **params_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'samp_sec': 20,\n",
       " 'pre_emphasis': 0,\n",
       " 'hop_length': 256,\n",
       " 'win_length': 512,\n",
       " 'n_mels': 120,\n",
       " 'filter_scale': 1,\n",
       " 'n_bins': 80,\n",
       " 'fmin': 10,\n",
       " 'maxlen1': 120000,\n",
       " 'min_dist': 1970,\n",
       " 'max_interval_len': 220,\n",
       " 'trim': 1,\n",
       " 'use_mel': True,\n",
       " 'use_cqt': False,\n",
       " 'use_stft': False,\n",
       " 'use_interval_seq': False,\n",
       " 'per_sec': 4000}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    mm_weight = 3 #np.random.choice([2,3,4,5])\n",
    "    oo_weight = 3 #np.random.choice([2,3,4,5,6])\n",
    "    ord1 = True #np.random.choice([True,False])\n",
    "    mm_mean = False #np.random.choice([True,False])\n",
    "    dp = 0 #np.random.choice([0, .1, .2, .3])\n",
    "    fc = False #np.random.choice([True,False])\n",
    "    interval_seq = np.random.choice([True,False])\n",
    "    ext = False\n",
    "    ext2 = True\n",
    "    \n",
    "    # model_select  =np.random.choice([0,1])\n",
    "    m1,m2=['lcnn1','lcnn2']\n",
    "    \n",
    "    chaug = 10 #np.random.choice([0, 10])\n",
    "    mixup = True #np.random.choice([True,False])\n",
    "    cout = .8 #np.random.choice([0, 0.8])\n",
    "    wunknown = 1 #np.random.choice([1, 0.7, .5, .2])\n",
    "    n1 = 0 #np.random.choice([0,2])\n",
    "    if n1 == 0 :\n",
    "        ranfil = False\n",
    "    else :\n",
    "        ranfil = [n1, [18,19,20,21,22,23]]\n",
    "    \n",
    "    nep = 10\n",
    "    \n",
    "    params_feature['ord1'] = ord1\n",
    "    params_feature['mm_mean'] = mm_mean\n",
    "    params_feature['dp'] = dp\n",
    "    params_feature['fc'] = fc\n",
    "    params_feature['ext'] = ext\n",
    "    params_feature['oo_weight'] = oo_weight\n",
    "    params_feature['mm_weight'] = mm_weight\n",
    "    params_feature['chaug'] = chaug\n",
    "    params_feature['cout'] = cout\n",
    "    params_feature['wunknown'] = wunknown\n",
    "    params_feature['mixup'] = mixup\n",
    "    params_feature['n1'] = n1\n",
    "    params_feature['use_interval_seq'] = interval_seq\n",
    "    params_feature['m_name1'] = m1\n",
    "    params_feature['m_name2'] = m2\n",
    "    \n",
    "    \n",
    "    \n",
    "    mel_input_shape = features_trn['mel1'].shape[1:]\n",
    "    cqt_input_shape = features_trn['cqt1'].shape[1:]\n",
    "    stft_input_shape = features_trn['stft1'].shape[1:]\n",
    "    interval_input_shape= features_trn['interval'].shape[1:]\n",
    "    interval_mean_input_shape = (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-08 23:23:20.927739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-08 23:23:20.932891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-08 23:23:20.933303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-08 23:23:20.934080: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-08 23:23:20.935189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-08 23:23:20.935583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-08 23:23:20.935957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-08 23:23:21.276404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-08 23:23:21.276718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-08 23:23:21.276987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-08 23:23:21.277238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9318 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "if m1 =='lcnn1':\n",
    "    model1=get_LCNN_o_1_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape,interval_input_shape,interval_input_shape, ord1= ord1,dp = dp, fc = fc, ext = False, ext2=True,use_interval_seq=interval_seq)\n",
    "if m2 =='lcnn2':\n",
    "    model2=get_LCNN_2_dr_rr(mel_input_shape, cqt_input_shape, stft_input_shape,interval_input_shape,interval_input_shape,dp = dp, fc = fc, ext = True,ext2=False,use_interval_seq=interval_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-08 23:23:27.174481: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 4s 71ms/step - loss: 1.3297 - accuracy: 0.4336 - auc: 0.3766 - lr: 9.9851e-04\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 72ms/step - loss: 1.2010 - accuracy: 0.3906 - auc: 0.4435 - lr: 9.9457e-04\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 1.1628 - accuracy: 0.4219 - auc: 0.4070 - lr: 9.8036e-04\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 73ms/step - loss: 1.1841 - accuracy: 0.4805 - auc: 0.5068 - lr: 9.3155e-04\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 1.1875 - accuracy: 0.4922 - auc: 0.5074 - lr: 7.8798e-04\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 1.1657 - accuracy: 0.4805 - auc: 0.5007 - lr: 5.0500e-04\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 1.2362 - accuracy: 0.5352 - auc: 0.6040 - lr: 2.2202e-04\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 74ms/step - loss: 1.1552 - accuracy: 0.4688 - auc: 0.4573 - lr: 7.8447e-05\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 72ms/step - loss: 1.1934 - accuracy: 0.4453 - auc: 0.4428 - lr: 2.9642e-05\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 74ms/step - loss: 1.1900 - accuracy: 0.5000 - auc: 0.4649 - lr: 1.5431e-05\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 3s 79ms/step - loss: 14.9842 - accuracy: 0.1641 - auc: 0.1358 - lr: 9.9851e-04\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 72ms/step - loss: 9.6747 - accuracy: 0.1914 - auc: 0.1564 - lr: 9.9457e-04\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 5.8161 - accuracy: 0.1992 - auc: 0.1423 - lr: 9.8036e-04\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 3.0635 - accuracy: 0.3672 - auc: 0.3124 - lr: 9.3155e-04\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 1.5407 - accuracy: 0.6602 - auc: 0.7462 - lr: 7.8798e-04\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 73ms/step - loss: 0.9947 - accuracy: 0.8125 - auc: 0.8649 - lr: 5.0500e-04\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 0.9538 - accuracy: 0.8086 - auc: 0.8296 - lr: 2.2202e-04\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 0.9500 - accuracy: 0.8125 - auc: 0.8505 - lr: 7.8447e-05\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 0.9049 - accuracy: 0.8516 - auc: 0.8773 - lr: 2.9642e-05\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 0.9604 - accuracy: 0.8359 - auc: 0.8288 - lr: 1.5431e-05\n"
     ]
    }
   ],
   "source": [
    "    n_epoch = nep\n",
    "    lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "    batch_size = 64\n",
    "\n",
    "    if mixup :\n",
    "        beta_param = .7\n",
    "    else :\n",
    "        beta_param = 0\n",
    "\n",
    "\n",
    "\n",
    "    params = {'batch_size': batch_size,\n",
    "              #          'input_shape': (100, 313, 1),\n",
    "              'shuffle': True,\n",
    "              'chaug': chaug,\n",
    "              'beta_param': beta_param,\n",
    "              'cout': cout\n",
    "    #              'mixup': mixup,\n",
    "              #          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "              #          'highpass': [.5, [78,79,80,81,82,83,84,85]]\n",
    "    #              'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "              #           'dropblock' : [30, 100]\n",
    "              #'device' : device\n",
    "    }\n",
    "\n",
    "    if mixup :\n",
    "        params['mixup'] = mixup\n",
    "        params['ranfilter2'] = ranfil\n",
    "    else :\n",
    "        params['cutout'] = cout\n",
    "\n",
    "    params_no_shuffle = {'batch_size': batch_size,\n",
    "                         #          'input_shape': (100, 313, 1),\n",
    "                         'shuffle': False,\n",
    "                         'beta_param': 0.7,\n",
    "                         'mixup': False\n",
    "                         #'device': device\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    if ord1 :\n",
    "        class_weight = {0: mm_weight, 1: 1.}\n",
    "    else :\n",
    "        class_weight = {0: mm_weight, 1: wunknown, 2:1.}\n",
    "\n",
    "\n",
    "    if mixup :\n",
    "\n",
    "\n",
    "        TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                                  features_trn['preg'], features_trn['loc'],\n",
    "                                  features_trn['mel1'], features_trn['cqt1'],features_trn['stft1'],\n",
    "                                  features_trn['interval_mean'],features_trn['interval']],features_trn['mm_labels'],**params)()\n",
    "\n",
    "        model1.fit(TrainDGen_1,\n",
    "#                    validation_data = ([features_test['age'],features_test['sex'], features_test['hw'],                                   \n",
    "#                                        features_test['preg'], features_test['loc'],\n",
    "#                                        features_test['interval'],\n",
    "#                                        features_test['mel1'],\n",
    "#                                        features_test['cqt1'],\n",
    "#                                        features_test['stft1'],features_test['interval_mean']],features_test['mm_labels']),\n",
    "\n",
    "                   callbacks=[lr],\n",
    "                   steps_per_epoch=np.ceil(len(features_trn['mm_labels'])/batch_size),\n",
    "                   class_weight=class_weight, \n",
    "                   epochs = n_epoch)\n",
    "    \n",
    "    \n",
    " \n",
    "    n_epoch = nep\n",
    "    lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "\n",
    "    batch_size = 64\n",
    "\n",
    "    if mixup :\n",
    "        beta_param = .7\n",
    "    else :\n",
    "        beta_param = 0\n",
    "\n",
    "    params = {'batch_size': batch_size,\n",
    "              #          'input_shape': (100, 313, 1),\n",
    "              'shuffle': True,\n",
    "              'chaug': 0,\n",
    "              'beta_param': beta_param,\n",
    "              'cout': cout,\n",
    "    #              'mixup': True,\n",
    "              #          'lowpass': [.5, [11,12,13,14,15,16,17,18]]\n",
    "    #            'highpass': [.5, [78,79,80,81,82,83,84,85]],\n",
    "    #              'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "            #           'dropblock' : [30, 100]\n",
    "              #'device' : device\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    if mixup :\n",
    "        params['mixup'] = mixup\n",
    "        params['ranfilter2'] = ranfil\n",
    "    else :\n",
    "        params['cutout'] = cout\n",
    "\n",
    "\n",
    "    params_no_shuffle = {'batch_size': batch_size,\n",
    "                         #          'input_shape': (100, 313, 1),\n",
    "                         'shuffle': False,\n",
    "                         'beta_param': 0.7,\n",
    "                         'mixup': False\n",
    "                         #'device': device\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if mixup :\n",
    "\n",
    "\n",
    "        class_weight = {0: oo_weight, 1: 1.}\n",
    "        TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], \n",
    "                                  features_trn['preg'], features_trn['loc'],\n",
    "                                  features_trn['mel1'], features_trn['cqt1'],features_trn['stft1'],\n",
    "                                  features_trn['interval_mean'],features_trn['interval']],features_trn['out_labels'],**params)()\n",
    "\n",
    "        model2.fit(TrainDGen_1,\n",
    "#                    validation_data = ([features_test['age'],features_test['sex'], features_test['hw'],                                   \n",
    "#                                        features_test['preg'], features_test['loc'],features_test['interval'],\n",
    "#                                        features_test['mel1'],\n",
    "#                                        features_test['cqt1'],\n",
    "#                                        features_test['stft1'],features_test['interval_mean']],features_test['out_labels']), \n",
    "                   callbacks=[lr],\n",
    "            \n",
    "                   steps_per_epoch=np.ceil(len(features_trn['out_labels'])/batch_size),\n",
    "                   class_weight=class_weight, \n",
    "                   epochs = n_epoch)\n",
    "    \n",
    "    params_feature['mel_shape'] = mel_input_shape\n",
    "    params_feature['cqt_shape'] = cqt_input_shape\n",
    "    params_feature['stft_shape'] = stft_input_shape\n",
    "    params_feature['interval_shape'] = interval_input_shape\n",
    "    params_feature['interval_mean_shape'] = interval_mean_input_shape\n",
    "    params_feature['max_interval_len'] = max_interval_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle as pk\n",
    "def save_challenge_model(model_folder, model1, model2, m_name1, m_name2, param_feature) :\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    filename1 = os.path.join(model_folder, m_name1 + '_model1.hdf5')\n",
    "    filename2 = os.path.join(model_folder, m_name2 + '_model2.hdf5')\n",
    "    model1.save(filename1)\n",
    "    model2.save(filename2)\n",
    "    params_feature['model1'] = m_name1\n",
    "    params_feature['model2'] = m_name2\n",
    "    params_feature['model_fnm1'] = filename1\n",
    "    params_feature['model_fnm2'] = filename2\n",
    "    with open(info_fnm, 'wb') as f:\n",
    "        pk.dump(params_feature, f, pk.HIGHEST_PROTOCOL)\n",
    "    return 1\n",
    "\n",
    "    with open(info_fnm, 'wb') as f:\n",
    "        pk.dump(params_feature, f, pk.HIGHEST_PROTOCOL)\n",
    "    return 1\n",
    "    \n",
    "save_challenge_model(model_folder, model1, model2, m_name1 = m1, m_name2 = m2, param_feature = params_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def load_challenge_model(model_folder, verbose):\n",
    "        info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "        with open(info_fnm, 'rb') as f:\n",
    "            info_m = pk.load(f)\n",
    "\n",
    "        return info_m\n",
    "    \n",
    "    ########################RunModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_challenge_model(model_folder, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'samp_sec': 20,\n",
       " 'pre_emphasis': 0,\n",
       " 'hop_length': 256,\n",
       " 'win_length': 512,\n",
       " 'n_mels': 120,\n",
       " 'filter_scale': 1,\n",
       " 'n_bins': 80,\n",
       " 'fmin': 10,\n",
       " 'maxlen1': 120000,\n",
       " 'min_dist': 1970,\n",
       " 'max_interval_len': 220,\n",
       " 'trim': 1,\n",
       " 'use_mel': True,\n",
       " 'use_cqt': False,\n",
       " 'use_stft': False,\n",
       " 'use_interval_seq': False,\n",
       " 'per_sec': 4000,\n",
       " 'ord1': True,\n",
       " 'mm_mean': False,\n",
       " 'dp': 0,\n",
       " 'fc': False,\n",
       " 'ext': False,\n",
       " 'oo_weight': 3,\n",
       " 'mm_weight': 3,\n",
       " 'chaug': 10,\n",
       " 'cout': 0.8,\n",
       " 'wunknown': 1,\n",
       " 'mixup': True,\n",
       " 'n1': 0,\n",
       " 'm_name1': 'lcnn1',\n",
       " 'm_name2': 'lcnn2',\n",
       " 'mel_shape': (120, 313, 1),\n",
       " 'cqt_shape': (1, 1, 1),\n",
       " 'stft_shape': (1, 1, 1),\n",
       " 'interval_shape': (1,),\n",
       " 'interval_mean_shape': (1,),\n",
       " 'model1': 'lcnn1',\n",
       " 'model2': 'lcnn2',\n",
       " 'model_fnm1': 'lcnn2/lcnn1_model1.hdf5',\n",
       " 'model_fnm2': 'lcnn2/lcnn2_model2.hdf5'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def run_challenge_model(model, data, recordings, verbose):\n",
    "        \n",
    "        murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "        outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "    ##########################################################################    \n",
    "        if model['model1'] == 'lcnn1' :\n",
    "            model1 = get_LCNN_o_1_dr_rr(model['mel_shape'],model['cqt_shape'],model['stft_shape'], model['interval_shape'],\n",
    "                                     use_mel = model['use_mel'],use_cqt = model['use_cqt'], \n",
    "                                     use_stft = model['use_stft'],\n",
    "                                     ord1 = model['ord1'], dp = model['dp'], fc = model['fc'], \n",
    "                                     ext = False, ext2= True,use_interval_seq=model['use_interval_seq'])  \n",
    "        if model['model2'] == 'lcnn2' :\n",
    "            model2 = get_LCNN_2_dr_rr(model['mel_shape'],model['cqt_shape'],model['stft_shape'], \n",
    "                                       model['interval_shape'],\n",
    "                                       use_mel = model['use_mel'],use_cqt = model['use_cqt'], \n",
    "                                       use_stft = model['use_stft'],\n",
    "                                       dp = model['dp'], fc = model['fc'], \n",
    "                                       ext = True, ext2=False,use_interval_seq=model['use_interval_seq']) \n",
    "\n",
    "        \n",
    "        # if model['model1'] == 'lcnn3' :\n",
    "        #     model1 = get_LCNN_pi_seq_mean_1(model['mel_shape'],model['cqt_shape'],model['stft_shape'], \n",
    "        #                                       model['interval_shape'],model['interval_mean_shape'],\n",
    "        #                                       use_mel = model['use_mel'],use_cqt = model['use_cqt'], \n",
    "        #                                       use_stft = model['use_stft'],\n",
    "        #                                       ord1 = model['ord1'], dp = model['dp'], fc = model['fc'], \n",
    "        #                                       ext = False, ext2= True) \n",
    "            \n",
    "        # if model['model2'] == 'lcnn4' :\n",
    "        #     model2 = get_LCNN_pi_seq_mean_2(model['mel_shape'],model['cqt_shape'],model['stft_shape'], \n",
    "        #                                         model['interval_shape'], model['interval_mean_shape'],\n",
    "        #                                         use_mel = model['use_mel'],use_cqt = model['use_cqt'], \n",
    "        #                                         use_stft = model['use_stft'],ord1 = model['ord1'], \n",
    "        #                                         dp = model['dp'], fc = model['fc'], \n",
    "        #                                     ext = True, ext2=False)\n",
    "\n",
    "#         if model['model1'] == 'lcnn5' :\n",
    "#             model1 = get_LCNN_o_4_dr_interval_mean(model['mel_shape'],model['cqt_shape'],model['stft_shape'], \n",
    "#                                                    model['interval_shape'],model['interval_mean_shape'],\n",
    "#                                                    use_mel = model['use_mel'],use_cqt = model['use_cqt'], \n",
    "#                                                    use_stft = model['use_stft'], ord1 = model['ord1'], \n",
    "#                                                    dp = model['dp'], fc = model['fc'], ext = model['ext'])  \n",
    "            \n",
    "#         if model['model2'] == 'lcnn6' :\n",
    "#             model2 = get_LCNN_o_4_dr_1_interval_mean(model['mel_shape'],model['cqt_shape'],model['stft_shape'], \n",
    "#                                                      model['interval_shape'],model['interval_mean_shape'],\n",
    "#                                                      use_mel = model['use_mel'],use_cqt = model['use_cqt'], \n",
    "#                                                      use_stft = model['use_stft'],ord1 = model['ord1'], \n",
    "#                                                      dp = model['dp'], fc = model['fc'], ext = model['ext'])\n",
    "        \n",
    "        \n",
    "\n",
    "        model1.load_weights(model['model_fnm1'])\n",
    "        model2.load_weights(model['model_fnm2'])\n",
    "    ############################################################################ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Load features.\n",
    "        features = get_feature_one(data, verbose = 0)\n",
    "\n",
    "        samp_sec = model['samp_sec'] \n",
    "        pre_emphasis = model['pre_emphasis']\n",
    "        hop_length = model['hop_length']\n",
    "        win_length = model['win_length']\n",
    "        n_mels = model['n_mels']\n",
    "        filter_scale = model['filter_scale']\n",
    "        n_bins = model['n_bins']\n",
    "        fmin = model['fmin']\n",
    "        trim = model['trim']\n",
    "        use_mel = model['use_mel']\n",
    "        use_cqt = model['use_cqt']\n",
    "        use_stft = model['use_stft']\n",
    "        use_interval_seq = model['use_interval_seq']\n",
    "        maxlen1 = model['maxlen1']\n",
    "        min_dist = model['min_dist']\n",
    "        per_sec = model['per_sec']\n",
    "\n",
    "    #    use_raw = model['use_raw']\n",
    "\n",
    "        max_interval_len = model['max_interval_len']\n",
    "\n",
    "\n",
    "        # Load features.\n",
    "\n",
    "\n",
    "        tmp_total_interval = [] \n",
    "        tmp_total_interval_mean = []\n",
    "\n",
    "        features['interval'] = []\n",
    "        features['interval_mean']=[]\n",
    "\n",
    "        if use_interval_seq:\n",
    "            \n",
    "\n",
    "            for i in range(len(recordings)):\n",
    "                datos=recordings[i]\n",
    "                filtros=sio.loadmat('./Filters1')\n",
    "                tmp_interval = []\n",
    "\n",
    "\n",
    "                try:\n",
    "                    X = datos\n",
    "                    Fs= 4000\n",
    "                    Fpa20=filtros['Fpa20'];\t\t\t        # High pass filter\n",
    "                    Fpa20=Fpa20[0];\t\t\t\t\t# High pass filter\n",
    "                    Fpb100=filtros['Fpb100'];\t\t        # Low-pass Filter\n",
    "                    Fpb100=Fpb100[0];\t\t\t\t# Low-pass Filter\n",
    "                    Xf=FpassBand(X,Fpa20,Fpb100); \t                # Apply a passband filter\n",
    "                    Xf=vec_nor(Xf);\t\t\t\n",
    "\n",
    "        # Derivate of the Signal\n",
    "                    dX=derivate(Xf);\t\t\t\t# Derivate of the signal\n",
    "                    dX=vec_nor(dX);\t\t\t\t\t# Vector Normalizing\n",
    "        # Square of the signal\n",
    "                    dy=np.square(Xf);\n",
    "                    dy=vec_nor(dy);\n",
    "\n",
    "                    size=np.shape(Xf)\t\t\t\t# Rank or dimension of the array\n",
    "                    fil=size[0];\t\t\t\t\t# Number of rows\n",
    "\n",
    "                    positive=np.zeros((1,fil+1));                   # Initializating Positives Values Vector \n",
    "                    positive=positive[0];                           # Getting the Vector\n",
    "\n",
    "                    points=np.zeros((1,fil));                       # Initializating the all Peak Points Vector\n",
    "                    points=points[0];                               # Getting the point vector\n",
    "\n",
    "                    peaks=np.zeros((1,fil));                        # Initializating the s1-s1 Peak Vector\n",
    "                    peaks=peaks[0];                                 # Getting the point vector\n",
    "\n",
    "\n",
    "                    for i in range(0,fil):\n",
    "                        if dX[i]>0:\n",
    "                            positive[i]=1;\n",
    "                        else:\n",
    "                            positive[i]=0;\n",
    "\n",
    "                    for i in range(0,fil):\n",
    "                        if (positive[i]==1 and positive[i+1]==0):\n",
    "                            points[i]=Xf[i];\n",
    "                        else:\n",
    "                            points[i]=0;\n",
    "\n",
    "                    indexes=peakutils.indexes(points,thres=0.5/max(points), min_dist=min_dist);\n",
    "                    lenght=np.shape(indexes)\t\t\t# Get the length of the index vector\t\t\n",
    "                    lenght=lenght[0];\t\t\t\t# Get the value of the index vector\n",
    "\n",
    "                    for i in range(0,lenght):\n",
    "                        p=indexes[i];\n",
    "                        peaks[p]=points[p];\n",
    "\n",
    "                    n=np.arange(0,fil);\n",
    "\n",
    "                    ############### interval distance ##############                   \n",
    "                    tmp_peaks = np.diff(indexes)/4000\n",
    "\n",
    "                    ############## interval sequence ###############\n",
    "                    tmp_peaks_inteval = tmp_peaks\n",
    "\n",
    "\n",
    "                    ########### interval distance mean ############\n",
    "                    tmp_peaks = np.array(tmp_peaks)\n",
    "                    tmp_peaks = np.mean(tmp_peaks)\n",
    "\n",
    "                    if len(tmp_peaks_inteval) == 1:\n",
    "                        tmp_peaks_inteval = np.zeros(1)\n",
    "\n",
    "    #                         tmp_peaks_mean = np.mean(tmp_peaks)\n",
    "    #                         tmp_peaks_std = np.std(tmp_peaks)\n",
    "\n",
    "    #                         tmp_final = np.divide((tmp_peaks-tmp_peaks_mean),tmp_peaks_std)\n",
    "\n",
    "                    tmp_peaks = np.nan_to_num(tmp_peaks)\n",
    "    #                     print(tmp_final)\n",
    "                    tmp_total_interval_mean.append(tmp_peaks)\n",
    "                    tmp_total_interval.append(tmp_peaks_inteval)\n",
    "\n",
    "\n",
    "                except:\n",
    "                    print(i)\n",
    "                    tmp_peaks = np.zeros(1)\n",
    "                    tmp_peaks_inteval = np.zeros(1)\n",
    "                    tmp_total_interval_mean.append(tmp_peaks)\n",
    "                    tmp_total_interval.append(tmp_peaks_inteval)\n",
    "\n",
    "        # else :\n",
    "        #     tmp_peaks = np.zeros(1)\n",
    "        #     tmp_peaks_inteval = np.zeros(1)\n",
    "        #     tmp_total_interval_mean.append(tmp_peaks)\n",
    "        #     tmp_total_interval.append(tmp_peaks_inteval)      \n",
    "\n",
    "\n",
    "        if use_interval_seq:\n",
    "\n",
    "\n",
    "            ############ interval Sequence #################\n",
    "            padded =pad_sequences(tmp_total_interval, maxlen=max_interval_len, dtype='float32', padding='post', truncating='post',\n",
    "                                  value=0.0)\n",
    "\n",
    "            for i in range(len(padded)):\n",
    "                features['interval'].append(padded[i])\n",
    "            for i in range(len(features['interval'])):\n",
    "                features['interval'][i]= features['interval'][i].reshape(-1,1)\n",
    "            features['interval']= np.array(features['interval'])\n",
    "\n",
    "\n",
    "            features['interval_mean'] = np.array(tmp_total_interval_mean,dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "        features['mel1'] = []\n",
    "        for i in range(len(recordings)) :\n",
    "            if use_mel :\n",
    "                mel1 = feature_extract_melspec(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length,\n",
    "                                               win_length = win_length, n_mels = n_mels)[0]\n",
    "            else :\n",
    "                mel1 = np.zeros( (1,1) )\n",
    "\n",
    "            features['mel1'].append(mel1)\n",
    "\n",
    "        M, N = features['mel1'][0].shape\n",
    "\n",
    "        if use_mel :\n",
    "            for i in range(len(features['mel1'])) :\n",
    "                features['mel1'][i] = features['mel1'][i].reshape(M,N,1)\n",
    "\n",
    "        features['mel1'] = np.array(features['mel1'])\n",
    "\n",
    "        features['cqt1'] = []\n",
    "        for i in range(len(recordings)) :\n",
    "            if use_cqt :\n",
    "                mel1 = feature_extract_cqt(recordings[i], samp_sec=samp_sec, pre_emphasis = pre_emphasis, filter_scale = filter_scale,\n",
    "                                            n_bins = n_bins, fmin = fmin)[0]\n",
    "            else:\n",
    "                mel1 = np.zeros( (1,1,1) )\n",
    "\n",
    "            features['cqt1'].append(mel1)\n",
    "\n",
    "        M, N,__ = features['cqt1'][0].shape\n",
    "\n",
    "        if use_cqt :\n",
    "            for i in range(len(features['cqt1'])) :\n",
    "                features['cqt1'][i] = features['cqt1'][i].reshape(M,N,1)\n",
    "\n",
    "        features['cqt1'] = np.array(features['cqt1'])\n",
    "\n",
    "\n",
    "        features['stft1'] = []\n",
    "        for i in range(len(recordings)) :\n",
    "            if use_stft :\n",
    "                mel1 = feature_extract_stft(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length,\n",
    "                                            win_length = win_length)[0]\n",
    "            else :\n",
    "                mel1 = np.zeros( (1,1,1) )\n",
    "\n",
    "            features['stft1'].append(mel1)\n",
    "\n",
    "        M, N,__ = features['stft1'][0].shape\n",
    "        if use_stft :\n",
    "            for i in range(len(features['stft1'])) :\n",
    "                features['stft1'][i] = features['stft1'][i].reshape(M,N,1)\n",
    "        features['stft1'] = np.array(features['stft1'])\n",
    "\n",
    "    #     return features\n",
    "\n",
    "        #    print(features)\n",
    "        # Impute missing data.\n",
    "        \n",
    "        \n",
    "        \n",
    "        res1 = model1.predict([features['age'], features['sex'], features['hw'], features['preg'],\n",
    "                               features['loc'], features['mel1'], features['stft1'], features['cqt1'],features['interval_mean'],features['interval']])\n",
    "        res2 = model2.predict([features['age'], features['sex'], features['hw'], features['preg'],\n",
    "                               features['loc'], features['mel1'], features['stft1'], features['cqt1'],features['interval_mean'],features['interval']])\n",
    "        \n",
    "        if model['ord1'] :\n",
    "            idx1 = res1.argmax(axis=0)[0]\n",
    "            murmur_p = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기ddd\n",
    "            murmur_probabilities = np.zeros((3,))\n",
    "            murmur_probabilities[0] = murmur_p[0]\n",
    "            murmur_probabilities[1] = 0\n",
    "            murmur_probabilities[2] = murmur_p[1]\n",
    "            outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "\n",
    "        else :\n",
    "            if model['mm_mean'] :\n",
    "                murmur_probabilities = res1.mean(axis = 0)\n",
    "            else :\n",
    "                idx1 = res1.argmax(axis=0)[0]\n",
    "                murmur_probabilities = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "            outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "\n",
    "\n",
    "        murmur_labels = np.zeros(len(murmur_classes), dtype=np.int_)\n",
    "\n",
    "\n",
    "        if murmur_probabilities[0] > 0.482 :\n",
    "            idx = 0\n",
    "        else :\n",
    "            idx = 2\n",
    "        murmur_labels[idx] = 1\n",
    "\n",
    "        outcome_labels = np.zeros(len(outcome_classes), dtype=np.int_)\n",
    "        if outcome_probabilities[0] > 0.607 :\n",
    "            idx = 0\n",
    "        else :\n",
    "            idx = 1\n",
    "            # idx = np.argmax(outcome_probabilities)\n",
    "        outcome_labels[idx] = 1\n",
    "\n",
    "        # Concatenate classes, labels, and probabilities.\n",
    "        classes = murmur_classes + outcome_classes\n",
    "        labels = np.concatenate((murmur_labels, outcome_labels))\n",
    "        probabilities = np.concatenate((murmur_probabilities, outcome_probabilities))\n",
    "\n",
    "        return classes, labels, probabilities\n",
    "    \n",
    "output_folder ='/home/jk21/Documents/submission_paper_out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = test_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_files = find_patient_files(data_folder)\n",
    "num_patient_files = len(patient_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "            patient_data = load_patient_data(patient_files[10])\n",
    "            recordings = load_recordings(data_folder, patient_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer count mismatch when loading weights from file. Model expected 27 layers, found 75 saved layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m classes, labels, probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mrun_challenge_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecordings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mrun_challenge_model\u001b[0;34m(model, data, recordings, verbose)\u001b[0m\n\u001b[1;32m     14\u001b[0m         model2 \u001b[38;5;241m=\u001b[39m get_LCNN_2_dr_rr(model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmel_shape\u001b[39m\u001b[38;5;124m'\u001b[39m],model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcqt_shape\u001b[39m\u001b[38;5;124m'\u001b[39m],model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstft_shape\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     15\u001b[0m                                    model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterval_shape\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     16\u001b[0m                                    use_mel \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_mel\u001b[39m\u001b[38;5;124m'\u001b[39m],use_cqt \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_cqt\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     17\u001b[0m                                    use_stft \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_stft\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     18\u001b[0m                                    dp \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdp\u001b[39m\u001b[38;5;124m'\u001b[39m], fc \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     19\u001b[0m                                    ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, ext2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,use_interval_seq\u001b[38;5;241m=\u001b[39mmodel[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_interval_seq\u001b[39m\u001b[38;5;124m'\u001b[39m]) \n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# if model['model1'] == 'lcnn3' :\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m#     model1 = get_LCNN_pi_seq_mean_1(model['mel_shape'],model['cqt_shape'],model['stft_shape'], \u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#                                       model['interval_shape'],model['interval_mean_shape'],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#                                                      use_stft = model['use_stft'],ord1 = model['ord1'], \u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#                                                      dp = model['dp'], fc = model['fc'], ext = model['ext'])\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_fnm1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     model2\u001b[38;5;241m.\u001b[39mload_weights(model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_fnm2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m############################################################################ \u001b[39;00m\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Load features.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/saving/hdf5_format.py:728\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, model)\u001b[0m\n\u001b[1;32m    726\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m filtered_layer_names\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(filtered_layers):\n\u001b[0;32m--> 728\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    729\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer count mismatch when loading weights from file. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    730\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_layers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m layers, found \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    731\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved layers.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    733\u001b[0m \u001b[38;5;66;03m# We batch weight value assignments in a single backend call\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;66;03m# which provides a speedup in TensorFlow.\u001b[39;00m\n\u001b[1;32m    735\u001b[0m weight_value_tuples \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 27 layers, found 75 saved layers."
     ]
    }
   ],
   "source": [
    "classes, labels, probabilities = run_challenge_model(model, patient_data, recordings, verbose) ### Teams: Implement this function!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Present', 'Unknown', 'Absent', 'Abnormal', 'Normal']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6cec2361aeb74e0b5ee9454472a5f6ffeaee3c517f88c30ed339271084c4c5a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d55dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73c09968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug  5 18:37:59 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| 35%   38C    P0    81W / 260W |    289MiB / 11019MiB |     43%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       998      G   /usr/lib/xorg/Xorg                 45MiB |\r\n",
      "|    0   N/A  N/A      1794      G   /usr/lib/xorg/Xorg                133MiB |\r\n",
      "|    0   N/A  N/A      2043      G   /usr/bin/gnome-shell               35MiB |\r\n",
      "|    0   N/A  N/A      2461      G   ...547988763393595631,131072       59MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad188626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-05 18:38:05.910631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-05 18:38:05.919786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-05 18:38:05.920376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-05 18:38:05.921961: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-05 18:38:05.922980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-05 18:38:05.923567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-05 18:38:05.924119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-05 18:38:06.274298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-05 18:38:06.274686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-05 18:38:06.274964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-05 18:38:06.275223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9383 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2022-08-05 18:38:06.753947: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import librosa\n",
    "import librosa.display\n",
    "import math\n",
    "import sys\n",
    "#sys.path.insert(0,'/home/ikwak2/hmd/notebooks')\n",
    "# sys.path.insert(0,'/home/jk21/Documents/hmd/jk_classifier/lucashnegri-peakutils-51a679cd8428')\n",
    "# sys.path.insert(0,'/home/jk21/Documents/hmd/jk_classifier/S1-S1-Phonocardiogram-Peak-Detection-Method-in-Python')\n",
    "sys.path.insert(0,'utils')\n",
    "from helper_code import *\n",
    "from get_feature import *\n",
    "from models import *\n",
    "from Generator0 import *\n",
    "from keras.preprocessing import sequence\n",
    "# import peakutils\n",
    "from scipy import special\n",
    "import scipy.io as sio\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "sys.path.insert(0,'/home/jk21/Documents/hmd/jk_classifier/lucashnegri-peakutils-51a679cd8428')\n",
    "import peakutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c573ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jk21/Documents/hmd/jk_classifier'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ab948dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'physionet.org/files/circor-heart-sound/1.0.3'\n",
    "# training_data_file = root_dir + '/' + 'training_data.csv'\n",
    "# training_data_dir = root_dir + '/' + 'training_data'\n",
    "model_dir = root_dir + '/' + 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a02ca4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug  5 18:38:07 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 35%   37C    P2    61W / 260W |  10493MiB / 11019MiB |      6%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       998      G   /usr/lib/xorg/Xorg                 45MiB |\n",
      "|    0   N/A  N/A      1794      G   /usr/lib/xorg/Xorg                133MiB |\n",
      "|    0   N/A  N/A      2043      G   /usr/bin/gnome-shell               35MiB |\n",
      "|    0   N/A  N/A      2461      G   ...547988763393595631,131072       46MiB |\n",
      "|    0   N/A  N/A    464681      C   ...3/envs/new_env/bin/python    10215MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23812c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jk21/Documents/hmd/jk_classifier'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42f5dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder =  'physionet.org/files/circor-heart-sound/1.0.3/training_data'\n",
    "train_folder =  '/home/jk21/Downloads/Data/data/murmur/train'\n",
    "test_folder = '/home/jk21/Downloads/Data/data/murmur/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca657f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = 'lcnn2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e49f868b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lcnn2'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac93e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e76c75d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidal_decay(e, start=0, end=100, lr_start=1e-3, lr_end=1e-5):\n",
    "    if e < start:\n",
    "        return lr_start\n",
    "    elif e > end:\n",
    "        return lr_end\n",
    "\n",
    "    middle = (start + end) / 2\n",
    "    s = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return s(13 * (-e + middle) / np.abs(end - start)) * np.abs(lr_start - lr_end) + lr_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac7432d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_files_trn = find_patient_files(train_folder)\n",
    "patient_files_test = find_patient_files(test_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca1fabdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_feature = {'samp_sec': 20,\n",
    "                  #### melspec, stft 피쳐 옵션들  \n",
    "                  'pre_emphasis': 0,\n",
    "                  'hop_length': 256,\n",
    "                  'win_length':512,\n",
    "                  'n_mels': 120,\n",
    "                  #### cqt 피쳐 옵션들  \n",
    "                  'filter_scale': 1,\n",
    "                  'n_bins': 80,\n",
    "                  'fmin': 10,\n",
    "                  'maxlen1': 120000,\n",
    "                  'min_dist':500,\n",
    "                  'max_interval_len' : 115,\n",
    "                  'trim' :1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82ce6a10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 751/751 [1:02:24<00:00,  4.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melspec:  120 313\n",
      "cqt:  1 1\n",
      "stft:  1 1\n",
      "interval:  115 1\n",
      "wav2:  374 32\n"
     ]
    }
   ],
   "source": [
    "features_trn,mel_input_shape, cqt_input_shape,stft_input_shape,interval_input_shape,wav2_input_shape = get_features_3lb_all(train_folder, patient_files_trn, **params_feature,use_mel = True, use_cqt = False, use_stft = False,use_raw=False,use_interval=True,use_wav2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02ba846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_3lb_all(data_folder, patient_files_trn, \n",
    "                          samp_sec=20, pre_emphasis = 0, hop_length=256, win_length = 512, n_mels = 100,\n",
    "                          filter_scale = 1, n_bins = 80, fmin = 10, trim = 4000,max_interval_len=115, maxlen1=120000,min_dist=1000,\n",
    "                         use_mel = True, use_cqt = False, use_stft= False, use_raw = False,use_interval = False,use_wav2=False\n",
    "                         ) :\n",
    "    features = dict()\n",
    "    features['id'] = []\n",
    "    features['age'] = []\n",
    "    features['sex'] = []\n",
    "    features['hw'] = []\n",
    "    features['preg'] = []\n",
    "    features['loc'] = []\n",
    "    features['mel1'] = []\n",
    "    features['cqt1'] = []\n",
    "    features['stft1'] = []\n",
    "    features['raw1'] = []\n",
    "    features['interval'] = []\n",
    "    features['wav2']=[]\n",
    "#    labels = []\n",
    "    features['mm_labels'] = []\n",
    "    features['out_labels'] = []\n",
    "    tmp_interval=[]\n",
    "    tmp_wav=[]\n",
    "    interval_len=[]\n",
    "\n",
    "    age_classes = ['Neonate', 'Infant', 'Child', 'Adolescent', 'Young Adult']\n",
    "    recording_locations = ['AV', 'MV', 'PV', 'TV', 'PhC']\n",
    "\n",
    "    num_patient_files = len(patient_files_trn)\n",
    "\n",
    "    for i in tqdm.tqdm(range(num_patient_files)):\n",
    "        \n",
    "\n",
    "        # Load the current patient data and recordings.\n",
    "        current_patient_data = load_patient_data(patient_files_trn[i])\n",
    "        num_locations = get_num_locations(current_patient_data)\n",
    "        recording_information = current_patient_data.split('\\n')[1:num_locations+1]\n",
    "        \n",
    "        for j in range(num_locations) :\n",
    "            entries = recording_information[j].split(' ')\n",
    "            recording_file = entries[2]\n",
    "            filename = os.path.join(data_folder, recording_file)\n",
    "\n",
    "            # Extract id\n",
    "            id1 = recording_file.split('_')[0]\n",
    "            features['id'].append(id1)\n",
    "\n",
    "            # Extract melspec\n",
    "            if use_mel :\n",
    "                mel1 = feature_extract_melspec(filename, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length, \n",
    "                                               win_length = win_length, n_mels = n_mels, trim = trim)[0]\n",
    "            else :\n",
    "                mel1 = np.zeros( (1,1,1) )\n",
    "            features['mel1'].append(np.array(mel1))\n",
    "\n",
    "            if use_cqt :\n",
    "                mel2 = feature_extract_cqt(filename, samp_sec=samp_sec, pre_emphasis = pre_emphasis, filter_scale = filter_scale, \n",
    "                                           n_bins = n_bins, fmin = fmin, trim = trim)[0]\n",
    "            else :\n",
    "                mel2 = np.zeros( (1,1,1))                \n",
    "            features['cqt1'].append(np.array(mel2))\n",
    "\n",
    "            if use_stft :\n",
    "                mel3 = feature_extract_stft(filename, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length, \n",
    "                                            win_length = win_length, trim = trim)[0]\n",
    "            else :\n",
    "                mel3 = np.zeros( (1,1,1) )\n",
    "            features['stft1'].append(np.array(mel3))\n",
    "\n",
    "            if use_raw :\n",
    "                recording1,frequency1 = librosa.load(filename)\n",
    "            else :\n",
    "                recording1 = np.zeros( (1) )\n",
    "            features['raw1'].append(recording1)\n",
    "            \n",
    "            if use_wav2:\n",
    "                recording1,frequency1 = librosa.load(filename)\n",
    "            else :\n",
    "                recording1 = np.zeros( (1) )\n",
    "            tmp_wav.append(recording1)                \n",
    "                \n",
    "            \n",
    "            if use_interval :\n",
    "                               \n",
    "                datos=sp.io.wavfile.read(filename)\n",
    "                filtros=sio.loadmat('/home/jk21/Documents/hmd/jk_classifier/S1-S1-Phonocardiogram-Peak-Detection-Method-in-Python/Filters1')\n",
    "                \n",
    "                try:\n",
    "                    \n",
    "                    X=datos[1]\n",
    "                    X=X[trim*3:-trim*3]\n",
    "                    Fs=datos[0]\n",
    "            \n",
    "                    Fpa20=filtros['Fpa20'];\t\t\t        # High pass filter\n",
    "                    Fpa20=Fpa20[0];\t\t\t\t\t# High pass filter\n",
    "                    Fpb100=filtros['Fpb100'];\t\t        # Low-pass Filter\n",
    "                    Fpb100=Fpb100[0];\t\t\t\t# Low-pass Filter\n",
    "            \n",
    "                    Xf=FpassBand(X,Fpa20,Fpb100); \t                # Apply a passband filter\n",
    "                    Xf=vec_nor(Xf);\t\t\t\n",
    "            \n",
    "                    # Derivate of the Signal\n",
    "                    dX=derivate(Xf);\t\t\t\t# Derivate of the signal\n",
    "                    dX=vec_nor(dX);\t\t\t\t\t# Vector Normalizing\n",
    "                    # Square of the signal\n",
    "                    dy=np.square(Xf);\n",
    "                    dy=vec_nor(dy);\n",
    "                    \n",
    "                    size=np.shape(Xf)\t\t\t\t# Rank or dimension of the array\n",
    "                    fil=size[0];\t\t\t\t\t# Number of rows\n",
    "\n",
    "                    positive=np.zeros((1,fil+1));                   # Initializating Positives Values Vector \n",
    "                    positive=positive[0];                           # Getting the Vector\n",
    "\n",
    "                    points=np.zeros((1,fil));                       # Initializating the all Peak Points Vector\n",
    "                    points=points[0];                               # Getting the point vector\n",
    "\n",
    "                    peaks=np.zeros((1,fil));                        # Initializating the s1-s1 Peak Vector\n",
    "                    peaks=peaks[0];                                 # Getting the point vector\n",
    "\n",
    "            \n",
    "                    for i in range(0,fil):\n",
    "                        if dX[i]>0:\n",
    "                            positive[i]=1;\n",
    "                        else:\n",
    "                            positive[i]=0;\n",
    "\n",
    "                    for i in range(0,fil):\n",
    "                        if (positive[i]==1 and positive[i+1]==0):\n",
    "                            points[i]=Xf[i];\n",
    "                        else:\n",
    "                            points[i]=0;\n",
    "\n",
    "                    indexes=peakutils.indexes(points,thres=0.5/max(points), min_dist=1000);\n",
    "                    lenght=np.shape(indexes)\t\t\t# Get the length of the index vector\t\t\n",
    "                    lenght=lenght[0];\t\t\t\t# Get the value of the index vector\n",
    "\n",
    "                    for i in range(0,lenght):\n",
    "                        p=indexes[i];\n",
    "                        peaks[p]=points[p];\n",
    "        \n",
    "                    n=np.arange(0,fil);                            # Vector to the X axes (Number of Samples)\n",
    "            \n",
    "                    tmp_peaks = np.diff(indexes)\n",
    "                    \n",
    "                    \n",
    "                    tmp_interval.append(tmp_peaks)\n",
    "                    \n",
    "           \n",
    "                except:\n",
    "                    print(filename)\n",
    "                    tmp_peaks = np.zeros(maxlen)\n",
    "                    tmp_interval.append(tmp_peaks)\n",
    "                    \n",
    "            else :\n",
    "                        \n",
    "                tmp_peaks = np.zeros(maxlen)\n",
    "                tmp_interval.append(tmp_peaks)\n",
    "                \n",
    "      \n",
    "            \n",
    "            # Extract age_group\n",
    "            age_group = get_age(current_patient_data)\n",
    "            current_age_group = np.zeros(6, dtype=int)\n",
    "            if age_group in age_classes:\n",
    "                j = age_classes.index(age_group)\n",
    "                current_age_group[j] = 1\n",
    "            else :\n",
    "                current_age_group[5] = 1\n",
    "            features['age'].append(current_age_group)\n",
    "\n",
    "            # Extract sex\n",
    "            sex = get_sex(current_patient_data)\n",
    "            sex_features = np.zeros(2, dtype=int)\n",
    "            if compare_strings(sex, 'Female'):\n",
    "                sex_features[0] = 1\n",
    "            elif compare_strings(sex, 'Male'):\n",
    "                sex_features[1] = 1\n",
    "            features['sex'].append(sex_features)\n",
    "\n",
    "            # Extract height and weight.\n",
    "            height = get_height(current_patient_data)\n",
    "            weight = get_weight(current_patient_data)\n",
    "            ## simple impute\n",
    "            if math.isnan(height) :\n",
    "                height = 110.846\n",
    "            if math.isnan(weight) :\n",
    "                weight = 23.767\n",
    "                \n",
    "            features['hw'].append(np.array([height, weight]))\n",
    "\n",
    "            # Extract pregnancy\n",
    "            is_pregnant = get_pregnancy_status(current_patient_data)\n",
    "            features['preg'].append(is_pregnant)\n",
    "\n",
    "            # Extract location\n",
    "            locations = entries[0]\n",
    "            num_recording_locations = len(recording_locations)\n",
    "            loc_features = np.zeros(num_recording_locations)\n",
    "            if locations in recording_locations:\n",
    "                j = recording_locations.index(locations)\n",
    "                loc_features[j] = 1\n",
    "            features['loc'].append(loc_features)\n",
    "\n",
    "            # Extract labels \n",
    "            mm_label = get_murmur(current_patient_data)\n",
    "            out_label = get_outcome(current_patient_data)\n",
    "            current_mm_labels = np.zeros(2)\n",
    "            current_out_labels = np.zeros(2)\n",
    "            if mm_label == 'Absent' :\n",
    "                current_mm_labels = np.array([0, 0, 1])\n",
    "            elif mm_label == 'unknown' :\n",
    "                current_mm_labels = np.array([0, 1, 0])\n",
    "            else :\n",
    "                mm_loc = get_murmur_loc(current_patient_data)\n",
    "                if mm_loc == 'nan' :\n",
    "                    current_mm_labels = np.array([0.9, 0.05, 0.05])\n",
    "                else :\n",
    "                    mm_loc = mm_loc.split('+')\n",
    "                    if locations in mm_loc :\n",
    "                        current_mm_labels = np.array([1, 0, 0])\n",
    "                    else :\n",
    "                        current_mm_labels = np.array([0.7, 0.2, 0.1])\n",
    "\n",
    "            if out_label == 'Normal' :\n",
    "                current_out_labels = np.array([0, 1])\n",
    "            else :\n",
    "                current_out_labels = np.array([1, 0])\n",
    "#                if mm_label == 'Absent' :\n",
    "#                    current_out_labels = np.array([0.8, 0.2])\n",
    "#                elif mm_label == 'unknown' :\n",
    "#                    current_out_labels = np.array([0.85, 0.15])\n",
    "#                else :\n",
    "#                    current_out_labels = np.array([1, 0])\n",
    "                \n",
    "            features['mm_labels'].append(current_mm_labels)\n",
    "            features['out_labels'].append(current_out_labels)\n",
    "\n",
    "    if use_mel :\n",
    "        M, N = features['mel1'][0].shape\n",
    "        for i in range(len(features['mel1'])) :\n",
    "            \n",
    "            features['mel1'][i] = features['mel1'][i].reshape(M,N,1)\n",
    "            \n",
    "    else :\n",
    "        M, N, _ = features['mel1'][0].shape\n",
    "    print(\"melspec: \", M,N)\n",
    "    \n",
    "    mel_input_shape = (M,N,1)\n",
    "        \n",
    "    if use_cqt :\n",
    "        M, N= features['cqt1'][0].shape\n",
    "        for i in range(len(features['cqt1'])) :\n",
    "            features['cqt1'][i] = features['cqt1'][i].reshape(M,N,1)\n",
    "       \n",
    "    else :\n",
    "        M, N,__ = features['cqt1'][0].shape\n",
    "    print(\"cqt: \", M,N)\n",
    "    cqt_input_shape = (M,N,1)\n",
    "\n",
    "    \n",
    "    if use_stft :\n",
    "        M, N = features['stft1'][0].shape\n",
    "        for i in range(len(features['stft1'])) :\n",
    "            features['stft1'][i] = features['stft1'][i].reshape(M,N,1)\n",
    "        \n",
    "    else :\n",
    "        M, N ,__= features['stft1'][0].shape\n",
    "    print(\"stft: \", M,N)\n",
    "    stft_input_shape = (M,N,1)\n",
    "    \n",
    "    if use_interval:\n",
    "        \n",
    "        for i in range(len(tmp_interval)):\n",
    "            tmp_len = len(tmp_interval[i])\n",
    "            interval_len.append(tmp_len)\n",
    "        \n",
    "        \n",
    "        interval_len = np.array(interval_len)\n",
    "        max_interval_len = np.max(interval_len)\n",
    "        \n",
    "        padded =pad_sequences(tmp_interval, maxlen=max_interval_len, dtype='float64', padding='post', truncating='post', value=0.0)\n",
    "        \n",
    "        for i in range(len(padded)):\n",
    "            features['interval'].append(padded[i])\n",
    "        \n",
    "    else:\n",
    "        for i in range(len(tmp_interval)):\n",
    "            features['interval'].append(tmp_interval[i])\n",
    "\n",
    "    if use_interval:\n",
    "        \n",
    "        for i in range(len(features['interval'])):\n",
    "            features['interval'][i]= features['interval'][i].reshape(-1,1)\n",
    "        \n",
    "    else:\n",
    "        for i in range(len(features['interval'])):\n",
    "            features['interval'][i]= features['interval'][i].reshape(-1,1)\n",
    "\n",
    "    for k1 in features.keys() :\n",
    "        features[k1] = np.array(features[k1])          \n",
    "\n",
    "    \n",
    "    if use_wav2:\n",
    "        padded =pad_sequences(tmp_wav, maxlen=maxlen1, dtype='float64', padding='pre', truncating='pre', value=0.0)\n",
    "        tmp_pad = padded[:,np.newaxis,:]\n",
    "        \n",
    "        tmp=[]\n",
    "        for i in range(len(tmp_pad)):\n",
    "            tmp_feature = model(tmp_pad[i])\n",
    "            tmp.append(tmp_feature)\n",
    "            \n",
    "        tmp=np.array(tmp, dtype=np.float32)\n",
    "        new_tmp1=tmp.reshape(-1,374,32)\n",
    "        features['wav2']=new_tmp1\n",
    "        \n",
    "    \n",
    "    interval_input_shape = features['interval'].shape[1:]\n",
    "    \n",
    "    M,N = interval_input_shape\n",
    "    \n",
    "    print(\"interval: \", M,N)\n",
    "    \n",
    "    wav2_input_shape = features['wav2'].shape[1:]\n",
    "    \n",
    "    M,N = wav2_input_shape\n",
    "    \n",
    "    print(\"wav2: \", M,N)\n",
    "    \n",
    "    return features, mel_input_shape, cqt_input_shape, stft_input_shape,interval_input_shape,wav2_input_shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8bd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5468424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a42707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb27174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c45e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf900b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8a026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████▌                 | 110/191 [11:09<09:07,  6.76s/it]"
     ]
    }
   ],
   "source": [
    "features_test,mel_input_shape, cqt_input_shape,stft_input_shape,interval_input_shape,wav2_input_shape= get_features_3lb_all(test_folder, patient_files_test, **params_feature,use_mel = True, use_cqt = False, use_stft = False,use_raw=False,use_interval=True,use_wav2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6457f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b2ff2db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('/home/jk21/Downloads/Data/features_trn_wav2.pickle', 'wb') as f:\n",
    "    pickle.dump(features_trn, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f7179a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('/home/jk21/Downloads/Data/features_test_wav2.pickle', 'wb') as f:\n",
    "    pickle.dump(features_test, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1499b589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f59c5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('/home/jk21/Downloads/Data/features_trn_wav2.pickle', 'rb') as f:\n",
    "    features_trn = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd412feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jk21/Downloads/Data/features_test_wav2.pickle', 'rb') as f:\n",
    "    features_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001bbe81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460de784",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_input_shape = features_trn['mel1'].shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0291cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec06d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cqt_input_shape = features_trn['cqt1'].shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31599947",
   "metadata": {},
   "outputs": [],
   "source": [
    "cqt_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861213a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_input_shape = features_trn['stft1'].shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e151a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7eb1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_input_shape= features_trn['interval'].shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31f94d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interval_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2_input_shape= features_trn['wav2'].shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7553a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9af0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test['raw1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(features_trn['interval'])):\n",
    "    features_trn['interval'][i]=np.array(features_trn['interval'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e660a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(features_trn['mel1'])):\n",
    "    features_trn['mel1'][i]=np.array(features_trn['mel1'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffbed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = get_LCNN_o_4_dr(mel_input_shape, cqt_input_shape,stft_input_shape, interval_input_shape, wav2_input_shape,use_mel=True, use_stft = False,use_cqt=False,ext = True,ord1=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0100c3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc8685",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2 = get_LCNN_o_4_dr(mel_input_shape, cqt_input_shape,stft_input_shape, interval_input_shape, wav2_input_shape,use_mel=True, use_stft = False,use_cqt=False,ext = True,ord1=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e462fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2279ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b2f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedc154b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bbf746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a145f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 50\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "params = {'batch_size': batch_size,\n",
    "#          'input_shape': (100, 313, 1),\n",
    "          'shuffle': True,\n",
    "          'beta_param': 0.7,\n",
    "          'mixup': True,\n",
    "          'lowpass': [.5, [11,12,13,14,15,16,17,18]],\n",
    "          'highpass': [.5, [78,79,80,81,82,83,84,85]],\n",
    "#           'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "#           'dropblock' : [30, 100]\n",
    "          #'device' : device\n",
    "}\n",
    "\n",
    "params_no_shuffle = {'batch_size': batch_size,\n",
    "#          'input_shape': (100, 313, 1),\n",
    "          'shuffle': False,\n",
    "          'beta_param': 0.7,\n",
    "          'mixup': False\n",
    "          #'device': device\n",
    "}\n",
    "\n",
    "TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], features_trn['preg'], features_trn['loc'],\n",
    "            features_trn['interval'],features_trn['wav2'],features_trn['mel1'],features_trn['cqt1'],features_trn['stft1']], \n",
    "           features_trn['mm_labels'],  ## our Y\n",
    "                        **params)()\n",
    "\n",
    "\n",
    "ValidDGen_1 = Generator0([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                              features_test['preg'], features_test['loc'],features_test['interval'],\n",
    "                              features_test['wav2'], features_test['mel1'],features_test['cqt1'],features_test['stft1']], \n",
    "                             features_test['mm_labels'],  ## our Y\n",
    "                        **params)()\n",
    "\n",
    "class_weight = {0: 3, 1: 1., 2: 1} \n",
    "\n",
    " \n",
    "model1.fit(TrainDGen_1, \n",
    "           validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                              features_test['preg'], features_test['loc'],features_test['interval'],\n",
    "                              features_test['wav2'], features_test['mel1'],features_test['cqt1'],features_test['stft1']], \n",
    "                             features_test['mm_labels']),\n",
    "           callbacks=[lr],\n",
    "           steps_per_epoch=np.ceil(len(features_trn['mm_labels'])/batch_size), \n",
    "           validation_steps=np.ceil(len(features_test['mm_labels'])/batch_size)\n",
    "           ,epochs = n_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f3f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bdd466",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_epoch = 50\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=n_epoch))\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "params = {'batch_size': batch_size,\n",
    "#          'input_shape': (100, 313, 1),\n",
    "          'shuffle': True,\n",
    "          'beta_param': 0.7,\n",
    "          'mixup': True,\n",
    "         'lowpass': [.5, [11,12,13,14,15,16,17,18]],\n",
    "          'highpass': [.5, [78,79,80,81,82,83,84,85]],\n",
    "#           'ranfilter2' : [3, [18,19,20,21,22,23]]\n",
    "#           'dropblock' : [30, 100]\n",
    "          #'device' : device\n",
    "}\n",
    "\n",
    "params_no_shuffle = {'batch_size': batch_size,\n",
    "#          'input_shape': (100, 313, 1),\n",
    "          'shuffle': False,\n",
    "          'beta_param': 0.7,\n",
    "          'mixup': False\n",
    "          #'device': device\n",
    "}\n",
    "\n",
    "TrainDGen_1 = Generator0([features_trn['age'],features_trn['sex'], features_trn['hw'], features_trn['preg'], features_trn['loc'],\n",
    "            features_trn['interval'],features_trn['wav2'],features_trn['mel1'],features_trn['cqt1'],features_trn['stft1']], \n",
    "           features_trn['out_labels'],  ## our Y\n",
    "                        **params)()\n",
    "\n",
    "#ValDGen_1 = Generator0([features_test[0]['age'],features_test[0]['sex'], features_test[0]['hw'], features_test[0]['preg'], features_test[0]['loc'], \n",
    "#           features_test[0]['mel1'],features_test[0]['cqt1'],features_test[0]['stft1']], \n",
    "#                        features_test[2],  ## our Y\n",
    "#                        **params_no_shuffle)()\n",
    "class_weight = {0: 3, 1: 1., 2: 1} \n",
    "    \n",
    "model2.fit(TrainDGen_1,\n",
    "           validation_data = ([features_test['age'],features_test['sex'], features_test['hw'], \n",
    "                              features_test['preg'], features_test['loc'],features_test['interval'],\n",
    "                              features_test['wav2'], features_test['mel1'],features_test['cqt1'],features_test['stft1']], \n",
    "                             features_test['out_labels']),\n",
    "           callbacks=[lr],\n",
    "           steps_per_epoch=np.ceil(len(features_trn['out_labels'])/batch_size), \n",
    "           validation_steps=np.ceil(len(features_test['out_labels'])/batch_size),\n",
    "           epochs = n_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f69250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9abb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f47fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_feature['mel_shape'] = mel_input_shape\n",
    "params_feature['cqt_shape'] = cqt_input_shape\n",
    "params_feature['stft_shape'] = stft_input_shape\n",
    "params_feature['interval_input_shape'] = interval_input_shape\n",
    "params_feature['wav2_input_shape'] = wav2_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786d2ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "def save_challenge_model(model_folder, model1, model2, m_name1, m_name2, param_feature) :\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    filename1 = os.path.join(model_folder, m_name1 + '_model1.hdf5')\n",
    "    filename2 = os.path.join(model_folder, m_name2 + '_model2.hdf5')\n",
    "    model1.save(filename1)\n",
    "    model2.save(filename2)\n",
    "    param_feature['model1'] = m_name1\n",
    "    param_feature['model2'] = m_name2\n",
    "    param_feature['model_fnm1'] = filename1\n",
    "    param_feature['model_fnm2'] = filename2\n",
    "\n",
    "    with open(info_fnm, 'wb') as f:\n",
    "        pk.dump(param_feature, f, pk.HIGHEST_PROTOCOL)\n",
    "    return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ae924",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1124d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_challenge_model(model_folder, model1, model2, m_name1 = 'lcnn1', m_name2 = 'lcnn2', param_feature = params_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba030c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_challenge_model(model_folder, verbose):\n",
    "    info_fnm = os.path.join(model_folder, 'desc.pk')\n",
    "    with open(info_fnm, 'rb') as f:\n",
    "        info_m = pk.load(f)\n",
    "#    if info_m['model'] == 'toy' :\n",
    "#        model = get_toy(info_m['mel_shape'])\n",
    "#    filename = os.path.join(model_folder, info_m['model'] + '_model.hdf5')\n",
    "#    model.load_weights(filename)\n",
    "    return info_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31909672",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a747dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_challenge_model(model, data, recordings, verbose,use_mel=True,use_stft=False,use_cqt=False,\n",
    "                        maxlen=80,min_dist=500):\n",
    "    \n",
    "    murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "    outcome_classes = ['Abnormal', 'Normal']\n",
    "    \n",
    "    if model['model1'] == 'lcnn1' :\n",
    "        model1 = get_LCNN_o_4_dr(model['mel_shape'],model['cqt_shape'],model['stft_shape'], model['interval_input_shape'],model['wav2_input_shape'],use_mel = True, use_stft = False,use_cqt=False,ord1=False,ext=True)\n",
    "    if model['model2'] == 'lcnn2' :\n",
    "        model2 = get_LCNN_o_4_dr(model['mel_shape'],model['cqt_shape'],model['stft_shape'], model['interval_input_shape'], model['wav2_input_shape'],use_mel = True,use_stft = False,use_cqt=False,ord1=True,ext=True)\n",
    "    model1.load_weights(model['model_fnm1'])\n",
    "    model2.load_weights(model['model_fnm2'])\n",
    "    \n",
    "    maxlen = params_feature['maxlen']\n",
    "    maxlen1 = params_feature['maxlen1']\n",
    "    min_dist = params_feature['min_dist']\n",
    "#    classes = model['classes']\n",
    "    # Load features.\n",
    "    features = get_feature_one(data, verbose = 0)\n",
    "\n",
    "    samp_sec = model['samp_sec'] \n",
    "    pre_emphasis = model['pre_emphasis']\n",
    "    hop_length = model['hop_length']\n",
    "    win_length = model['win_length']\n",
    "    n_mels = model['n_mels']\n",
    "    filter_scale = model['filter_scale']\n",
    "    n_bins = model['n_bins']\n",
    "    fmin = model['fmin']\n",
    "    \n",
    "\n",
    "    features['interval'] = []\n",
    "    tmp_interval=[]  \n",
    "    \n",
    "    features['wav2'] = []\n",
    "    tmp_raw = []\n",
    "    tmp=[]      \n",
    "    \n",
    "    from wav2vec2 import Wav2Vec2ForCTC, Wav2Vec2Config\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    config = Wav2Vec2Config()\n",
    "    tmp_model = Wav2Vec2ForCTC(config)\n",
    "    tmp_model.trainable=False\n",
    "    \n",
    "    for i in range(len(recordings)):\n",
    "        data = recordings[i]/32768\n",
    "        tmp_raw.append(data)\n",
    "        \n",
    "    tmp_pad =pad_sequences(tmp_raw, maxlen=maxlen1, dtype='float64', padding='pre', truncating='pre', value=0.0)\n",
    "    tmp_pad = tmp_pad[:,np.newaxis,:]\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(tmp_pad))):\n",
    "        tmp_feature = tmp_model(tmp_pad[i])\n",
    "        tmp.append(tmp_feature)\n",
    "    \n",
    "    tmp=np.array(tmp, dtype=np.float32)\n",
    "    new_tmp1=tmp.reshape(-1,374,32)\n",
    "    features['wav2']=new_tmp1\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(recordings)) :\n",
    "        datos=recordings[i]\n",
    "        filtros=sio.loadmat('/home/jk21/Documents/hmd/jk_classifier/S1-S1-Phonocardiogram-Peak-Detection-Method-in-Python/Filters1')\n",
    "        try:\n",
    "            X=datos\n",
    "            X=X[12000:-12000]\n",
    "            Fs=4000\n",
    "            \n",
    "            Fpa20=filtros['Fpa20'];\t\t\t        # High pass filter\n",
    "            Fpa20=Fpa20[0];\t\t\t\t\t# High pass filter\n",
    "            Fpb100=filtros['Fpb100'];\t\t        # Low-pass Filter\n",
    "            Fpb100=Fpb100[0];\t\t\t\t# Low-pass Filter\n",
    "            \n",
    "            Xf=FpassBand(X,Fpa20,Fpb100); \t                # Apply a passband filter\n",
    "            Xf=vec_nor(Xf);\t\t\t\n",
    "            \n",
    "                    # Derivate of the Signal\n",
    "            dX=derivate(Xf);\t\t\t\t# Derivate of the signal\n",
    "            dX=vec_nor(dX);\t\t\t\t\t# Vector Normalizing\n",
    "                    # Square of the signal\n",
    "            dy=np.square(Xf);\n",
    "            dy=vec_nor(dy);\n",
    "                    \n",
    "            size=np.shape(Xf)\t\t\t\t# Rank or dimension of the array\n",
    "            fil=size[0];\t\t\t\t\t# Number of rows\n",
    "\n",
    "            positive=np.zeros((1,fil+1));                   # Initializating Positives Values Vector \n",
    "            positive=positive[0];                           # Getting the Vector\n",
    "\n",
    "            points=np.zeros((1,fil));                       # Initializating the all Peak Points Vector\n",
    "            points=points[0];                               # Getting the point vector\n",
    "\n",
    "            peaks=np.zeros((1,fil));                        # Initializating the s1-s1 Peak Vector\n",
    "            peaks=peaks[0];                                 # Getting the point vector\n",
    "\n",
    "            \n",
    "            for i in range(0,fil):\n",
    "                if dX[i]>0:\n",
    "                    positive[i]=1;\n",
    "                else:\n",
    "                    positive[i]=0;\n",
    "\n",
    "            for i in range(0,fil):\n",
    "                if (positive[i]==1 and positive[i+1]==0):\n",
    "                    points[i]=Xf[i];\n",
    "                else:\n",
    "                    points[i]=0;\n",
    "\n",
    "            indexes=peakutils.indexes(points,thres=0.5/max(points), min_dist=min_dist);\n",
    "            lenght=np.shape(indexes)\t\t\t# Get the length of the index vector\t\t\n",
    "            lenght=lenght[0];\t\t\t\t# Get the value of the index vector\n",
    "\n",
    "            for i in range(0,lenght):\n",
    "                p=indexes[i];\n",
    "                peaks[p]=points[p];\n",
    "        \n",
    "            n=np.arange(0,fil);                            # Vector to the X axes (Number of Samples)\n",
    "            \n",
    "            tmp_peaks = np.diff(indexes)\n",
    "                    \n",
    "            tmp_interval.append(tmp_peaks)\n",
    "                    \n",
    "           \n",
    "        except:\n",
    "            print(i)\n",
    "            tmp_peaks = np.zeros(maxlen)\n",
    "            tmp_interval.append(tmp_peaks)\n",
    "                    \n",
    "     \n",
    "    padded =pad_sequences(tmp_interval, maxlen=maxlen, dtype='float64', padding='pre', truncating='pre', value=0.0)\n",
    "        \n",
    "    for i in range(len(padded)):\n",
    "        features['interval'].append(padded[i])\n",
    "        \n",
    "        \n",
    "    for i in range(len(features['interval'])):\n",
    "        features['interval'][i] = np.array(features['interval'][i])\n",
    "        \n",
    "    features['interval'] = np.array(features['interval'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    features['mel1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_mel :\n",
    "            mel1 = feature_extract_melspec(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length, \n",
    "                                           win_length = win_length, n_mels = n_mels)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1) )\n",
    "            \n",
    "        features['mel1'].append(mel1)\n",
    "        \n",
    "    M, N = features['mel1'][0].shape\n",
    "    \n",
    "    if use_mel :\n",
    "        for i in range(len(features['mel1'])) :\n",
    "            features['mel1'][i] = features['mel1'][i].reshape(M,N,1)   \n",
    "    \n",
    "    features['mel1'] = np.array(features['mel1'])\n",
    "\n",
    "    \n",
    "  \n",
    "    features['cqt1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_cqt :\n",
    "            mel1 = feature_extract_cqt(recordings[i], samp_sec=samp_sec, pre_emphasis = pre_emphasis, filter_scale = filter_scale, \n",
    "                                        n_bins = n_bins, fmin = fmin)[0]\n",
    "        else:\n",
    "            mel1 = np.zeros( (1,1,1) )\n",
    "            \n",
    "        features['cqt1'].append(mel1)\n",
    "        \n",
    "    M, N,__ = features['cqt1'][0].shape\n",
    "    \n",
    "    if use_cqt :\n",
    "        for i in range(len(features['cqt1'])) :\n",
    "            features['cqt1'][i] = features['cqt1'][i].reshape(M,N,1)   \n",
    "    \n",
    "    features['cqt1'] = np.array(features['cqt1'])\n",
    "    \n",
    "    \n",
    "    features['stft1'] = []\n",
    "    for i in range(len(recordings)) :\n",
    "        if use_stft :\n",
    "            mel1 = feature_extract_stft(recordings[i]/ 32768, samp_sec=samp_sec, pre_emphasis = pre_emphasis, hop_length=hop_length, \n",
    "                                        win_length = win_length)[0]\n",
    "        else :\n",
    "            mel1 = np.zeros( (1,1,1) )\n",
    "        \n",
    "        features['stft1'].append(mel1)\n",
    "        \n",
    "    M, N,__ = features['stft1'][0].shape\n",
    "    if use_stft :\n",
    "        for i in range(len(features['stft1'])) :\n",
    "            features['stft1'][i] = features['stft1'][i].reshape(M,N,1)           \n",
    "    features['stft1'] = np.array(features['stft1'])\n",
    "    \n",
    "#     return features       \n",
    "\n",
    "    #    print(features)\n",
    "    # Impute missing data.\n",
    "    res1 = model1.predict([features['age'], features['sex'], features['hw'], features['preg'], \n",
    "                           features['loc'], features['interval'],features['wav2'],features['mel1'], features['stft1'], features['cqt1']])\n",
    "    res2 = model2.predict([features['age'], features['sex'], features['hw'], features['preg'], \n",
    "                           features['loc'], features['interval'],features['wav2'],features['mel1'], features['stft1'], features['cqt1']])\n",
    "\n",
    "    # Get classifier probabilities.\n",
    "    idx1 = res1.argmax(axis=0)[0]\n",
    "    murmur_probabilities = res1[idx1,]  ## mumur 확률 최대화 되는 애 뽑기\n",
    "    outcome_probabilities = res2.mean(axis = 0) ##  outcome 은 그냥 평균으로 뽑기\n",
    "#    idx = np.argmax(prob1)\n",
    "\n",
    "    ## 이부분도 생각 필요.. rule 을 cost를 maximize 하는 기준으로 threshold 탐색 필요할지도..\n",
    "    # Choose label with highest probability.\n",
    "    murmur_labels = np.zeros(len(murmur_classes), dtype=np.int_)\n",
    "    idx = np.argmax(murmur_probabilities)\n",
    "    murmur_labels[idx] = 1\n",
    "    outcome_labels = np.zeros(len(outcome_classes), dtype=np.int_)\n",
    "    idx = np.argmax(outcome_probabilities)\n",
    "    outcome_labels[idx] = 1\n",
    "    \n",
    "    # Concatenate classes, labels, and probabilities.\n",
    "    classes = murmur_classes + outcome_classes\n",
    "    labels = np.concatenate((murmur_labels, outcome_labels))\n",
    "    probabilities = np.concatenate((murmur_probabilities, outcome_probabilities))\n",
    "    \n",
    "    return classes, labels, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eecd4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = test_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f523ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ffdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder ='/home/jk21/Documents/hmd/jk_classifier/out_lcnn2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e8d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "allow_failures=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccc27ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c582c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73579324",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_challenge_model(model_folder, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a7edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_files = find_patient_files(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450b3026",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patient_files = len(patient_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data = load_patient_data(patient_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f039e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "recordings = load_recordings(data_folder, patient_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3acd534",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, labels, probabilities = run_challenge_model(model, patient_data, recordings, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a44c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model.\n",
    "def run_model(model_folder, data_folder, output_folder, allow_failures, verbose=1):\n",
    "    # Load models.\n",
    "    if verbose >= 1:\n",
    "        print('Loading Challenge model...')\n",
    "\n",
    "    model = load_challenge_model(model_folder, verbose) ### Teams: Implement this function!!!\n",
    "\n",
    "    # Find the patient data files.\n",
    "    patient_files = find_patient_files(data_folder)\n",
    "    num_patient_files = len(patient_files)\n",
    "\n",
    "    if num_patient_files==0:\n",
    "        raise Exception('No data was provided.')\n",
    "\n",
    "    # Create a folder for the Challenge outputs if it does not already exist.\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Run the team's model on the Challenge data.\n",
    "    if verbose >= 1:\n",
    "        print('Running model on Challenge data...')\n",
    "\n",
    "    # Iterate over the patient files.\n",
    "    for i in range(num_patient_files):\n",
    "        if verbose >= 2:\n",
    "            print('    {}/{}...'.format(i+1, num_patient_files))\n",
    "\n",
    "        patient_data = load_patient_data(patient_files[i])\n",
    "        recordings = load_recordings(data_folder, patient_data)\n",
    "\n",
    "        # Allow or disallow the model to fail on parts of the data; helpful for debugging.\n",
    "        try:\n",
    "            classes, labels, probabilities = run_challenge_model(model, patient_data, recordings, verbose) ### Teams: Implement this function!!!\n",
    "        except:\n",
    "            if allow_failures:\n",
    "                if verbose >= 2:\n",
    "                    print('... failed.')\n",
    "                classes, labels, probabilities = list(), list(), list()\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # Save Challenge outputs.\n",
    "        head, tail = os.path.split(patient_files[i])\n",
    "        root, extension = os.path.splitext(tail)\n",
    "        output_file = os.path.join(output_folder, root + '.csv')\n",
    "        patient_id = get_patient_id(patient_data)\n",
    "        save_challenge_outputs(output_file, patient_id, classes, labels, probabilities)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe188c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ee9f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '/home/jk21/Documents/hmd/jk_classifier/out_lcnn2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be81f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_model(model_folder, test_folder, output_folder, allow_failures = True, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a6478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b770afd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "murmur_scores, outcome_scores = evaluate_model(test_folder, output_folder)\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "    + '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "\n",
    "if len(sys.argv) == 3:\n",
    "    print(output_string)\n",
    "elif len(sys.argv) == 4:\n",
    "    with open(sys.argv[3], 'w') as f:\n",
    "        f.write(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a27458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4235c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_folder = test_folder\n",
    "murmur_classes = ['Present', 'Unknown', 'Absent']\n",
    "outcome_classes = ['Abnormal', 'Normal']\n",
    "\n",
    "# Load and parse label and model output files.\n",
    "label_files, output_files = find_challenge_files(label_folder, output_folder)\n",
    "murmur_labels = load_murmurs(label_files, murmur_classes)\n",
    "murmur_binary_outputs, murmur_scalar_outputs = load_classifier_outputs(output_files, murmur_classes)\n",
    "outcome_labels = load_outcomes(label_files, outcome_classes)\n",
    "outcome_binary_outputs, outcome_scalar_outputs = load_classifier_outputs(output_files, outcome_classes)\n",
    "\n",
    "\n",
    "print(np.mean(murmur_scalar_outputs[:,0]))\n",
    "print(np.mean(murmur_scalar_outputs[:,2]))\n",
    "print(np.mean(outcome_scalar_outputs[:,0]))\n",
    "print(np.mean(outcome_scalar_outputs[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb59f7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for th1 in [0.01, 0.05, 0.1, 0.15,0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8] :\n",
    "    murmur_binary_outputs[:,0] = murmur_scalar_outputs[:,0] > th1\n",
    "    murmur_binary_outputs[:,2] = murmur_scalar_outputs[:,2] > 1 - th1\n",
    "    outcome_binary_outputs[:,0] = outcome_scalar_outputs[:,0] > th1\n",
    "    outcome_binary_outputs[:,1] = outcome_scalar_outputs[:,1] > 1 - th1\n",
    "    # For each patient, set the 'Present' or 'Abnormal' class to positive if no class is positive or if multiple classes are positive.\n",
    "    murmur_labels = enforce_positives(murmur_labels, murmur_classes, 'Present')\n",
    "    murmur_binary_outputs = enforce_positives(murmur_binary_outputs, murmur_classes, 'Present')\n",
    "    outcome_labels = enforce_positives(outcome_labels, outcome_classes, 'Abnormal')\n",
    "    outcome_binary_outputs = enforce_positives(outcome_binary_outputs, outcome_classes, 'Abnormal')\n",
    "    # Evaluate the murmur model by comparing the labels and model outputs.\n",
    "    murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes = compute_auc(murmur_labels, murmur_scalar_outputs)\n",
    "    murmur_f_measure, murmur_f_measure_classes = compute_f_measure(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_accuracy, murmur_accuracy_classes = compute_accuracy(murmur_labels, murmur_binary_outputs)\n",
    "    murmur_weighted_accuracy = compute_weighted_accuracy(murmur_labels, murmur_binary_outputs, murmur_classes) # This is the murmur scoring metric.\n",
    "    murmur_cost = compute_cost(outcome_labels, murmur_binary_outputs, outcome_classes, murmur_classes) # Use *outcomes* to score *murmurs* for the Challenge cost metric, but this is not the actual murmur scoring metric.\n",
    "    murmur_scores = (murmur_classes, murmur_auroc, murmur_auprc, murmur_auroc_classes, murmur_auprc_classes, \\\n",
    "                 murmur_f_measure, murmur_f_measure_classes, murmur_accuracy, murmur_accuracy_classes, murmur_weighted_accuracy, murmur_cost)\n",
    "\n",
    "    # Evaluate the outcome model by comparing the labels and model outputs.\n",
    "    outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes = compute_auc(outcome_labels, outcome_scalar_outputs)\n",
    "    outcome_f_measure, outcome_f_measure_classes = compute_f_measure(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_accuracy, outcome_accuracy_classes = compute_accuracy(outcome_labels, outcome_binary_outputs)\n",
    "    outcome_weighted_accuracy = compute_weighted_accuracy(outcome_labels, outcome_binary_outputs, outcome_classes)\n",
    "    outcome_cost = compute_cost(outcome_labels, outcome_binary_outputs, outcome_classes, outcome_classes) # This is the clinical outcomes scoring metric.\n",
    "    outcome_scores = (outcome_classes, outcome_auroc, outcome_auprc, outcome_auroc_classes, outcome_auprc_classes, \\\n",
    "                  outcome_f_measure, outcome_f_measure_classes, outcome_accuracy, outcome_accuracy_classes, outcome_weighted_accuracy, outcome_cost)\n",
    "\n",
    "\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = murmur_scores\n",
    "    murmur_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    murmur_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    classes, auroc, auprc, auroc_classes, auprc_classes, f_measure, f_measure_classes, accuracy, accuracy_classes, weighted_accuracy, cost = outcome_scores\n",
    "    outcome_output_string = 'AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\\n{:.3f},{:.3f},{:.3f},{:.3f},{:.3f},{:.3f}\\n'.format(auroc, auprc, f_measure, accuracy, weighted_accuracy, cost)\n",
    "    outcome_class_output_string = 'Classes,{}\\nAUROC,{}\\nAUPRC,{}\\nF-measure,{}\\nAccuracy,{}\\n'.format(\n",
    "    ','.join(classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auroc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in auprc_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in f_measure_classes),\n",
    "    ','.join('{:.3f}'.format(x) for x in accuracy_classes))\n",
    "\n",
    "    output_string = '#Murmur scores\\n' + murmur_output_string + '\\n#Outcome scores\\n' + outcome_output_string \\\n",
    "                + '\\n#Murmur scores (per class)\\n' + murmur_class_output_string + '\\n#Outcome scores (per class)\\n' + outcome_class_output_string\n",
    "    print(\"threshold: \", th1)\n",
    "    print(output_string)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6615bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f95548d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681adbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f084c345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c260645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367ffff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95bdc02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f085d621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c13c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db42aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4f90b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa06e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7407d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c27aa5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f283a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
